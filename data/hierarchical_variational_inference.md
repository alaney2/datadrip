# Hierarchical Variational Inference

Hierarchical Variational Inference (HVI) is a method for performing approximate Bayesian inference in hierarchical latent variable models. It extends the standard variational inference framework to handle models with multiple layers of latent variables. HVI is particularly useful for deep generative models, such as Variational Autoencoders (VAEs), where the latent variables have a hierarchical structure.

## Background

Bayesian inference is a method for updating the probabilities of hypotheses based on observed data. In the context of latent variable models, the goal is to infer the posterior distribution of the latent variables given the observed data. However, exact Bayesian inference is often intractable due to the high-dimensional integrals involved in computing the posterior distribution. Variational inference is an approximate inference method that addresses this issue by transforming the problem into an optimization problem.

In variational inference, the goal is to find an approximate posterior distribution $q(\mathbf{z}|\mathbf{x})$ that is close to the true posterior $p(\mathbf{z}|\mathbf{x})$. This is achieved by minimizing the Kullback-Leibler (KL) divergence between the approximate and true posteriors:


$$

\text{KL}(q(\mathbf{z}|\mathbf{x})||p(\mathbf{z}|\mathbf{x})) = \int q(\mathbf{z}|\mathbf{x}) \log \frac{q(\mathbf{z}|\mathbf{x})}{p(\mathbf{z}|\mathbf{x})} d\mathbf{z}.

$$


However, the KL divergence is still intractable due to the presence of the true posterior $p(\mathbf{z}|\mathbf{x})$. To overcome this issue, the variational lower bound (also known as the evidence lower bound, or ELBO) is introduced:


$$

\text{ELBO}(q) = \mathbb{E}_{q(\mathbf{z}|\mathbf{x})}[\log p(\mathbf{x}, \mathbf{z})] - \mathbb{E}_{q(\mathbf{z}|\mathbf{x})}[\log q(\mathbf{z}|\mathbf{x})].

$$


Maximizing the ELBO with respect to the variational distribution $q(\mathbf{z}|\mathbf{x})$ is equivalent to minimizing the KL divergence. The ELBO can be optimized using gradient-based methods, such as stochastic gradient descent.

## Hierarchical Latent Variable Models

Hierarchical latent variable models are a class of models where the latent variables have a hierarchical structure. In these models, the observed data $\mathbf{x}$ is generated by a process that involves multiple layers of latent variables $\mathbf{z}^{(1)}, \mathbf{z}^{(2)}, \dots, \mathbf{z}^{(L)}$. The generative process can be represented as:


$$

p(\mathbf{x}, \mathbf{z}^{(1)}, \dots, \mathbf{z}^{(L)}) = p(\mathbf{x}|\mathbf{z}^{(L)}) \prod_{l=1}^{L} p(\mathbf{z}^{(l)}|\mathbf{z}^{(l-1)}),

$$


where $\mathbf{z}^{(0)}$ is an auxiliary variable that is usually set to a fixed value, such as the zero vector.

## Hierarchical Variational Inference

In HVI, the goal is to find an approximate posterior distribution $q(\mathbf{z}^{(1)}, \dots, \mathbf{z}^{(L)}|\mathbf{x})$ that is close to the true posterior $p(\mathbf{z}^{(1)}, \dots, \mathbf{z}^{(L)}|\mathbf{x})$. To achieve this, HVI introduces a hierarchical variational distribution:


$$

q(\mathbf{z}^{(1)}, \dots, \mathbf{z}^{(L)}|\mathbf{x}) = q(\mathbf{z}^{(1)}|\mathbf{x}) \prod_{l=2}^{L} q(\mathbf{z}^{(l)}|\mathbf{z}^{(l-1)}).

$$


The hierarchical variational distribution is designed to mimic the structure of the generative model, with each layer of latent variables conditioned on the previous layer. This allows HVI to capture the dependencies between the latent variables more effectively than standard variational inference, which assumes a factorized variational distribution.

The ELBO for HVI can be written as:


$$

\text{ELBO}(q) = \mathbb{E}_{q(\mathbf{z}^{(1)}, \dots, \mathbf{z}^{(L)}|\mathbf{x})}[\log p(\mathbf{x}, \mathbf{z}^{(1)}, \dots, \mathbf{z}^{(L)})] - \mathbb{E}_{q(\mathbf{z}^{(1)}, \dots, \mathbf{z}^{(L)}|\mathbf{x})}[\log q(\mathbf{z}^{(1)}, \dots, \mathbf{z}^{(L)}|\mathbf{x})].

$$


The ELBO can be optimized using gradient-based methods, such as stochastic gradient descent or more advanced optimization algorithms like Adam. In practice, the gradients of the ELBO with respect to the variational parameters are often estimated using the reparameterization trick, which allows for low-variance gradient estimates.

## Applications

HVI has been successfully applied to various deep generative models, such as Variational Autoencoders (VAEs) and their extensions. By incorporating a hierarchical structure in the variational distribution, HVI can improve the quality of the learned latent representations and the generated samples. Moreover, HVI can be combined with other advanced variational inference techniques, such as normalizing flows and amortized inference, to further improve the performance of deep generative models.

## Conclusion

Hierarchical Variational Inference is a powerful method for performing approximate Bayesian inference in hierarchical latent variable models. By introducing a hierarchical variational distribution that mimics the structure of the generative model, HVI can capture the dependencies between the latent variables more effectively than standard variational inference. HVI has been successfully applied to various deep generative models, such as Variational Autoencoders, and can be combined with other advanced variational inference techniques to further improve the performance of these models.
