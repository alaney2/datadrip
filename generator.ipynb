{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/alaney2/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import shutil\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import nltk\n",
    "from dotenv import load_dotenv\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "load_dotenv('.env.local')\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(topic):\n",
    "    parts = topic.split('_')\n",
    "    parts = [part.capitalize() for part in parts]\n",
    "    topic = ' '.join(parts)\n",
    "    prompt = f'Topic: {topic}\\n' + '''\n",
    "    You are a world-renowned AI and ML expert.\n",
    "    Provide a JSON object containing the topic, a list of 0-8 prerequisite topics, and a list of 0-8 further readings related to AI, ML, and DL.\n",
    "    Ensure that the prerequisites and further readings are specifically relevant to the given, rather than broad topics like calculus or statistics.\n",
    "    When generating topics, prefer the singular form of the topic, such as \"convolutional_neural_network\" instead of \"convolutional_neural_networks\" but use the plural form when it makes more sense to (\"policy_gradient_methods\").\n",
    "    The name of the JSON object must match exactly with the given topic.\n",
    "    Ensure that the title field is properly capitalized and spaced and has the right punctuation (such as Q-Learning).\n",
    "    Also ensure that the topic, prerequisites, and further readings are in snake_case. Do not put single quotes anywhere in the JSON object.\n",
    "    Use a similar format to the example provided below and ensure that the JSON object is valid.:\n",
    "\n",
    "    Example:\n",
    "    {\n",
    "        \"topic_example\": {\n",
    "            \"title\": \"Topic Example\",\n",
    "            \"prerequisites\": [\"page_a\", \"page_b\", \"page_d\"],\n",
    "            \"further_readings\": [\"page_c\", \"page_f\", \"page_z\", \"page_s\"]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    Next, write a detailed wiki page about the given topic in Markdown format. Always write from a third-person perspective and remain unopinionated.\n",
    "    Ensure that this wiki page is explicitly in code format. \n",
    "    Do not include a \"Contents\" section. \n",
    "    Do not include a \"Further Readings\" nor a \"Prerequisites\" section if they just include related topics.\n",
    "    Use a neutral, unbiased tone without exclamation marks. \n",
    "    Ensure that the heading is the same as the title in the JSON object.\n",
    "    Follow Markdown syntax for headings and formatting, and use LaTeX for equations, with inline equations in pairs of $ and multiline equations in $$.\n",
    "    Ensure the entire output is less than 3600 tokens long and does not include an extra line at the end of the Markdown.\n",
    "    '''\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def generate_completion(prompt):\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        # model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.2,\n",
    "    )\n",
    "    finish_reason = completion.choices[0]['finish_reason']\n",
    "    message = completion.choices[0].message.content\n",
    "    return finish_reason, message, completion\n",
    "\n",
    "\n",
    "def generate_json(message, topic):\n",
    "    message = message.strip()\n",
    "    json_string = re.search(r'(?s){\\s*\\\"[^\"]+\\\":\\s*{.*?}\\s*}', message, re.DOTALL)\n",
    "    \n",
    "    if json_string:\n",
    "        json_string = json_string.group()\n",
    "        # json_string = json_string.lower()\n",
    "        json_string = re.sub(r',\\s*([\\]}])', r'\\1', json_string)\n",
    "        json_object = json.loads(json_string)\n",
    "        # title_words = json_object[topic]['title'].split()\n",
    "        # result_title = [word.capitalize() if word.lower() not in stop_words else word for word in title_words]\n",
    "        # result_title = \" \".join(result_title)\n",
    "        # json_object[topic]['title'] = result_title\n",
    "        # print(json_object[topic]['title'])\n",
    "\n",
    "        # if topic not in json_string:\n",
    "        #     print(\"Error: Could not find topic in JSON.\")\n",
    "        #     exit(1)\n",
    "        with open('wiki-connections.json', 'r') as file:\n",
    "            existing_data = json.load(file)\n",
    "        existing_data.update(json_object)\n",
    "        with open('wiki-connections.json', 'w') as file:\n",
    "            json.dump(existing_data, file, indent=4)\n",
    "        return True\n",
    "    else:\n",
    "        print(\"Error: Could not extract JSON from message.\")\n",
    "        return False\n",
    "        exit(1)\n",
    "\n",
    "\n",
    "def generate_markdown(message, topic):\n",
    "    if os.path.exists('data/' + topic + '.md'):\n",
    "        print(\"Error: Markdown file already exists.\")\n",
    "        return False\n",
    "\n",
    "    message = message.strip()\n",
    "    markdown_start_pos = message.find('#')\n",
    "    markdown_content = message[markdown_start_pos:].strip() + '\\n'\n",
    "\n",
    "    md_filename = topic + '.md'\n",
    "    with open(md_filename, 'w') as file:\n",
    "        file.write(markdown_content)\n",
    "\n",
    "    destination_folder = 'data'\n",
    "    shutil.move(md_filename, destination_folder)\n",
    "    return True\n",
    "\n",
    "def generate_js(topic):\n",
    "    if os.path.exists('pages/' + topic + '.js'):\n",
    "        print(\"Error: JS file already exists.\")\n",
    "        return False\n",
    "\n",
    "    md_filename = topic + '.md'\n",
    "    js_string = f'''\n",
    "    import React from 'react';\n",
    "    import path from 'path';\n",
    "    import fs from 'fs';\n",
    "    import PageContent from '@/components/PageContent/PageContent';\n",
    "\n",
    "    const filename = '{md_filename}';\n",
    "\n",
    "    export default function MarkdownPage({{ markdownContent }}) {{\n",
    "    return <PageContent content={{markdownContent}} filename={{filename}} />;\n",
    "    }}\n",
    "\n",
    "    export async function getStaticProps() {{\n",
    "    const filePath = path.join(process.cwd(), 'data', filename);\n",
    "    const markdownContent = fs.readFileSync(filePath, 'utf8');\n",
    "    return {{\n",
    "        props: {{\n",
    "        markdownContent,\n",
    "        }},\n",
    "    }};\n",
    "    }}\n",
    "    '''\n",
    "    js_filename = topic + '.js'\n",
    "    with open(js_filename, 'w') as file:\n",
    "        file.write(js_string)\n",
    "\n",
    "    destination_folder = 'pages'\n",
    "    shutil.move(js_filename, destination_folder)\n",
    "    return True\n",
    "\n",
    "\n",
    "def extract_markdown(message):\n",
    "    markdown_start = message.find('```')\n",
    "    markdown_end = message.rfind('```')\n",
    "    markdown_string = message[markdown_start:markdown_end+3]\n",
    "    return markdown_string\n",
    "\n",
    "\n",
    "def save_visited_pages(visited_pages, file_name='visited_pages.pickle'):\n",
    "    with open(file_name, 'wb') as handle:\n",
    "        pickle.dump(visited_pages, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def load_visited_pages(file_name='visited_pages.pickle'):\n",
    "    try:\n",
    "        with open(file_name, 'rb') as handle:\n",
    "            visited_pages = pickle.load(handle)\n",
    "        return visited_pages\n",
    "    except FileNotFoundError:\n",
    "        return set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "visited_pages = load_visited_pages()\n",
    "visited_pages.update([''])\n",
    "save_visited_pages(visited_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reinforcement_learning_in_robotics', 'reinforcement_learning_in_games', 'reinforcement_learning_in_natural_language_processing']\n",
      "NOW GENERATING: reinforcement_learning_in_robotics\n",
      "FINISH_REASON: stop\n",
      "{\n",
      "    \"reinforcement_learning_in_robotics\": {\n",
      "        \"title\": \"Reinforcement Learning In Robotics\",\n",
      "        \"prerequisites\": [\"reinforcement_learning\", \"robotics\", \"deep_learning\"],\n",
      "        \"further_readings\": [\"policy_gradient_methods\", \"actor_critic_methods\", \"inverse_reinforcement_learning\", \"imitation_learning\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "# Reinforcement Learning In Robotics\n",
      "\n",
      "Reinforcement learning is a type of machine learning that involves an agent learning to make decisions in an environment in order to maximize a reward signal. Robotics is the field of study that deals with the design, construction, operation, and use of robots. Reinforcement learning in robotics involves the use of reinforcement learning algorithms to train robots to perform tasks in the real world.\n",
      "\n",
      "## Applications of Reinforcement Learning in Robotics\n",
      "\n",
      "Reinforcement learning has a wide range of applications in robotics, including but not limited to:\n",
      "\n",
      "- Autonomous navigation: Reinforcement learning can be used to train robots to navigate through complex environments without human intervention.\n",
      "- Object manipulation: Reinforcement learning can be used to train robots to manipulate objects in the real world.\n",
      "- Robotic control: Reinforcement learning can be used to train robots to perform complex control tasks, such as balancing a pole or flying a drone.\n",
      "- Human-robot interaction: Reinforcement learning can be used to train robots to interact with humans in a natural and intuitive way.\n",
      "\n",
      "## Challenges in Reinforcement Learning in Robotics\n",
      "\n",
      "Reinforcement learning in robotics poses several challenges, including but not limited to:\n",
      "\n",
      "- Safety: Reinforcement learning algorithms can cause robots to take actions that are unsafe for humans or the environment.\n",
      "- Real-world complexity: The real world is complex and unpredictable, which makes it difficult to train robots using reinforcement learning algorithms.\n",
      "- Sample efficiency: Reinforcement learning algorithms require a large number of samples to learn, which can be time-consuming and expensive in the real world.\n",
      "- Generalization: Reinforcement learning algorithms can overfit to the training data, which makes it difficult for robots to generalize to new situations.\n",
      "\n",
      "## Reinforcement Learning Algorithms for Robotics\n",
      "\n",
      "There are several reinforcement learning algorithms that are commonly used in robotics, including but not limited to:\n",
      "\n",
      "- Q-Learning: Q-Learning is a model-free reinforcement learning algorithm that learns an optimal action-value function by iteratively updating the Q-values of state-action pairs.\n",
      "- Policy Gradient Methods: Policy gradient methods are a class of model-free reinforcement learning algorithms that learn a policy directly by optimizing a performance measure.\n",
      "- Actor-Critic Methods: Actor-critic methods are a class of model-based reinforcement learning algorithms that learn both a policy and a value function by using an actor and a critic network.\n",
      "- Inverse Reinforcement Learning: Inverse reinforcement learning is a type of reinforcement learning that involves learning a reward function from expert demonstrations.\n",
      "- Imitation Learning: Imitation learning is a type of reinforcement learning that involves learning a policy from expert demonstrations.\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "Reinforcement learning in robotics is a rapidly growing field that has the potential to revolutionize the way robots are designed, built, and operated. While there are several challenges that need to be addressed, the development of new reinforcement learning algorithms and the increasing availability of real-world data are making it possible to train robots to perform complex tasks in the real world.\n",
      "DONE GENERATING: reinforcement_learning_in_robotics\n",
      "NOW GENERATING: reinforcement_learning_in_games\n",
      "FINISH_REASON: stop\n",
      "{\n",
      "    \"reinforcement_learning_in_games\": {\n",
      "        \"title\": \"Reinforcement Learning In Games\",\n",
      "        \"prerequisites\": [\"markov_decision_process\", \"q_learning\", \"deep_q_networks\"],\n",
      "        \"further_readings\": [\"monte_carlo_tree_search\", \"actor_critic_methods\", \"policy_gradient_methods\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "# Reinforcement Learning In Games\n",
      "\n",
      "Reinforcement learning is a subfield of machine learning that involves an agent learning to interact with an environment in order to maximize a cumulative reward signal. In the context of games, reinforcement learning can be used to train agents to play games at a high level without any human intervention. Reinforcement learning has been successfully applied to a wide range of games, including board games, video games, and even games of chance.\n",
      "\n",
      "## Markov Decision Process\n",
      "\n",
      "A Markov decision process (MDP) is a mathematical framework used to model decision-making in situations where outcomes are partly random and partly under the control of a decision maker. In the context of reinforcement learning, an MDP is used to model the interaction between an agent and its environment. The agent takes actions based on its current state, and the environment responds by transitioning to a new state and providing a reward signal. The goal of the agent is to learn a policy that maximizes the expected cumulative reward over time.\n",
      "\n",
      "## Q-Learning\n",
      "\n",
      "Q-learning is a popular reinforcement learning algorithm that is used to learn the optimal action-value function for an MDP. The action-value function is a function that maps a state-action pair to an expected cumulative reward. Q-learning works by iteratively updating the action-value function based on the observed rewards and transitions. The algorithm is guaranteed to converge to the optimal action-value function under certain conditions.\n",
      "\n",
      "## Deep Q-Networks\n",
      "\n",
      "Deep Q-networks (DQNs) are a type of neural network that is used to approximate the action-value function in Q-learning. DQNs have been shown to be effective at learning to play a wide range of games, including Atari games and board games. The key innovation of DQNs is the use of a deep neural network to approximate the action-value function, which allows the agent to learn more complex strategies than traditional Q-learning algorithms.\n",
      "\n",
      "## Monte Carlo Tree Search\n",
      "\n",
      "Monte Carlo tree search (MCTS) is a search algorithm that is commonly used in games to find the optimal move. MCTS works by simulating many possible games from the current state and selecting the move that leads to the highest expected reward. MCTS has been successfully applied to a wide range of games, including Go and chess.\n",
      "\n",
      "## Actor-Critic Methods\n",
      "\n",
      "Actor-critic methods are a family of reinforcement learning algorithms that combine the advantages of both policy-based and value-based methods. In actor-critic methods, the agent learns both a policy and a value function. The policy is used to select actions, while the value function is used to estimate the expected cumulative reward. Actor-critic methods have been shown to be effective at learning to play a wide range of games, including video games and board games.\n",
      "\n",
      "## Policy Gradient Methods\n",
      "\n",
      "Policy gradient methods are a family of reinforcement learning algorithms that directly optimize the policy of the agent. In policy gradient methods, the agent learns to adjust its policy in the direction of higher expected cumulative reward. Policy gradient methods have been shown to be effective at learning to play a wide range of games, including video games and board games.\n",
      "\n",
      "In conclusion, reinforcement learning has proven to be a powerful tool for training agents to play games at a high level. By using techniques such as Q-learning, DQNs, MCTS, actor-critic methods, and policy gradient methods, researchers have been able to train agents to play a wide range of games, from simple board games to complex video games. As the field of reinforcement learning continues to advance, it is likely that we will see even more impressive results in the future.\n",
      "DONE GENERATING: reinforcement_learning_in_games\n",
      "NOW GENERATING: reinforcement_learning_in_natural_language_processing\n",
      "FINISH_REASON: stop\n",
      "{\n",
      "    \"reinforcement_learning_in_natural_language_processing\": {\n",
      "        \"title\": \"Reinforcement Learning In Natural Language Processing\",\n",
      "        \"prerequisites\": [\"reinforcement_learning\", \"natural_language_processing\", \"deep_learning\"],\n",
      "        \"further_readings\": [\"policy_gradient_methods\", \"word_embeddings\", \"sequence_to_sequence_models\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "# Reinforcement Learning In Natural Language Processing\n",
      "\n",
      "Reinforcement learning (RL) is a subfield of machine learning (ML) that deals with decision-making in an environment. Natural language processing (NLP) is a subfield of artificial intelligence (AI) that deals with the interaction between computers and human language. Reinforcement learning in natural language processing is the application of RL techniques to NLP problems.\n",
      "\n",
      "In reinforcement learning, an agent learns to take actions in an environment to maximize a reward signal. The agent interacts with the environment by taking actions and receiving feedback in the form of rewards. The goal of the agent is to learn a policy that maximizes the expected cumulative reward over time.\n",
      "\n",
      "In natural language processing, the goal is to enable computers to understand and generate human language. NLP tasks include language translation, sentiment analysis, and question answering. Reinforcement learning can be applied to NLP tasks to learn policies that maximize the expected reward.\n",
      "\n",
      "One example of reinforcement learning in NLP is the use of RL to learn word embeddings. Word embeddings are vector representations of words that capture their semantic meaning. In RL, an agent can learn to generate word embeddings that maximize the reward signal, such as improving the accuracy of a language model.\n",
      "\n",
      "Another example of reinforcement learning in NLP is the use of RL to learn sequence-to-sequence models. Sequence-to-sequence models are used for tasks such as machine translation and text summarization. In RL, an agent can learn to generate sequences that maximize the reward signal, such as improving the fluency and coherence of the generated text.\n",
      "\n",
      "Policy gradient methods are a popular RL technique for NLP tasks. Policy gradient methods learn a policy by directly optimizing the expected cumulative reward. They have been successfully applied to tasks such as machine translation and dialogue generation.\n",
      "\n",
      "In conclusion, reinforcement learning can be a powerful tool for solving NLP problems. By learning policies that maximize the expected reward, RL agents can improve the accuracy and fluency of NLP models. Further research in this area is needed to explore the full potential of reinforcement learning in natural language processing.\n",
      "DONE GENERATING: reinforcement_learning_in_natural_language_processing\n",
      "NOW GENERATING: markov_chain\n",
      "FINISH_REASON: stop\n",
      "{\n",
      "    \"markov_chain\": {\n",
      "        \"title\": \"Markov Chain\",\n",
      "        \"prerequisites\": [\"probability_theory\", \"matrix_multiplication\"],\n",
      "        \"further_readings\": [\"hidden_markov_models\", \"monte_carlo_methods\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "# Markov Chain\n",
      "\n",
      "A Markov chain is a mathematical model that describes a sequence of events where the probability of each event depends only on the state attained in the previous event. It is a stochastic process that is used to model a wide range of phenomena in various fields such as physics, chemistry, economics, and computer science.\n",
      "\n",
      "## Definition\n",
      "\n",
      "A Markov chain is a sequence of random variables $X_1, X_2, X_3, ...$ where each variable $X_i$ takes values from a finite or countably infinite set of states $S$. The probability of moving from one state to another is given by a transition probability matrix $P$, where $P_{ij}$ is the probability of moving from state $i$ to state $j$.\n",
      "\n",
      "A Markov chain is said to be **homogeneous** if the transition probabilities do not depend on the time step $n$. In this case, the transition probability matrix $P$ is constant over time.\n",
      "\n",
      "## Properties\n",
      "\n",
      "### State Space\n",
      "\n",
      "The set of all possible states in a Markov chain is called the **state space**. It can be finite or countably infinite.\n",
      "\n",
      "### Transition Matrix\n",
      "\n",
      "The transition matrix $P$ is a square matrix where each row represents the probabilities of moving from one state to all other states. The sum of probabilities in each row is equal to 1.\n",
      "\n",
      "### Stationary Distribution\n",
      "\n",
      "A stationary distribution is a probability distribution that remains unchanged over time. In a Markov chain, a stationary distribution is a probability distribution $\\pi$ such that $\\pi = \\pi P$. That is, if the Markov chain starts in the stationary distribution, it will remain in the same distribution over time.\n",
      "\n",
      "### Irreducibility\n",
      "\n",
      "A Markov chain is said to be **irreducible** if it is possible to reach any state from any other state with a positive probability.\n",
      "\n",
      "### Periodicity\n",
      "\n",
      "A state $i$ in a Markov chain is said to be **periodic** if the chain can return to state $i$ only at multiples of some integer $d > 1$. If $d = 1$, the state is said to be **aperiodic**.\n",
      "\n",
      "### Ergodicity\n",
      "\n",
      "A Markov chain is said to be **ergodic** if it is irreducible and aperiodic. In this case, the chain has a unique stationary distribution, and the distribution of the chain converges to the stationary distribution as the number of time steps goes to infinity.\n",
      "\n",
      "## Applications\n",
      "\n",
      "Markov chains are used in various fields to model a wide range of phenomena. Some examples include:\n",
      "\n",
      "- **Queueing systems**: Markov chains can be used to model the behavior of customers in a queueing system, where the state represents the number of customers in the system.\n",
      "- **Language modeling**: Markov chains can be used to model the probability of a sequence of words in a language, where the state represents the current word and the transition probabilities represent the probability of the next word given the current word.\n",
      "- **Image processing**: Markov chains can be used to model the spatial relationships between pixels in an image, where the state represents the color or intensity of a pixel and the transition probabilities represent the probability of a pixel taking a certain color or intensity given its neighbors.\n",
      "\n",
      "## Further Readings\n",
      "\n",
      "- Hidden Markov Models\n",
      "- Monte Carlo Methods\n",
      "DONE GENERATING: markov_chain\n",
      "NOW GENERATING: partially_observable_markov_decision_process\n",
      "FINISH_REASON: stop\n",
      "{\n",
      "    \"partially_observable_markov_decision_process\": {\n",
      "        \"title\": \"Partially Observable Markov Decision Process\",\n",
      "        \"prerequisites\": [\"markov_decision_process\", \"hidden_markov_model\"],\n",
      "        \"further_readings\": [\"reinforcement_learning_an_introduction\", \"probabilistic_robotics\", \"partially_observable_markov_decision_processes_algorithms_and_applications\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "# Partially Observable Markov Decision Process\n",
      "\n",
      "A Partially Observable Markov Decision Process (POMDP) is a mathematical framework used to model decision-making problems where the state of the system is not fully observable. POMDPs are an extension of Markov Decision Processes (MDPs) and are used in various fields such as robotics, control theory, and economics.\n",
      "\n",
      "## Definition\n",
      "\n",
      "A POMDP is defined by a tuple $(S, A, T, R, Z, O, \\gamma)$, where:\n",
      "\n",
      "- $S$ is a finite set of states.\n",
      "- $A$ is a finite set of actions.\n",
      "- $T$ is a set of transition probabilities $T(s,a,s') = P(s'|s,a)$.\n",
      "- $R$ is a reward function $R(s,a)$.\n",
      "- $Z$ is a set of observation probabilities $Z(o|s,a)$.\n",
      "- $O$ is a set of observations.\n",
      "- $\\gamma$ is a discount factor.\n",
      "\n",
      "The difference between a POMDP and an MDP is that in a POMDP, the agent does not directly observe the state of the system. Instead, the agent receives an observation that is probabilistically related to the underlying state. The observation function $Z(o|s,a)$ specifies the probability of observing $o$ given that the system is in state $s$ and the agent takes action $a$.\n",
      "\n",
      "## Belief State\n",
      "\n",
      "Since the agent does not know the true state of the system, it maintains a belief state $b_t$, which is a probability distribution over the states $S$ at time $t$. The belief state is updated using Bayes' rule:\n",
      "\n",
      "$$\n",
      "b_{t+1}(s') = \\frac{Z(o_{t+1}|s',a) \\sum_{s \\in S} T(s,a,s')b_t(s)}{\\sum_{s'' \\in S} Z(o_{t+1}|s'',a) \\sum_{s \\in S} T(s,a,s'')b_t(s)}\n",
      "$$\n",
      "\n",
      "where $o_{t+1}$ is the observation received at time $t+1$.\n",
      "\n",
      "## Policy\n",
      "\n",
      "The goal of the agent is to maximize the expected sum of discounted rewards:\n",
      "\n",
      "$$\n",
      "\\sum_{t=0}^\\infty \\gamma^t R(s_t,a_t)\n",
      "$$\n",
      "\n",
      "where $s_t$ is the state at time $t$ and $a_t$ is the action taken at time $t$. Since the agent does not know the true state of the system, it must choose actions based on its belief state. A policy $\\pi(b)$ maps a belief state to a probability distribution over actions.\n",
      "\n",
      "## Solving POMDPs\n",
      "\n",
      "Solving POMDPs is computationally expensive since the belief state is a probability distribution over the states. Exact solutions are only possible for small problems. Approximate solutions include:\n",
      "\n",
      "- **Value Iteration**: This algorithm iteratively computes the optimal value function for a given policy. The optimal policy is then obtained by selecting the action that maximizes the value function at each belief state.\n",
      "- **Policy Iteration**: This algorithm iteratively improves a given policy by computing the optimal value function for the policy and then updating the policy based on the value function.\n",
      "- **Monte Carlo Tree Search**: This algorithm builds a search tree by simulating trajectories from the current belief state. The tree is then used to select actions that maximize the expected sum of discounted rewards.\n",
      "\n",
      "## Applications\n",
      "\n",
      "POMDPs are used in various fields such as robotics, control theory, and economics. In robotics, POMDPs are used to model problems such as robot localization and navigation. In control theory, POMDPs are used to model problems such as optimal control of stochastic systems. In economics, POMDPs are used to model problems such as decision-making under uncertainty.\n",
      "\n",
      "## Further Readings\n",
      "\n",
      "- Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction. MIT press.\n",
      "- Thrun, S., Burgard, W., & Fox, D. (2005). Probabilistic robotics. MIT press.\n",
      "- Kochenderfer, M. J., & Wheeler, T. A. (2019). Partially observable markov decision processes: Algorithms and applications. Morgan & Claypool.\n",
      "DONE GENERATING: partially_observable_markov_decision_process\n",
      "NOW GENERATING: value_iteration_algorithm\n",
      "FINISH_REASON: stop\n",
      "{\n",
      "    \"value_iteration_algorithm\": {\n",
      "        \"title\": \"Value Iteration Algorithm\",\n",
      "        \"prerequisites\": [\"bellman_equations\", \"markov_decision_process\", \"dynamic_programming\"],\n",
      "        \"further_readings\": [\"q_learning\", \"policy_iteration_algorithm\", \"monte_carlo_tree_search\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "# Value Iteration Algorithm\n",
      "\n",
      "The **Value Iteration Algorithm** is a dynamic programming algorithm used to compute the optimal value function and policy for a Markov Decision Process (MDP). It is an iterative algorithm that starts with an initial estimate of the value function and updates it in each iteration until convergence. The algorithm is guaranteed to converge to the optimal value function and policy for the given MDP.\n",
      "\n",
      "## Algorithm\n",
      "\n",
      "The algorithm starts with an initial estimate of the value function, denoted by V_0. In each iteration, it updates the value function using the Bellman optimality equation:\n",
      "\n",
      "$$V_{k+1}(s) = \\max_{a \\in A} \\sum_{s' \\in S} P(s'|s,a)[R(s,a,s') + \\gamma V_k(s')]$$\n",
      "\n",
      "where V_k is the value function at iteration k, s is the current state, a is the action taken in state s, s' is the next state, P(s'|s,a) is the transition probability from state s to state s' under action a, R(s,a,s') is the reward obtained for transitioning from state s to state s' under action a, and \\gamma is the discount factor.\n",
      "\n",
      "The algorithm continues to iterate until the difference between the value function in two consecutive iterations is less than a predefined threshold, denoted by \\epsilon.\n",
      "\n",
      "Once the algorithm converges, the optimal policy can be obtained by selecting the action that maximizes the right-hand side of the Bellman optimality equation for each state:\n",
      "\n",
      "$$\\pi^*(s) = \\arg\\max_{a \\in A} \\sum_{s' \\in S} P(s'|s,a)[R(s,a,s') + \\gamma V^*(s')]$$\n",
      "\n",
      "where V^* is the optimal value function.\n",
      "\n",
      "## Applications\n",
      "\n",
      "The Value Iteration Algorithm is widely used in various fields, including robotics, game theory, and finance. It is used to solve problems that can be modeled as MDPs, such as robot navigation, game playing, and portfolio optimization.\n",
      "\n",
      "## Further Readings\n",
      "\n",
      "- **Q-Learning**: A reinforcement learning algorithm that learns the optimal policy without knowing the transition probabilities and rewards of the MDP.\n",
      "- **Policy Iteration Algorithm**: An iterative algorithm that alternates between policy evaluation and policy improvement to find the optimal policy for a given MDP.\n",
      "- **Monte Carlo Tree Search**: A heuristic search algorithm that uses random simulations to find the optimal action in a large search space.\n",
      "DONE GENERATING: value_iteration_algorithm\n",
      "NOW GENERATING: reinforcement_learning_algorithms\n",
      "FINISH_REASON: stop\n",
      "{\n",
      "    \"reinforcement_learning_algorithms\": {\n",
      "        \"title\": \"Reinforcement Learning Algorithms\",\n",
      "        \"prerequisites\": [\"markov_decision_process\", \"q_learning\", \"policy_gradient_methods\"],\n",
      "        \"further_readings\": [\"actor_critic_methods\", \"deep_reinforcement_learning\", \"monte_carlo_methods\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "# Reinforcement Learning Algorithms\n",
      "\n",
      "Reinforcement learning is a type of machine learning that involves an agent learning to interact with an environment in order to maximize a reward signal. Reinforcement learning algorithms are used to train these agents to make decisions based on the rewards they receive. \n",
      "\n",
      "## Markov Decision Process\n",
      "\n",
      "A Markov Decision Process (MDP) is a mathematical framework used to model decision-making processes in which outcomes are partly random and partly under the control of a decision maker. MDPs are used in reinforcement learning to model the environment in which the agent operates. \n",
      "\n",
      "## Q-Learning\n",
      "\n",
      "Q-Learning is a model-free reinforcement learning algorithm that learns to make decisions by estimating the value of taking a particular action in a particular state. The Q-value of a state-action pair is the expected reward that the agent will receive by taking that action in that state. \n",
      "\n",
      "## Policy Gradient Methods\n",
      "\n",
      "Policy Gradient Methods are a class of reinforcement learning algorithms that learn a policy directly, without estimating the value function. These algorithms optimize the policy by directly maximizing the expected reward. \n",
      "\n",
      "## Actor-Critic Methods\n",
      "\n",
      "Actor-Critic Methods are a class of reinforcement learning algorithms that combine the advantages of both policy gradient methods and value-based methods. These algorithms use two separate networks: one to learn the policy and another to estimate the value function. \n",
      "\n",
      "## Deep Reinforcement Learning\n",
      "\n",
      "Deep Reinforcement Learning is a type of reinforcement learning that uses deep neural networks to approximate the value function or policy. These algorithms have been successful in solving complex tasks, such as playing video games and controlling robots. \n",
      "\n",
      "## Monte Carlo Methods\n",
      "\n",
      "Monte Carlo Methods are a class of reinforcement learning algorithms that estimate the value function by averaging the returns received from many episodes. These algorithms do not require a model of the environment and are often used in situations where the dynamics of the environment are unknown. \n",
      "\n",
      "Reinforcement learning algorithms are used in a wide range of applications, including robotics, game playing, and autonomous driving. By learning to make decisions based on the rewards they receive, these algorithms can be used to solve complex problems that would be difficult or impossible to solve using traditional programming techniques.\n",
      "DONE GENERATING: reinforcement_learning_algorithms\n",
      "NOW GENERATING: recursion\n",
      "FINISH_REASON: stop\n",
      "{\n",
      "    \"recursion\": {\n",
      "        \"title\": \"Recursion\",\n",
      "        \"prerequisites\": [\"functions\", \"data_structures\", \"control_flow\"],\n",
      "        \"further_readings\": [\"tail_recursion\", \"recursive_data_structures\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "# Recursion\n",
      "\n",
      "Recursion is a programming technique where a function calls itself to solve a problem. It is a powerful tool in computer science and is used in many algorithms and data structures. Recursion is a fundamental concept in programming and is used in many programming languages.\n",
      "\n",
      "## How Recursion Works\n",
      "\n",
      "Recursion works by breaking down a problem into smaller sub-problems and solving them recursively. A recursive function calls itself with a smaller input until it reaches a base case, which is a problem that can be solved without recursion. The base case is the stopping condition for the recursion.\n",
      "\n",
      "For example, consider the problem of computing the factorial of a number. The factorial of a number is the product of all the integers from 1 to that number. The factorial of 5 is 5 * 4 * 3 * 2 * 1 = 120. We can compute the factorial of a number recursively as follows:\n",
      "\n",
      "```\n",
      "function factorial(n):\n",
      "    if n == 0:\n",
      "        return 1\n",
      "    else:\n",
      "        return n * factorial(n-1)\n",
      "```\n",
      "\n",
      "In this example, the base case is when n is 0, and the function returns 1. Otherwise, the function calls itself with n-1 as the input and multiplies the result by n.\n",
      "\n",
      "## Advantages and Disadvantages of Recursion\n",
      "\n",
      "Recursion has several advantages over iterative solutions. It can be more concise and easier to understand for some problems. It can also be more elegant and efficient for some algorithms. Recursion is also useful for solving problems that have a recursive structure, such as tree traversal and graph traversal.\n",
      "\n",
      "However, recursion can also have some disadvantages. It can be less efficient than iterative solutions for some problems, especially if the recursion depth is large. Recursion can also be more difficult to debug and can lead to stack overflow errors if not implemented correctly.\n",
      "\n",
      "## Tail Recursion\n",
      "\n",
      "Tail recursion is a special case of recursion where the recursive call is the last operation in the function. In tail recursion, the function does not need to keep track of the previous state, and the compiler can optimize the code to use a loop instead of recursion. Tail recursion can be more efficient than regular recursion and can avoid stack overflow errors.\n",
      "\n",
      "## Recursive Data Structures\n",
      "\n",
      "Recursive data structures are data structures that contain references to themselves. Examples of recursive data structures include linked lists, trees, and graphs. Recursive data structures can be defined recursively, and recursive algorithms can be used to traverse and manipulate them.\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "Recursion is a powerful programming technique that can be used to solve many problems. It works by breaking down a problem into smaller sub-problems and solving them recursively. Recursion has advantages and disadvantages and can be used in conjunction with other programming techniques. Tail recursion and recursive data structures are special cases of recursion that have their own advantages and disadvantages.\n",
      "DONE GENERATING: recursion\n",
      "NOW GENERATING: memoization\n",
      "FINISH_REASON: stop\n",
      "{\n",
      "    \"memoization\": {\n",
      "        \"title\": \"Memoization\",\n",
      "        \"prerequisites\": [\"dynamic_programming\"],\n",
      "        \"further_readings\": [\"tabulation\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "# Memoization\n",
      "\n",
      "Memoization is a technique used in computer science to speed up the execution time of programs by storing the results of expensive function calls and returning the cached result when the same inputs occur again. This technique is used in dynamic programming to avoid redundant computations.\n",
      "\n",
      "## How it Works\n",
      "\n",
      "Memoization works by storing the result of a function call in a cache, usually a dictionary, with the input arguments as the key. When the function is called again with the same input arguments, the cached result is returned instead of recomputing the function. This can significantly reduce the execution time of the program, especially for functions with expensive computations.\n",
      "\n",
      "## Advantages and Disadvantages\n",
      "\n",
      "The main advantage of memoization is that it can significantly reduce the execution time of programs by avoiding redundant computations. This is especially useful for functions with expensive computations or functions that are called frequently with the same input arguments.\n",
      "\n",
      "However, memoization can also have some disadvantages. The cache can consume a lot of memory, especially if the function has a large number of possible input arguments. Additionally, if the function has side effects, such as modifying global variables or performing I/O operations, memoization can cause unexpected behavior.\n",
      "\n",
      "## Example\n",
      "\n",
      "Consider the following recursive function to calculate the nth Fibonacci number:\n",
      "\n",
      "```python\n",
      "def fibonacci(n):\n",
      "    if n <= 1:\n",
      "        return n\n",
      "    else:\n",
      "        return fibonacci(n-1) + fibonacci(n-2)\n",
      "```\n",
      "\n",
      "This function has an exponential time complexity, as it recursively computes the same values multiple times. However, we can use memoization to avoid redundant computations and reduce the time complexity to linear:\n",
      "\n",
      "```python\n",
      "cache = {}\n",
      "\n",
      "def fibonacci(n):\n",
      "    if n in cache:\n",
      "        return cache[n]\n",
      "    elif n <= 1:\n",
      "        return n\n",
      "    else:\n",
      "        result = fibonacci(n-1) + fibonacci(n-2)\n",
      "        cache[n] = result\n",
      "        return result\n",
      "```\n",
      "\n",
      "In this version of the function, we use a cache to store the results of previous function calls. If the result for a given input argument is already in the cache, we return the cached result instead of recomputing the function. Otherwise, we compute the result and store it in the cache for future use.\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "Memoization is a powerful technique for reducing the execution time of programs by avoiding redundant computations. It is commonly used in dynamic programming to solve optimization problems. However, it is important to be aware of the potential disadvantages of memoization, such as increased memory usage and unexpected behavior for functions with side effects.\n",
      "DONE GENERATING: memoization\n",
      "NOW GENERATING: divide_and_conquer\n",
      "FINISH_REASON: stop\n",
      "{\n",
      "    \"divide_and_conquer\": {\n",
      "        \"title\": \"Divide and Conquer\",\n",
      "        \"prerequisites\": [\"recursion\", \"algorithm_analysis\"],\n",
      "        \"further_readings\": [\"merge_sort\", \"quick_sort\", \"binary_search\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "# Divide and Conquer\n",
      "\n",
      "Divide and Conquer is a problem-solving strategy that involves breaking down a problem into smaller sub-problems, solving each sub-problem independently, and then combining the solutions to the sub-problems to solve the original problem. This approach is often used in computer science and mathematics to solve complex problems efficiently.\n",
      "\n",
      "## Algorithm\n",
      "\n",
      "The Divide and Conquer algorithm can be broken down into three steps:\n",
      "\n",
      "1. **Divide**: The problem is divided into smaller sub-problems that are similar to the original problem but smaller in size.\n",
      "\n",
      "2. **Conquer**: Each sub-problem is solved independently using the same algorithm. This step is typically done recursively until the sub-problems become simple enough to be solved directly.\n",
      "\n",
      "3. **Combine**: The solutions to the sub-problems are combined to solve the original problem.\n",
      "\n",
      "## Applications\n",
      "\n",
      "Divide and Conquer is a widely used problem-solving strategy in computer science and mathematics. Some common applications include:\n",
      "\n",
      "- Sorting algorithms: Merge Sort and Quick Sort are examples of sorting algorithms that use the Divide and Conquer strategy to sort a list of elements efficiently.\n",
      "\n",
      "- Searching algorithms: Binary Search is an example of a searching algorithm that uses the Divide and Conquer strategy to search for an element in a sorted list of elements.\n",
      "\n",
      "- Matrix multiplication: The Divide and Conquer strategy can be used to multiply two matrices efficiently.\n",
      "\n",
      "- Closest pair of points: The Divide and Conquer strategy can be used to find the closest pair of points in a set of points.\n",
      "\n",
      "## Complexity Analysis\n",
      "\n",
      "The time complexity of a Divide and Conquer algorithm can be analyzed using the Master Theorem. The Master Theorem provides a formula for the time complexity of a Divide and Conquer algorithm based on the size of the problem and the time complexity of the sub-problems.\n",
      "\n",
      "The formula for the time complexity of a Divide and Conquer algorithm is:\n",
      "\n",
      "$$T(n) = aT(\\frac{n}{b}) + f(n)$$\n",
      "\n",
      "where:\n",
      "\n",
      "- T(n) is the time complexity of the algorithm for a problem of size n.\n",
      "- a is the number of sub-problems.\n",
      "- b is the size of each sub-problem.\n",
      "- f(n) is the time complexity of the algorithm for a problem of size n that is not spent on dividing and combining.\n",
      "\n",
      "The time complexity of the algorithm can be determined by comparing f(n) to n^log_b(a). There are three cases:\n",
      "\n",
      "1. If f(n) = O(n^log_b(a - ε)) for some ε > 0, then the time complexity of the algorithm is O(n^log_b(a)).\n",
      "2. If f(n) = Θ(n^log_b(a)), then the time complexity of the algorithm is O(n^log_b(a) * log n).\n",
      "3. If f(n) = Ω(n^log_b(a + ε)) for some ε > 0 and if a * f(n/b) ≤ c * f(n) for some constant c < 1 and all sufficiently large n, then the time complexity of the algorithm is O(f(n)).\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "Divide and Conquer is a powerful problem-solving strategy that is widely used in computer science and mathematics. It involves breaking down a problem into smaller sub-problems, solving each sub-problem independently, and then combining the solutions to the sub-problems to solve the original problem. The time complexity of a Divide and Conquer algorithm can be analyzed using the Master Theorem.\n",
      "DONE GENERATING: divide_and_conquer\n",
      "NOW GENERATING: greedy_algorithms\n",
      "FINISH_REASON: stop\n",
      "{\n",
      "    \"greedy_algorithms\": {\n",
      "        \"title\": \"Greedy Algorithms\",\n",
      "        \"prerequisites\": [\"dynamic_programming\", \"graph_theory\"],\n",
      "        \"further_readings\": [\"minimum_spanning_trees\", \"shortest_path_algorithms\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "# Greedy Algorithms\n",
      "\n",
      "A greedy algorithm is a type of algorithmic paradigm that follows the problem-solving heuristic of making the locally optimal choice at each stage with the hope of finding a global optimum. In other words, a greedy algorithm never reconsiders its choices. This property makes them very efficient but not always optimal.\n",
      "\n",
      "Greedy algorithms are used in many optimization problems, such as finding the shortest path between two points, finding the minimum spanning tree of a graph, and finding the optimal Huffman code for a set of characters.\n",
      "\n",
      "## Properties\n",
      "\n",
      "Greedy algorithms have the following properties:\n",
      "\n",
      "- They are simple to implement and easy to understand.\n",
      "- They are efficient in terms of time complexity.\n",
      "- They do not always provide the optimal solution.\n",
      "\n",
      "## Examples\n",
      "\n",
      "### Minimum Spanning Tree\n",
      "\n",
      "A minimum spanning tree is a tree that spans all the vertices of a connected, undirected graph with the minimum possible total edge weight. Kruskal's algorithm and Prim's algorithm are two examples of greedy algorithms that can be used to find the minimum spanning tree of a graph.\n",
      "\n",
      "### Shortest Path\n",
      "\n",
      "The shortest path problem is the problem of finding a path between two vertices in a graph such that the sum of the weights of its constituent edges is minimized. Dijkstra's algorithm and Bellman-Ford algorithm are two examples of greedy algorithms that can be used to find the shortest path between two vertices in a graph.\n",
      "\n",
      "## Limitations\n",
      "\n",
      "Greedy algorithms do not always provide the optimal solution. In some cases, they may provide a solution that is close to optimal, but in other cases, they may provide a solution that is far from optimal. Therefore, it is important to carefully analyze the problem and determine whether a greedy algorithm is appropriate.\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "Greedy algorithms are a powerful tool in the field of computer science. They are simple to implement, efficient, and can be used to solve a wide range of optimization problems. However, they do have limitations and may not always provide the optimal solution. Therefore, it is important to carefully analyze the problem and determine whether a greedy algorithm is appropriate.\n",
      "DONE GENERATING: greedy_algorithms\n",
      "NOW GENERATING: graph_theory\n",
      "FINISH_REASON: stop\n",
      "{\n",
      "    \"graph_theory\": {\n",
      "        \"title\": \"Graph Theory\",\n",
      "        \"prerequisites\": [\"discrete_mathematics\", \"set_theory\", \"combinatorics\"],\n",
      "        \"further_readings\": [\"network_flow\", \"random_walks_on_graphs\", \"spectral_graph_theory\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "# Graph Theory\n",
      "\n",
      "Graph theory is a branch of mathematics that studies the properties of graphs, which are mathematical structures used to model pairwise relations between objects. A graph consists of a set of vertices (also called nodes) and a set of edges (also called links or arcs) that connect pairs of vertices. Graph theory has many applications in computer science, operations research, social sciences, and other fields.\n",
      "\n",
      "## Basic Concepts\n",
      "\n",
      "### Graphs\n",
      "\n",
      "A graph is a pair $G = (V, E)$, where $V$ is a set of vertices and $E$ is a set of edges. An edge is an unordered pair of vertices. A graph is said to be undirected if its edges are unordered pairs, and directed if its edges are ordered pairs. A graph is said to be simple if it has no loops (edges that connect a vertex to itself) and no multiple edges (two or more edges that connect the same pair of vertices).\n",
      "\n",
      "### Paths and Cycles\n",
      "\n",
      "A path in a graph is a sequence of vertices connected by edges. A cycle is a path that starts and ends at the same vertex. A graph is said to be connected if there is a path between any two vertices. A graph is said to be acyclic if it has no cycles.\n",
      "\n",
      "### Degrees and Adjacency\n",
      "\n",
      "The degree of a vertex in a graph is the number of edges incident to it. The degree sequence of a graph is the sequence of degrees of its vertices. The adjacency matrix of a graph is a square matrix that represents its edges. The entry $a_{ij}$ is 1 if there is an edge from vertex $i$ to vertex $j$, and 0 otherwise.\n",
      "\n",
      "## Advanced Concepts\n",
      "\n",
      "### Trees and Forests\n",
      "\n",
      "A tree is a connected acyclic graph. A forest is a disjoint union of trees. A spanning tree of a connected graph is a subgraph that is a tree and contains all the vertices of the graph.\n",
      "\n",
      "### Planar Graphs\n",
      "\n",
      "A planar graph is a graph that can be drawn on a plane without any edges crossing. The Euler's formula states that for any connected planar graph, $V - E + F = 2$, where $V$ is the number of vertices, $E$ is the number of edges, and $F$ is the number of faces (regions bounded by edges).\n",
      "\n",
      "### Graph Coloring\n",
      "\n",
      "A vertex coloring of a graph is an assignment of colors to its vertices such that no two adjacent vertices have the same color. The chromatic number of a graph is the minimum number of colors needed for a vertex coloring. The chromatic polynomial of a graph is a polynomial that counts the number of vertex colorings of the graph.\n",
      "\n",
      "### Graph Algorithms\n",
      "\n",
      "Many important algorithms in computer science are based on graph theory, such as Dijkstra's algorithm for finding shortest paths in a graph, the Ford-Fulkerson algorithm for computing maximum flows in a network, and the PageRank algorithm for ranking web pages based on their links.\n",
      "\n",
      "## Applications\n",
      "\n",
      "Graph theory has many applications in various fields, such as:\n",
      "\n",
      "- Computer networks: Graphs are used to model the topology of computer networks and to design routing protocols.\n",
      "- Social networks: Graphs are used to model social networks and to analyze their properties, such as centrality and clustering.\n",
      "- Operations research: Graphs are used to model transportation networks, supply chains, and scheduling problems.\n",
      "- Chemistry: Graphs are used to model molecules and to study their properties, such as symmetry and stability.\n",
      "\n",
      "## Further Readings\n",
      "\n",
      "- Network Flow\n",
      "- Random Walks on Graphs\n",
      "- Spectral Graph Theory\n",
      "DONE GENERATING: graph_theory\n",
      "NOW GENERATING: data_structures\n",
      "FINISH_REASON: stop\n",
      "{\n",
      "    \"data_structures\": {\n",
      "        \"title\": \"Data Structures\",\n",
      "        \"prerequisites\": [\"arrays\", \"linked_lists\", \"stacks\", \"queues\"],\n",
      "        \"further_readings\": [\"binary_trees\", \"hash_tables\", \"graph_data_structures\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "# Data Structures\n",
      "\n",
      "Data structures are a fundamental concept in computer science and are essential for efficient and effective programming. A data structure is a way of organizing and storing data in a computer so that it can be accessed and used efficiently. \n",
      "\n",
      "## Arrays\n",
      "\n",
      "Arrays are a collection of elements of the same data type. They are stored in contiguous memory locations and can be accessed using an index. Arrays are useful for storing and accessing large amounts of data, but their size is fixed, and it can be challenging to insert or delete elements.\n",
      "\n",
      "## Linked Lists\n",
      "\n",
      "Linked lists are a collection of elements, each containing a value and a pointer to the next element in the list. They are useful for dynamic data structures, where the size of the data can change during runtime. Linked lists are efficient for inserting and deleting elements, but accessing elements can be slower than arrays.\n",
      "\n",
      "## Stacks\n",
      "\n",
      "Stacks are a collection of elements that follow the Last-In-First-Out (LIFO) principle. Elements can be added or removed only from the top of the stack. Stacks are useful for implementing algorithms such as depth-first search and backtracking.\n",
      "\n",
      "## Queues\n",
      "\n",
      "Queues are a collection of elements that follow the First-In-First-Out (FIFO) principle. Elements can be added to the back of the queue and removed from the front. Queues are useful for implementing algorithms such as breadth-first search and job scheduling.\n",
      "\n",
      "## Binary Trees\n",
      "\n",
      "Binary trees are a hierarchical data structure that consists of nodes, each containing a value and two pointers to its left and right child nodes. Binary trees are useful for searching and sorting algorithms, and they have a time complexity of O(log n) for most operations.\n",
      "\n",
      "## Hash Tables\n",
      "\n",
      "Hash tables are a data structure that uses a hash function to map keys to their corresponding values. Hash tables are useful for fast retrieval of data, and they have a time complexity of O(1) for most operations.\n",
      "\n",
      "## Graph Data Structures\n",
      "\n",
      "Graph data structures are a collection of nodes and edges that connect them. Graphs are useful for modeling relationships between objects, and they are used in many applications such as social networks and routing algorithms.\n",
      "\n",
      "In conclusion, understanding data structures is essential for efficient and effective programming. Different data structures have their advantages and disadvantages, and choosing the right one for a particular problem is crucial for optimal performance.\n",
      "DONE GENERATING: data_structures\n",
      "NOW GENERATING: algorithms\n",
      "FINISH_REASON: stop\n",
      "{\n",
      "    \"algorithms\": {\n",
      "        \"title\": \"Algorithms\",\n",
      "        \"prerequisites\": [\"data_structures\", \"complexity_theory\", \"graph_theory\"],\n",
      "        \"further_readings\": [\"machine_learning_algorithms\", \"optimization_algorithms\", \"search_algorithms\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "# Algorithms\n",
      "\n",
      "Algorithms are a set of instructions or rules that are used to solve a problem or perform a specific task. In computer science, algorithms are used to solve a wide range of problems, from simple arithmetic calculations to complex optimization problems. They are an essential part of computer science and are used in various fields, including artificial intelligence (AI), machine learning (ML), and deep learning (DL).\n",
      "\n",
      "## Types of Algorithms\n",
      "\n",
      "There are various types of algorithms, including:\n",
      "\n",
      "- **Sorting Algorithms**: These algorithms are used to sort a list of items in a specific order, such as alphabetical or numerical order. Examples of sorting algorithms include bubble sort, merge sort, and quicksort.\n",
      "\n",
      "- **Search Algorithms**: These algorithms are used to search for a specific item in a list of items. Examples of search algorithms include linear search and binary search.\n",
      "\n",
      "- **Graph Algorithms**: These algorithms are used to solve problems related to graphs, such as finding the shortest path between two nodes or detecting cycles in a graph. Examples of graph algorithms include Dijkstra's algorithm and Bellman-Ford algorithm.\n",
      "\n",
      "- **Optimization Algorithms**: These algorithms are used to find the optimal solution to a problem, such as minimizing the cost of a production process or maximizing the profit of a business. Examples of optimization algorithms include linear programming and gradient descent.\n",
      "\n",
      "## Applications of Algorithms in AI, ML, and DL\n",
      "\n",
      "Algorithms are an essential part of AI, ML, and DL. They are used to train models, make predictions, and optimize performance. Some of the most commonly used algorithms in these fields include:\n",
      "\n",
      "- **Neural Networks**: These are a type of ML algorithm that is inspired by the structure and function of the human brain. They are used for tasks such as image recognition, natural language processing, and speech recognition.\n",
      "\n",
      "- **Decision Trees**: These are a type of ML algorithm that is used for classification and regression tasks. They are often used in medical diagnosis, credit scoring, and fraud detection.\n",
      "\n",
      "- **Support Vector Machines**: These are a type of ML algorithm that is used for classification and regression tasks. They are often used in image classification, text classification, and bioinformatics.\n",
      "\n",
      "- **Reinforcement Learning**: This is a type of ML algorithm that is used for decision-making tasks. It is often used in robotics, game playing, and autonomous vehicles.\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "Algorithms are an essential part of computer science and are used in various fields, including AI, ML, and DL. They are used to solve a wide range of problems, from simple arithmetic calculations to complex optimization problems. Understanding algorithms is crucial for anyone working in these fields, as it enables them to develop more efficient and effective solutions to problems.\n",
      "DONE GENERATING: algorithms\n",
      "NOW GENERATING: bellman_ford_algorithm\n",
      "FINISH_REASON: stop\n",
      "{\n",
      "    \"bellman_ford_algorithm\": {\n",
      "        \"title\": \"Bellman Ford Algorithm\",\n",
      "        \"prerequisites\": [\"graph_theory\", \"shortest_path_algorithms\"],\n",
      "        \"further_readings\": [\"dijkstra_algorithm\", \"floyd_warshall_algorithm\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "# Bellman Ford Algorithm\n",
      "\n",
      "The Bellman Ford Algorithm is a shortest path algorithm used to find the shortest path between two nodes in a weighted graph. It is named after its inventors, Richard Bellman and Lester Ford Jr.\n",
      "\n",
      "## Algorithm\n",
      "\n",
      "The Bellman Ford Algorithm works by iteratively relaxing all the edges in the graph. Relaxing an edge means updating the distance to the destination node if a shorter path is found. The algorithm repeats this process for a number of iterations equal to the number of nodes in the graph.\n",
      "\n",
      "The algorithm starts by initializing the distance to the source node as 0 and the distance to all other nodes as infinity. Then, it iteratively relaxes all the edges in the graph. If the distance to a node is updated during an iteration, the algorithm continues to the next iteration. If no distance is updated during an iteration, the algorithm terminates.\n",
      "\n",
      "## Time Complexity\n",
      "\n",
      "The time complexity of the Bellman Ford Algorithm is O(V * E), where V is the number of nodes in the graph and E is the number of edges in the graph. This makes it less efficient than other shortest path algorithms, such as Dijkstra's Algorithm and the Floyd Warshall Algorithm.\n",
      "\n",
      "## Applications\n",
      "\n",
      "The Bellman Ford Algorithm is used in various applications, such as:\n",
      "\n",
      "- Routing protocols in computer networks\n",
      "- Finding negative cycles in a graph\n",
      "- Solving optimization problems in economics and finance\n",
      "\n",
      "## Example\n",
      "\n",
      "Consider the following graph:\n",
      "\n",
      "```\n",
      "     2\n",
      "1 ------- 2\n",
      "|         |\n",
      "|         | -1\n",
      "|         |\n",
      "3 ------- 4\n",
      "     1\n",
      "```\n",
      "\n",
      "The shortest path from node 1 to node 4 is 1 -> 2 -> 4, with a total distance of 1 + 2 = 3.\n",
      "\n",
      "Using the Bellman Ford Algorithm, the distance to node 1 is initialized as 0 and the distance to all other nodes is initialized as infinity. Then, the algorithm iteratively relaxes all the edges in the graph:\n",
      "\n",
      "- Iteration 1: Update distance to node 2 to 2 and distance to node 3 to 1.\n",
      "- Iteration 2: Update distance to node 4 to 3.\n",
      "- Iteration 3: No updates, terminate.\n",
      "\n",
      "The final distances are:\n",
      "\n",
      "```\n",
      "Node 1: 0\n",
      "Node 2: 2\n",
      "Node 3: 1\n",
      "Node 4: 3\n",
      "```\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "The Bellman Ford Algorithm is a useful algorithm for finding the shortest path between two nodes in a weighted graph. Although it has a higher time complexity than other shortest path algorithms, it can handle graphs with negative edge weights and is useful in various applications.\n",
      "DONE GENERATING: bellman_ford_algorithm\n",
      "NOW GENERATING: dijkstra_algorithm\n",
      "FINISH_REASON: stop\n",
      "{\n",
      "    \"dijkstra_algorithm\": {\n",
      "        \"title\": \"Dijkstra Algorithm\",\n",
      "        \"prerequisites\": [\"graph_theory\", \"shortest_path_problem\"],\n",
      "        \"further_readings\": [\"bellman_ford_algorithm\", \"a_star_algorithm\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "# Dijkstra Algorithm\n",
      "\n",
      "The Dijkstra Algorithm is a popular algorithm used to find the shortest path between two nodes in a graph. It is named after its inventor, Edsger W. Dijkstra, a Dutch computer scientist.\n",
      "\n",
      "## Background\n",
      "\n",
      "The Dijkstra Algorithm was first published in 1959 and is considered to be one of the most popular algorithms used in graph theory. It is used to solve the shortest path problem, which is the problem of finding the shortest path between two nodes in a graph. The algorithm is widely used in various fields, including computer networking, transportation, and logistics.\n",
      "\n",
      "## Algorithm\n",
      "\n",
      "The Dijkstra Algorithm works by maintaining a set of unvisited nodes and a set of visited nodes. Initially, all nodes are unvisited, and the distance to each node is set to infinity. The algorithm then selects the node with the smallest distance and visits all of its neighbors. For each neighbor, the algorithm calculates the distance from the starting node to that neighbor through the current node. If this distance is smaller than the current distance to the neighbor, the distance is updated, and the neighbor is added to the set of unvisited nodes. This process is repeated until the destination node is reached or all nodes have been visited.\n",
      "\n",
      "The algorithm can be implemented using a priority queue to efficiently select the node with the smallest distance. The time complexity of the algorithm is O(E + V log V), where E is the number of edges and V is the number of vertices in the graph.\n",
      "\n",
      "## Applications\n",
      "\n",
      "The Dijkstra Algorithm has many applications in various fields. In computer networking, it is used to find the shortest path between two nodes in a network. In transportation, it is used to find the shortest route between two locations. In logistics, it is used to optimize the delivery of goods from one location to another.\n",
      "\n",
      "## Variants\n",
      "\n",
      "There are several variants of the Dijkstra Algorithm, including the bidirectional Dijkstra Algorithm, which searches from both the source and destination nodes simultaneously, and the A* Algorithm, which uses heuristics to guide the search towards the destination node.\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "The Dijkstra Algorithm is a powerful tool for finding the shortest path between two nodes in a graph. It is widely used in various fields and has many applications. Its variants, such as the bidirectional Dijkstra Algorithm and the A* Algorithm, have further improved its efficiency and usefulness.\n",
      "DONE GENERATING: dijkstra_algorithm\n",
      "NOW GENERATING: knapsack_problem\n",
      "FINISH_REASON: stop\n",
      "{\n",
      "    \"knapsack_problem\": {\n",
      "        \"title\": \"Knapsack Problem\",\n",
      "        \"prerequisites\": [\"dynamic_programming\", \"greedy_algorithms\"],\n",
      "        \"further_readings\": [\"integer_programming\", \"approximation_algorithms\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "# Knapsack Problem\n",
      "\n",
      "The Knapsack Problem is a well-known optimization problem in computer science and mathematics. It is a classic example of a combinatorial optimization problem, where the goal is to find the best combination of items to include in a knapsack, subject to certain constraints.\n",
      "\n",
      "## Problem Statement\n",
      "\n",
      "The problem can be stated as follows: given a set of items, each with a weight and a value, determine the items to include in a knapsack of fixed capacity such that the total value of the items included is maximized, while the total weight of the items does not exceed the capacity of the knapsack.\n",
      "\n",
      "Formally, let there be a set of $n$ items, where item $i$ has a weight $w_i$ and a value $v_i$. Let $W$ be the capacity of the knapsack. The goal is to find a subset $S$ of the items such that:\n",
      "\n",
      "$$\\sum_{i \\in S} w_i \\leq W$$\n",
      "\n",
      "and\n",
      "\n",
      "$$\\sum_{i \\in S} v_i$$\n",
      "\n",
      "is maximized.\n",
      "\n",
      "## Solution Approaches\n",
      "\n",
      "### Dynamic Programming\n",
      "\n",
      "One approach to solving the Knapsack Problem is to use dynamic programming. This approach involves breaking the problem down into smaller subproblems and solving each subproblem only once. The solution to the original problem can then be obtained by combining the solutions to the subproblems.\n",
      "\n",
      "The dynamic programming approach to the Knapsack Problem involves constructing a table where each cell represents the maximum value that can be obtained using a subset of the items and a certain capacity of the knapsack. The table is filled in a bottom-up manner, starting with the smallest subproblems and working up to the original problem.\n",
      "\n",
      "### Greedy Algorithms\n",
      "\n",
      "Another approach to solving the Knapsack Problem is to use greedy algorithms. Greedy algorithms make locally optimal choices at each step in the hope of finding a global optimum. In the case of the Knapsack Problem, a greedy algorithm might sort the items by their value-to-weight ratio and then include as many of the highest ratio items as possible until the knapsack is full.\n",
      "\n",
      "While greedy algorithms can be faster than dynamic programming, they do not always produce optimal solutions to the Knapsack Problem.\n",
      "\n",
      "### Integer Programming\n",
      "\n",
      "The Knapsack Problem can also be formulated as an integer programming problem. In this formulation, the decision variables are binary variables indicating whether each item is included in the knapsack or not. The objective function is the total value of the items included, and the constraints ensure that the total weight of the items does not exceed the capacity of the knapsack.\n",
      "\n",
      "Integer programming solvers can be used to solve the Knapsack Problem, but they can be computationally expensive for large problem sizes.\n",
      "\n",
      "### Approximation Algorithms\n",
      "\n",
      "For large Knapsack Problems, approximation algorithms can be used to find near-optimal solutions. These algorithms provide a trade-off between solution quality and computational complexity.\n",
      "\n",
      "One example of an approximation algorithm for the Knapsack Problem is the greedy algorithm mentioned earlier. Another example is the FPTAS (Fully Polynomial-Time Approximation Scheme), which provides a polynomial-time algorithm that produces a solution within a certain factor of the optimal solution.\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "The Knapsack Problem is a classic optimization problem that has been studied extensively in computer science and mathematics. It can be solved using dynamic programming, greedy algorithms, integer programming, and approximation algorithms. Each approach has its own advantages and disadvantages, and the choice of approach depends on the problem size and the desired solution quality.\n",
      "DONE GENERATING: knapsack_problem\n",
      "NOW GENERATING: traveling_salesman_problem\n",
      "FINISH_REASON: stop\n",
      "{\n",
      "    \"traveling_salesman_problem\": {\n",
      "        \"title\": \"Traveling Salesman Problem\",\n",
      "        \"prerequisites\": [\"graph_theory\", \"dynamic_programming\", \"combinatorial_optimization\"],\n",
      "        \"further_readings\": [\"christofides_algorithm\", \"branch_and_bound_algorithm\", \"simulated_annealing\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "# Traveling Salesman Problem\n",
      "\n",
      "The Traveling Salesman Problem (TSP) is a well-known problem in computer science and mathematics that involves finding the shortest possible route that visits a set of given cities and returns to the starting city. The problem is considered to be NP-hard, meaning that it is computationally infeasible to solve for large numbers of cities using brute force methods.\n",
      "\n",
      "## Problem Statement\n",
      "\n",
      "Given a set of cities and the distances between each pair of cities, the objective of the TSP is to find the shortest possible route that visits each city exactly once and returns to the starting city. The problem can be represented as a complete graph, where each city is a node and the distance between each pair of cities is the weight of the edge connecting them.\n",
      "\n",
      "## Approaches\n",
      "\n",
      "### Brute Force\n",
      "\n",
      "The most straightforward approach to solving the TSP is to enumerate all possible routes and select the one with the shortest distance. However, this approach quickly becomes infeasible as the number of cities increases, as the number of possible routes grows exponentially with the number of cities.\n",
      "\n",
      "### Dynamic Programming\n",
      "\n",
      "Dynamic programming can be used to solve the TSP by breaking the problem down into smaller subproblems. The solution to the TSP for a given set of cities can be obtained by solving the TSP for all subsets of the cities that include the starting city and one additional city. This approach has a time complexity of O(n^2 * 2^n), where n is the number of cities.\n",
      "\n",
      "### Approximation Algorithms\n",
      "\n",
      "Various approximation algorithms have been developed to solve the TSP in a reasonable amount of time for large numbers of cities. One such algorithm is the Christofides algorithm, which has a worst-case approximation ratio of 3/2. Another approach is the branch and bound algorithm, which involves recursively partitioning the search space into smaller subproblems and pruning branches that cannot lead to an optimal solution. Simulated annealing is another popular approach that involves iteratively improving a randomly generated solution by accepting worse solutions with a certain probability.\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "The Traveling Salesman Problem is a well-known problem in computer science and mathematics that involves finding the shortest possible route that visits a set of given cities and returns to the starting city. The problem is considered to be NP-hard, meaning that it is computationally infeasible to solve for large numbers of cities using brute force methods. Various approaches, including dynamic programming and approximation algorithms such as the Christofides algorithm, have been developed to solve the problem in a reasonable amount of time.\n",
      "DONE GENERATING: traveling_salesman_problem\n",
      "NOW GENERATING: longest_common_subsequence\n",
      "FINISH_REASON: stop\n",
      "{\n",
      "    \"longest_common_subsequence\": {\n",
      "        \"title\": \"Longest Common Subsequence\",\n",
      "        \"prerequisites\": [\"dynamic_programming\", \"string_algorithms\"],\n",
      "        \"further_readings\": [\"edit_distance\", \"suffix_trees\", \"suffix_arrays\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "# Longest Common Subsequence\n",
      "\n",
      "In computer science, the **Longest Common Subsequence (LCS)** is a problem that involves finding the longest subsequence that is common to two or more sequences. A subsequence is a sequence that can be derived from another sequence by deleting some or no elements without changing the order of the remaining elements. The LCS problem is a classic example of dynamic programming and is widely used in various applications such as bioinformatics, text comparison, and version control systems.\n",
      "\n",
      "## Problem Statement\n",
      "\n",
      "Given two sequences `X` and `Y`, the LCS problem involves finding the longest subsequence that is common to both `X` and `Y`. For example, consider the following two sequences:\n",
      "\n",
      "```\n",
      "X: ABCBDAB\n",
      "Y: BDCABA\n",
      "```\n",
      "\n",
      "The LCS of `X` and `Y` is `BCBA` with a length of 4. Note that there can be multiple LCS for a given pair of sequences.\n",
      "\n",
      "## Dynamic Programming Solution\n",
      "\n",
      "The LCS problem can be solved using dynamic programming. The basic idea is to build a table `L` of size `(m+1) x (n+1)` where `m` and `n` are the lengths of the sequences `X` and `Y`, respectively. The entry `L[i][j]` of the table represents the length of the LCS of the first `i` elements of `X` and the first `j` elements of `Y`. The table can be filled in a bottom-up manner using the following recurrence relation:\n",
      "\n",
      "$$\n",
      "L[i][j] = \\begin{cases}\n",
      "0 & \\text{if } i=0 \\text{ or } j=0 \\\\\n",
      "L[i-1][j-1]+1 & \\text{if } X[i]=Y[j] \\\\\n",
      "\\max(L[i-1][j], L[i][j-1]) & \\text{if } X[i]\\neq Y[j]\n",
      "\\end{cases}\n",
      "$$\n",
      "\n",
      "The first case represents the base case where one of the sequences is empty. The second case represents the case where the last elements of both sequences match, in which case we add 1 to the LCS of the remaining sequences. The third case represents the case where the last elements of both sequences do not match, in which case we take the maximum of the LCS of the remaining sequences.\n",
      "\n",
      "Once the table is filled, the LCS can be obtained by backtracking from the bottom-right corner of the table. Starting from `L[m][n]`, if `X[i]=Y[j]`, then the current element is part of the LCS and we move diagonally to `L[i-1][j-1]`. Otherwise, we move to the left if `L[i-1][j] > L[i][j-1]` or to the top otherwise.\n",
      "\n",
      "The time complexity of the dynamic programming solution is `O(mn)` and the space complexity is `O(mn)`.\n",
      "\n",
      "## Applications\n",
      "\n",
      "The LCS problem has various applications in computer science and related fields. Some of the notable applications are:\n",
      "\n",
      "- **Bioinformatics**: The LCS problem is used in DNA sequencing to find the similarity between two DNA sequences. The LCS of two DNA sequences represents the longest common subsequence of nucleotides that are present in both sequences.\n",
      "- **Text Comparison**: The LCS problem is used in text comparison to find the difference between two texts. The LCS of two texts represents the longest common subsequence of words or characters that are present in both texts.\n",
      "- **Version Control Systems**: The LCS problem is used in version control systems such as Git to find the difference between two versions of a file. The LCS of two versions represents the longest common subsequence of lines that are present in both versions.\n",
      "\n",
      "## Further Readings\n",
      "\n",
      "- [Edit Distance](edit_distance): Another classic dynamic programming problem that involves finding the minimum number of operations required to transform one sequence into another.\n",
      "- [Suffix Trees](suffix_trees): A data structure that can be used to solve the LCS problem in linear time.\n",
      "- [Suffix Arrays](suffix_arrays): Another data structure that can be used to solve the LCS problem in linear time.\n",
      "DONE GENERATING: longest_common_subsequence\n",
      "NOW GENERATING: maximum_subarray_problem\n",
      "FINISH_REASON: stop\n",
      "{\n",
      "    \"maximum_subarray_problem\": {\n",
      "        \"title\": \"Maximum Subarray Problem\",\n",
      "        \"prerequisites\": [\"dynamic_programming\", \"divide_and_conquer\"],\n",
      "        \"further_readings\": [\"kadane_algorithm\", \"linear_time_algorithm\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "# Maximum Subarray Problem\n",
      "\n",
      "The Maximum Subarray Problem is a classic problem in computer science and is used in various applications, including finance, image processing, and data analysis. The problem is to find the contiguous subarray within an array of integers that has the largest sum. \n",
      "\n",
      "## Problem Statement\n",
      "\n",
      "Given an array of integers, the problem is to find the contiguous subarray that has the largest sum. Formally, given an array $A$ of $n$ integers, the problem is to find indices $i$ and $j$ such that $i \\leq j$ and the sum of the subarray $A[i:j]$ is maximum among all subarrays of $A$. \n",
      "\n",
      "## Approaches\n",
      "\n",
      "### Brute Force\n",
      "\n",
      "The brute force approach is to consider all possible subarrays and compute their sum. The subarray with the largest sum is the maximum subarray. The time complexity of this approach is $O(n^3)$.\n",
      "\n",
      "### Divide and Conquer\n",
      "\n",
      "The divide and conquer approach is to divide the array into two halves and recursively find the maximum subarray in each half. The maximum subarray can either be entirely in the left half, entirely in the right half, or crossing the midpoint. The time complexity of this approach is $O(n \\log n)$.\n",
      "\n",
      "### Dynamic Programming\n",
      "\n",
      "The dynamic programming approach is to compute the maximum subarray ending at each index of the array. The maximum subarray ending at index $i$ is either the element at index $i$ or the sum of the maximum subarray ending at index $i-1$ and the element at index $i$. The maximum subarray among all indices is the maximum subarray of the entire array. The time complexity of this approach is $O(n)$.\n",
      "\n",
      "### Kadane's Algorithm\n",
      "\n",
      "Kadane's algorithm is a variation of the dynamic programming approach that only uses constant space. The algorithm computes the maximum subarray ending at each index of the array and keeps track of the maximum subarray seen so far. The time complexity of this algorithm is $O(n)$.\n",
      "\n",
      "## Further Readings\n",
      "\n",
      "- Kadane's Algorithm\n",
      "- Linear Time Algorithm\n",
      "DONE GENERATING: maximum_subarray_problem\n",
      "NOW GENERATING: floyd_warshall_algorithm\n",
      "FINISH_REASON: stop\n",
      "{\n",
      "    \"floyd_warshall_algorithm\": {\n",
      "        \"title\": \"Floyd Warshall Algorithm\",\n",
      "        \"prerequisites\": [\"graph_theory\", \"shortest_path_algorithms\"],\n",
      "        \"further_readings\": [\"dijkstra_algorithm\", \"bellman_ford_algorithm\", \"johnson_algorithm\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "# Floyd Warshall Algorithm\n",
      "\n",
      "The Floyd Warshall algorithm is a dynamic programming algorithm used to find the shortest path between all pairs of vertices in a weighted graph. It is named after Robert Floyd and Stephen Warshall, who independently published the algorithm in 1962 and 1962 respectively. \n",
      "\n",
      "## Algorithm\n",
      "\n",
      "The algorithm works by considering all possible intermediate vertices between any two vertices and updating the shortest path accordingly. The algorithm maintains a distance matrix `D` where `D[i][j]` represents the shortest distance between vertices `i` and `j`. Initially, `D[i][j]` is set to the weight of the edge between vertices `i` and `j` if there is an edge, and infinity otherwise. \n",
      "\n",
      "The algorithm then considers all possible intermediate vertices `k` and updates `D[i][j]` as follows:\n",
      "\n",
      "$$D[i][j] = \\min(D[i][j], D[i][k] + D[k][j])$$\n",
      "\n",
      "The above equation means that the shortest path between vertices `i` and `j` either goes directly from `i` to `j` or goes through an intermediate vertex `k`. The algorithm repeats this process for all possible intermediate vertices and updates the distance matrix accordingly. \n",
      "\n",
      "## Complexity\n",
      "\n",
      "The Floyd Warshall algorithm has a time complexity of O(n^3) and a space complexity of O(n^2), where n is the number of vertices in the graph. This makes it less efficient than other shortest path algorithms such as Dijkstra's algorithm and the Bellman-Ford algorithm. However, the Floyd Warshall algorithm has the advantage of being able to handle negative edge weights and detect negative cycles in the graph. \n",
      "\n",
      "## Applications\n",
      "\n",
      "The Floyd Warshall algorithm has various applications in computer science and engineering. It can be used to find the shortest path between all pairs of nodes in a network, such as in routing algorithms for computer networks. It can also be used in image processing to find the shortest path between two points in an image. Additionally, the algorithm can be used to solve problems in game theory, such as finding the optimal strategy for a two-player game. \n",
      "\n",
      "## Further Readings\n",
      "\n",
      "- Dijkstra's Algorithm\n",
      "- Bellman-Ford Algorithm\n",
      "- Johnson's Algorithm\n",
      "DONE GENERATING: floyd_warshall_algorithm\n",
      "NOW GENERATING: edit_distance\n",
      "FINISH_REASON: stop\n",
      "{\n",
      "    \"edit_distance\": {\n",
      "        \"title\": \"Edit Distance\",\n",
      "        \"prerequisites\": [\"dynamic_programming\", \"string_matching_algorithms\"],\n",
      "        \"further_readings\": [\"levenshtein_distance\", \"needleman_wunsch_algorithm\", \"smith_waterman_algorithm\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "# Edit Distance\n",
      "\n",
      "Edit distance, also known as Levenshtein distance, is a measure of the difference between two strings. It is defined as the minimum number of operations required to transform one string into another. The operations allowed are insertion, deletion, and substitution of a single character.\n",
      "\n",
      "Edit distance has applications in various fields such as natural language processing, bioinformatics, and computer science. It is used in spell-checking, DNA sequencing, and plagiarism detection, among others.\n",
      "\n",
      "## Algorithm\n",
      "\n",
      "The algorithm to compute edit distance is based on dynamic programming. The idea is to build a matrix where each cell represents the edit distance between two substrings of the input strings. The matrix is filled in a bottom-up manner, starting from the base case of empty substrings.\n",
      "\n",
      "Let `s1` and `s2` be the input strings of length `m` and `n`, respectively. The matrix `D` of size `(m+1) x (n+1)` is initialized as follows:\n",
      "\n",
      "$$\n",
      "D_{i,0} = i \\quad \\text{for } i = 0, 1, \\dots, m \\\\\n",
      "D_{0,j} = j \\quad \\text{for } j = 0, 1, \\dots, n \\\\\n",
      "$$\n",
      "\n",
      "The value of `D[i,j]` is computed as follows:\n",
      "\n",
      "$$\n",
      "D_{i,j} = \\begin{cases}\n",
      "D_{i-1,j-1} & \\text{if } s1_i = s2_j \\\\\n",
      "\\min(D_{i-1,j}, D_{i,j-1}, D_{i-1,j-1}) + 1 & \\text{otherwise}\n",
      "\\end{cases}\n",
      "$$\n",
      "\n",
      "The first case corresponds to the situation where the characters at positions `i` and `j` are the same, in which case no operation is needed. The second case corresponds to the situation where the characters are different, in which case the edit distance is the minimum of the edit distances obtained by deleting, inserting, or substituting a character.\n",
      "\n",
      "The final value of `D[m,n]` is the edit distance between the two input strings.\n",
      "\n",
      "## Variants\n",
      "\n",
      "There are several variants of the edit distance algorithm that differ in the cost of the operations allowed. For example, the Damerau-Levenshtein distance allows transposition of adjacent characters as an additional operation. The Jaro-Winkler distance is a variant that takes into account the similarity of the characters being compared.\n",
      "\n",
      "## Further Readings\n",
      "\n",
      "- Levenshtein distance\n",
      "- Needleman-Wunsch algorithm\n",
      "- Smith-Waterman algorithm\n",
      "DONE GENERATING: edit_distance\n",
      "NOW GENERATING: approximation_algorithms\n",
      "FINISH_REASON: stop\n",
      "{\n",
      "    \"approximation_algorithms\": {\n",
      "        \"title\": \"Approximation Algorithms\",\n",
      "        \"prerequisites\": [\"graph_theory\", \"complexity_theory\", \"optimization_algorithms\"],\n",
      "        \"further_readings\": [\"approximation_algorithms_in_the_real_world\", \"randomized_approximation_algorithms\", \"online_approximation_algorithms\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "# Approximation Algorithms\n",
      "\n",
      "Approximation algorithms are algorithms that provide near-optimal solutions to optimization problems in a reasonable amount of time. These algorithms are used when finding the exact solution to a problem is computationally infeasible or too time-consuming. In such cases, approximation algorithms provide a solution that is guaranteed to be within a certain factor of the optimal solution.\n",
      "\n",
      "## Overview\n",
      "\n",
      "Approximation algorithms are used to solve optimization problems in various fields, including computer science, operations research, and engineering. These algorithms are designed to provide a solution that is close to the optimal solution, but not necessarily the exact solution. The quality of the approximation is measured by the approximation ratio, which is the ratio of the approximation algorithm's solution to the optimal solution.\n",
      "\n",
      "## Types of Approximation Algorithms\n",
      "\n",
      "There are several types of approximation algorithms, including:\n",
      "\n",
      "- **Deterministic approximation algorithms:** These algorithms provide a deterministic approximation ratio for a given problem. They are typically easier to analyze than randomized algorithms, but may not always provide the best approximation ratio.\n",
      "\n",
      "- **Randomized approximation algorithms:** These algorithms use randomness to provide an approximation ratio that is better than the deterministic approximation ratio. They are typically harder to analyze than deterministic algorithms, but can provide better results.\n",
      "\n",
      "- **Online approximation algorithms:** These algorithms are used when the input to the problem is not known in advance. They provide a solution that is competitive with the optimal solution, where the competitive ratio is the worst-case ratio of the algorithm's solution to the optimal solution.\n",
      "\n",
      "## Applications\n",
      "\n",
      "Approximation algorithms are used in various applications, including:\n",
      "\n",
      "- **Network design:** Approximation algorithms are used to design efficient networks that connect different locations.\n",
      "\n",
      "- **Scheduling:** Approximation algorithms are used to schedule tasks in a way that minimizes the total completion time.\n",
      "\n",
      "- **Clustering:** Approximation algorithms are used to group similar items together in a way that minimizes the distance between them.\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "Approximation algorithms are an important tool for solving optimization problems in a reasonable amount of time. They provide near-optimal solutions that are guaranteed to be within a certain factor of the optimal solution. Deterministic, randomized, and online approximation algorithms are used in various applications, including network design, scheduling, and clustering.\n",
      "DONE GENERATING: approximation_algorithms\n",
      "NOW GENERATING: online_learning_in_mdps\n",
      "FINISH_REASON: stop\n",
      "{\n",
      "    \"online_learning_in_mdps\": {\n",
      "        \"title\": \"Online Learning in Markov Decision Processes\",\n",
      "        \"prerequisites\": [\"markov_decision_process\", \"reinforcement_learning\", \"online_learning\"],\n",
      "        \"further_readings\": [\"stochastic_approximation\", \"temporal_difference_learning\", \"q_learning\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "# Online Learning in Markov Decision Processes\n",
      "\n",
      "Online learning in Markov decision processes (MDPs) is a subfield of reinforcement learning that deals with learning optimal policies in an online setting. In this setting, an agent interacts with an environment and learns from the feedback it receives in real-time. The agent's goal is to maximize its cumulative reward over time.\n",
      "\n",
      "## Markov Decision Processes\n",
      "\n",
      "Markov decision processes are mathematical models used to describe decision-making in situations where outcomes are partly random and partly under the control of a decision-maker. They consist of a set of states, actions, transition probabilities, and rewards. At each time step, the agent observes the current state of the environment, selects an action, and receives a reward. The next state of the environment is then determined by the transition probabilities.\n",
      "\n",
      "## Reinforcement Learning\n",
      "\n",
      "Reinforcement learning is a subfield of machine learning that deals with learning from feedback in an interactive environment. In reinforcement learning, an agent learns to take actions that maximize a cumulative reward signal. The agent's goal is to learn an optimal policy that maps states to actions.\n",
      "\n",
      "## Online Learning\n",
      "\n",
      "Online learning is a type of machine learning where the learning algorithm receives data in a sequential order and updates its model as it receives new data. In online learning, the model is updated after each data point is received, rather than waiting for all the data to be collected before updating the model.\n",
      "\n",
      "## Stochastic Approximation\n",
      "\n",
      "Stochastic approximation is a method used in online learning to update the model parameters based on noisy feedback. In stochastic approximation, the model parameters are updated using a noisy estimate of the gradient of the loss function.\n",
      "\n",
      "## Temporal Difference Learning\n",
      "\n",
      "Temporal difference learning is a method used in reinforcement learning to update the value function based on the difference between the estimated value of the current state and the estimated value of the next state. Temporal difference learning is a type of online learning because the value function is updated after each time step.\n",
      "\n",
      "## Q-Learning\n",
      "\n",
      "Q-learning is a popular reinforcement learning algorithm that learns an optimal policy by estimating the value of each state-action pair. Q-learning is a type of online learning because the Q-values are updated after each time step.\n",
      "\n",
      "In summary, online learning in Markov decision processes is a subfield of reinforcement learning that deals with learning optimal policies in an online setting. It involves using stochastic approximation, temporal difference learning, and Q-learning to update the model parameters and estimate the optimal policy.\n",
      "DONE GENERATING: online_learning_in_mdps\n",
      "NOW GENERATING: approximate_dynamic_programming\n",
      "FINISH_REASON: stop\n",
      "{\n",
      "    \"approximate_dynamic_programming\": {\n",
      "        \"title\": \"Approximate Dynamic Programming\",\n",
      "        \"prerequisites\": [\"dynamic_programming\", \"reinforcement_learning\", \"function_approximation\"],\n",
      "        \"further_readings\": [\"deep_reinforcement_learning\", \"monte_carlo_tree_search\", \"value_iteration\", \"policy_iteration\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "# Approximate Dynamic Programming\n",
      "\n",
      "**Approximate Dynamic Programming (ADP)** is a class of reinforcement learning algorithms that use function approximation to solve large-scale Markov decision processes (MDPs). It is a generalization of **Dynamic Programming (DP)**, which is a method for solving MDPs by iteratively computing the optimal value function and policy.\n",
      "\n",
      "ADP algorithms are used in situations where the state space of an MDP is too large to be represented explicitly, making it infeasible to apply DP directly. Instead, ADP algorithms use function approximation techniques to represent the value function or policy implicitly, allowing them to scale to much larger problems.\n",
      "\n",
      "## Background\n",
      "\n",
      "ADP is based on the principle of **Bellman's equation**, which expresses the optimal value function of an MDP in terms of the optimal value function of its successor states. The Bellman equation can be written as:\n",
      "\n",
      "$$V^*(s) = \\max_{a \\in A} \\sum_{s' \\in S} P(s' | s, a) [R(s,a,s') + \\gamma V^*(s')]$$\n",
      "\n",
      "where $V^*(s)$ is the optimal value function, $s$ is the current state, $a$ is the action taken, $s'$ is the next state, $P(s' | s, a)$ is the probability of transitioning from state $s$ to state $s'$ under action $a$, $R(s,a,s')$ is the reward obtained from transitioning from state $s$ to state $s'$ under action $a$, and $\\gamma$ is the discount factor.\n",
      "\n",
      "DP algorithms solve the Bellman equation by iteratively computing the optimal value function and policy until convergence. However, this approach is computationally expensive and infeasible for large-scale problems.\n",
      "\n",
      "## Function Approximation\n",
      "\n",
      "ADP algorithms use function approximation techniques to represent the value function or policy implicitly, allowing them to scale to much larger problems. Function approximation involves approximating the value function or policy using a set of basis functions, such as linear or nonlinear functions.\n",
      "\n",
      "The most common function approximation technique used in ADP is **linear function approximation**, which involves approximating the value function or policy using a linear combination of basis functions. The linear function approximation can be written as:\n",
      "\n",
      "$$\\hat{V}(s,w) = \\sum_{i=1}^n w_i \\phi_i(s)$$\n",
      "\n",
      "where $\\hat{V}(s,w)$ is the approximate value function, $w$ is the weight vector, $\\phi_i(s)$ is the $i$-th basis function, and $n$ is the number of basis functions.\n",
      "\n",
      "## ADP Algorithms\n",
      "\n",
      "There are several ADP algorithms, including **Q-Learning**, **SARSA**, and **Actor-Critic**. These algorithms use function approximation to represent the value function or policy implicitly, allowing them to scale to much larger problems.\n",
      "\n",
      "Q-Learning is a model-free ADP algorithm that learns the optimal Q-value function by iteratively updating the Q-value estimates using the Bellman equation. SARSA is a model-free ADP algorithm that learns the Q-value function by iteratively updating the Q-value estimates using the Bellman equation and the current policy. Actor-Critic is a model-based ADP algorithm that learns the policy and value function simultaneously by using an actor network to represent the policy and a critic network to represent the value function.\n",
      "\n",
      "## Applications\n",
      "\n",
      "ADP algorithms have been successfully applied to a wide range of problems, including robotics, game playing, and finance. For example, ADP algorithms have been used to control the motion of robots, play games such as chess and Go, and optimize trading strategies in finance.\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "ADP is a class of reinforcement learning algorithms that use function approximation to solve large-scale MDPs. It is a generalization of DP and is used in situations where the state space of an MDP is too large to be represented explicitly. ADP algorithms use function approximation techniques to represent the value function or policy implicitly, allowing them to scale to much larger problems. There are several ADP algorithms, including Q-Learning, SARSA, and Actor-Critic, which have been successfully applied to a wide range of problems.\n",
      "DONE GENERATING: approximate_dynamic_programming\n",
      "NOW GENERATING: imitation_learning_in_rl\n"
     ]
    },
    {
     "ename": "Timeout",
     "evalue": "Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/urllib3/connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    450\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/urllib3/connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m     httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    445\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m     \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m     \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m     \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/http/client.py:1374\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1374\u001b[0m     response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1375\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    706\u001b[0m \u001b[39mexcept\u001b[39;00m timeout:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1271\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1272\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1273\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1274\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1275\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1129\u001b[0m \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1131\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mTimeoutError\u001b[0m: The read operation timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/requests/adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 489\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    490\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    491\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    492\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    493\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    494\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    496\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    497\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    498\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    499\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    500\u001b[0m     )\n\u001b[1;32m    502\u001b[0m \u001b[39m# Send the request.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    785\u001b[0m     e \u001b[39m=\u001b[39m ProtocolError(\u001b[39m\"\u001b[39m\u001b[39mConnection aborted.\u001b[39m\u001b[39m\"\u001b[39m, e)\n\u001b[0;32m--> 787\u001b[0m retries \u001b[39m=\u001b[39m retries\u001b[39m.\u001b[39;49mincrement(\n\u001b[1;32m    788\u001b[0m     method, url, error\u001b[39m=\u001b[39;49me, _pool\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, _stacktrace\u001b[39m=\u001b[39;49msys\u001b[39m.\u001b[39;49mexc_info()[\u001b[39m2\u001b[39;49m]\n\u001b[1;32m    789\u001b[0m )\n\u001b[1;32m    790\u001b[0m retries\u001b[39m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/urllib3/util/retry.py:550\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[39mif\u001b[39;00m read \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_method_retryable(method):\n\u001b[0;32m--> 550\u001b[0m     \u001b[39mraise\u001b[39;00m six\u001b[39m.\u001b[39;49mreraise(\u001b[39mtype\u001b[39;49m(error), error, _stacktrace)\n\u001b[1;32m    551\u001b[0m \u001b[39melif\u001b[39;00m read \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/urllib3/packages/six.py:770\u001b[0m, in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[39mraise\u001b[39;00m value\u001b[39m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m--> 770\u001b[0m     \u001b[39mraise\u001b[39;00m value\n\u001b[1;32m    771\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    704\u001b[0m     conn,\n\u001b[1;32m    705\u001b[0m     method,\n\u001b[1;32m    706\u001b[0m     url,\n\u001b[1;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    711\u001b[0m )\n\u001b[1;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/urllib3/connectionpool.py:451\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 451\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_timeout(err\u001b[39m=\u001b[39;49me, url\u001b[39m=\u001b[39;49murl, timeout_value\u001b[39m=\u001b[39;49mread_timeout)\n\u001b[1;32m    452\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/urllib3/connectionpool.py:340\u001b[0m, in \u001b[0;36mHTTPConnectionPool._raise_timeout\u001b[0;34m(self, err, url, timeout_value)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(err, SocketTimeout):\n\u001b[0;32m--> 340\u001b[0m     \u001b[39mraise\u001b[39;00m ReadTimeoutError(\n\u001b[1;32m    341\u001b[0m         \u001b[39mself\u001b[39m, url, \u001b[39m\"\u001b[39m\u001b[39mRead timed out. (read timeout=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m timeout_value\n\u001b[1;32m    342\u001b[0m     )\n\u001b[1;32m    344\u001b[0m \u001b[39m# See the above comment about EAGAIN in Python 3. In Python 2 we have\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[39m# to specifically catch it and throw the timeout error\u001b[39;00m\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mReadTimeout\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/openai/api_requestor.py:516\u001b[0m, in \u001b[0;36mAPIRequestor.request_raw\u001b[0;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 516\u001b[0m     result \u001b[39m=\u001b[39m _thread_context\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    517\u001b[0m         method,\n\u001b[1;32m    518\u001b[0m         abs_url,\n\u001b[1;32m    519\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    520\u001b[0m         data\u001b[39m=\u001b[39;49mdata,\n\u001b[1;32m    521\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[1;32m    522\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    523\u001b[0m         timeout\u001b[39m=\u001b[39;49mrequest_timeout \u001b[39mif\u001b[39;49;00m request_timeout \u001b[39melse\u001b[39;49;00m TIMEOUT_SECS,\n\u001b[1;32m    524\u001b[0m         proxies\u001b[39m=\u001b[39;49m_thread_context\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mproxies,\n\u001b[1;32m    525\u001b[0m     )\n\u001b[1;32m    526\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mTimeout \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/requests/adapters.py:578\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(e, ReadTimeoutError):\n\u001b[0;32m--> 578\u001b[0m     \u001b[39mraise\u001b[39;00m ReadTimeout(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[1;32m    579\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(e, _InvalidHeader):\n",
      "\u001b[0;31mReadTimeout\u001b[0m: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTimeout\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 54\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mNOW GENERATING:\u001b[39m\u001b[39m'\u001b[39m, topic)\n\u001b[1;32m     53\u001b[0m prompt \u001b[39m=\u001b[39m generate_prompt(topic)\n\u001b[0;32m---> 54\u001b[0m finish_reason, message, completion \u001b[39m=\u001b[39m generate_completion(prompt)\n\u001b[1;32m     55\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFINISH_REASON:\u001b[39m\u001b[39m\"\u001b[39m, finish_reason)\n\u001b[1;32m     56\u001b[0m \u001b[39mprint\u001b[39m(message)\n",
      "Cell \u001b[0;32mIn[2], line 37\u001b[0m, in \u001b[0;36mgenerate_completion\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_completion\u001b[39m(prompt):\n\u001b[0;32m---> 37\u001b[0m     completion \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m     38\u001b[0m         model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgpt-3.5-turbo\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     39\u001b[0m         \u001b[39m# model=\"gpt-4\",\u001b[39;49;00m\n\u001b[1;32m     40\u001b[0m         messages\u001b[39m=\u001b[39;49m[\n\u001b[1;32m     41\u001b[0m             {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: prompt}\n\u001b[1;32m     42\u001b[0m         ],\n\u001b[1;32m     43\u001b[0m         temperature\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m,\n\u001b[1;32m     44\u001b[0m     )\n\u001b[1;32m     45\u001b[0m     finish_reason \u001b[39m=\u001b[39m completion\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mfinish_reason\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     46\u001b[0m     message \u001b[39m=\u001b[39m completion\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mmessage\u001b[39m.\u001b[39mcontent\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/openai/api_requestor.py:216\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    206\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    207\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    214\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    215\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m--> 216\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest_raw(\n\u001b[1;32m    217\u001b[0m         method\u001b[39m.\u001b[39;49mlower(),\n\u001b[1;32m    218\u001b[0m         url,\n\u001b[1;32m    219\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    220\u001b[0m         supplied_headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    221\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[1;32m    222\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    223\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    224\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    225\u001b[0m     )\n\u001b[1;32m    226\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response(result, stream)\n\u001b[1;32m    227\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/openai/api_requestor.py:527\u001b[0m, in \u001b[0;36mAPIRequestor.request_raw\u001b[0;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    516\u001b[0m     result \u001b[39m=\u001b[39m _thread_context\u001b[39m.\u001b[39msession\u001b[39m.\u001b[39mrequest(\n\u001b[1;32m    517\u001b[0m         method,\n\u001b[1;32m    518\u001b[0m         abs_url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    524\u001b[0m         proxies\u001b[39m=\u001b[39m_thread_context\u001b[39m.\u001b[39msession\u001b[39m.\u001b[39mproxies,\n\u001b[1;32m    525\u001b[0m     )\n\u001b[1;32m    526\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mTimeout \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 527\u001b[0m     \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mTimeout(\u001b[39m\"\u001b[39m\u001b[39mRequest timed out: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(e)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    528\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mRequestException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    529\u001b[0m     \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mAPIConnectionError(\n\u001b[1;32m    530\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mError communicating with OpenAI: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(e)\n\u001b[1;32m    531\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mTimeout\u001b[0m: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600)"
     ]
    }
   ],
   "source": [
    "visited_pages = load_visited_pages()\n",
    "queue = []\n",
    "visited_pages.add('voxel-based_method')\n",
    "\n",
    "if not queue:\n",
    "    with open('wiki-connections.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "        for key in data:\n",
    "            for new_topic in data[key]['prerequisites']:\n",
    "                if os.path.exists('data/' + new_topic + '.md'):\n",
    "                    visited_pages.add(new_topic)\n",
    "                    continue\n",
    "                if new_topic not in visited_pages and new_topic not in queue and new_topic not in data:\n",
    "                    queue.append(new_topic)\n",
    "            for new_topic in data[key]['further_readings']:\n",
    "                if os.path.exists('data/' + new_topic + '.md'):\n",
    "                    visited_pages.add(new_topic)\n",
    "                    continue\n",
    "                if new_topic not in visited_pages and new_topic not in queue and new_topic not in data:\n",
    "                    queue.append(new_topic)\n",
    "            if len(queue) > 0:\n",
    "                break\n",
    "print(queue)\n",
    "\n",
    "while queue:\n",
    "    topic = queue.pop(0)\n",
    "    topic = topic.lower()\n",
    "    topic = topic.replace(\"'\", \"\")\n",
    "\n",
    "    if topic in visited_pages:\n",
    "        continue\n",
    "\n",
    "    with open('wiki-connections.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "        if topic in data:\n",
    "            continue\n",
    "        while len(queue) == 0:\n",
    "            for key in data:\n",
    "                for new_topic in data[key]['prerequisites']:\n",
    "                    if os.path.exists('data/' + new_topic + '.md'):\n",
    "                        visited_pages.add(new_topic)\n",
    "                        continue\n",
    "                    if new_topic not in visited_pages and new_topic not in queue and new_topic != topic and new_topic not in data:\n",
    "                        queue.append(new_topic)\n",
    "                for new_topic in data[key]['further_readings']:\n",
    "                    if os.path.exists('data/' + new_topic + '.md'):\n",
    "                        visited_pages.add(new_topic)\n",
    "                        continue\n",
    "                    if new_topic not in visited_pages and new_topic not in queue and new_topic != topic and new_topic not in data:\n",
    "                        queue.append(new_topic)\n",
    "\n",
    "    print('NOW GENERATING:', topic)\n",
    "    prompt = generate_prompt(topic)\n",
    "    finish_reason, message, completion = generate_completion(prompt)\n",
    "    print(\"FINISH_REASON:\", finish_reason)\n",
    "    print(message)\n",
    "\n",
    "    if finish_reason != 'stop':\n",
    "        print(\"Error: Did not finish generating.\")\n",
    "        exit(1)\n",
    "    \n",
    "    has_generated_json = generate_json(message, topic)\n",
    "    has_generated_markdown = generate_markdown(message, topic)\n",
    "    has_generated_js = generate_js(topic)\n",
    "\n",
    "    visited_pages.add(topic)\n",
    "    save_visited_pages(visited_pages)\n",
    "\n",
    "    if not has_generated_json or not has_generated_markdown or not has_generated_js:\n",
    "        exit(1)\n",
    "\n",
    "    # with open('wiki-connections.json', 'r') as file:\n",
    "    #     wiki_connections = json.load(file)\n",
    "    #     queue += wiki_connections[topic]['prerequisites']\n",
    "    #     queue += wiki_connections[topic]['further_readings']\n",
    "\n",
    "    print('DONE GENERATING:', topic)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
