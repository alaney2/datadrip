{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import shutil\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('.env.local')\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(topic):\n",
    "    parts = topic.split('_')\n",
    "    parts = [part.capitalize() for part in parts]\n",
    "    topic = ' '.join(parts)\n",
    "    prompt = f'Topic: {topic}\\n' + '''\n",
    "    You are a world-renowned AI and ML expert.\n",
    "    Provide a JSON object containing the topic, a list of 1-12 prerequisite topics, and a list of 1-12 further readings related to AI, ML, and DL. \n",
    "    Ensure that the prerequisites and further readings are specifically relevant to the given, rather than broad topics like calculus or statistics.\n",
    "    Ensure that the title field is properly capitalized and spaced and has the right punctuation (such as Q-Learning).\n",
    "    Also ensure that the topic, prerequisites, and further readings are in snake_case.\n",
    "    Use a similar format to the example provided below.:\n",
    "\n",
    "    Example:\n",
    "    {\n",
    "        \"generative_adversarial_network\": {\n",
    "            \"title\": \"Generative Adversarial Network\",\n",
    "            \"prerequisites\": [\"expectation_maximization_algorithm\", \"probability_distributions\", \"convolutional_neural_networks\", \"backpropagation\", \"stochastic_gradient_descent\", \"loss_functions\", \"optimization_algorithms\", \"deep_learning_frameworks\", \"regularization_techniques\", \"unsupervised_learning\"],\n",
    "            \"further_readings\": [\"conditional_gans\", \"cycle_gans\", \"stylegan_and_stylegan2\", \"wasserstein_gans\", \"domain_adaptation\", \"image_to_image_translation\", \"semi_supervised_learning\", \"adversarial_training\", \"adversarial_attacks_and_defenses\", \"transfer_learning\"]\n",
    "        }\n",
    "    }\n",
    "    Next, write a detailed wiki page about the given topic in Markdown format. Always write from a third-person perspective and remain unopinionated.\n",
    "    Ensure that this wiki page is explicitly in code format. \n",
    "    Do not include a \"Contents\" section. \n",
    "    Do not include a \"Further Readings\" nor a \"Prerequisites\" section if they just include related topics.\n",
    "    Use a neutral, unbiased tone without exclamation marks. \n",
    "    Ensure that the heading is the same as the title in the JSON object.\n",
    "    Follow Markdown syntax for headings and formatting, and use LaTeX for equations, with inline equations in pairs of $ and multiline equations in $$.\n",
    "    Ensure the entire output is less than 3600 tokens long and does not include an extra line at the end of the Markdown.\n",
    "    '''\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def generate_completion(prompt):\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    finish_reason = completion.choices[0]['finish_reason']\n",
    "    message = completion.choices[0].message.content\n",
    "    return finish_reason, message, completion\n",
    "\n",
    "\n",
    "def generate_json(message, topic):\n",
    "    message = message.strip()\n",
    "    json_string = re.search(r'(?s){\\s*\\\"[^\"]+\\\":\\s*{.*?}\\s*}', message, re.DOTALL)\n",
    "    \n",
    "    if json_string:\n",
    "        json_string = json_string.group()\n",
    "        json_string = json_string.lower()\n",
    "        json_object = json.loads(json_string)\n",
    "\n",
    "        if json_object['title'] != topic:\n",
    "            print(\"Error: Title field does not match topic.\")\n",
    "            exit(1)\n",
    "\n",
    "        with open('wiki-connections.json', 'r') as file:\n",
    "            existing_data = json.load(file)\n",
    "\n",
    "        existing_data.update(json_object)\n",
    "        \n",
    "        with open('wiki-connections.json', 'w') as file:\n",
    "            json.dump(existing_data, file, indent=4)\n",
    "    else:\n",
    "        print(\"Error: Could not extract JSON from message.\")\n",
    "        exit(1)\n",
    "\n",
    "\n",
    "def generate_markdown(message, topic):\n",
    "    message = message.strip()\n",
    "    markdown_start_pos = message.find('#')\n",
    "    markdown_content = message[markdown_start_pos:].strip() + '\\n'\n",
    "\n",
    "    md_filename = topic + '.md'\n",
    "    with open(md_filename, 'w') as file:\n",
    "        file.write(markdown_content)\n",
    "\n",
    "    destination_folder = 'data'\n",
    "    shutil.move(md_filename, destination_folder)\n",
    "\n",
    "\n",
    "def generate_js(topic):\n",
    "    md_filename = topic + '.md'\n",
    "    js_string = f'''\n",
    "    import React from 'react';\n",
    "    import path from 'path';\n",
    "    import fs from 'fs';\n",
    "    import PageContent from '@/components/PageContent/PageContent';\n",
    "\n",
    "    const filename = '{md_filename}';\n",
    "\n",
    "    export default function MarkdownPage({{ markdownContent }}) {{\n",
    "    return <PageContent content={{markdownContent}} filename={{filename}} />;\n",
    "    }}\n",
    "\n",
    "    export async function getStaticProps() {{\n",
    "    const filePath = path.join(process.cwd(), 'data', filename);\n",
    "    const markdownContent = fs.readFileSync(filePath, 'utf8');\n",
    "    return {{\n",
    "        props: {{\n",
    "        markdownContent,\n",
    "        }},\n",
    "    }};\n",
    "    }}\n",
    "    '''\n",
    "    js_filename = topic + '.js'\n",
    "    with open(js_filename, 'w') as file:\n",
    "        file.write(js_string)\n",
    "\n",
    "    destination_folder = 'pages'\n",
    "    shutil.move(js_filename, destination_folder)\n",
    "\n",
    "\n",
    "def extract_markdown(message):\n",
    "    markdown_start = message.find('```')\n",
    "    markdown_end = message.rfind('```')\n",
    "    markdown_string = message[markdown_start:markdown_end+3]\n",
    "    return markdown_string\n",
    "\n",
    "\n",
    "def save_visited_pages(visited_pages, file_name='visited_pages.pickle'):\n",
    "    with open(file_name, 'wb') as handle:\n",
    "        pickle.dump(visited_pages, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def load_visited_pages(file_name='visited_pages.pickle'):\n",
    "    try:\n",
    "        with open(file_name, 'rb') as handle:\n",
    "            visited_pages = pickle.load(handle)\n",
    "        return visited_pages\n",
    "    except FileNotFoundError:\n",
    "        return set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW GENERATING: policy_gradient_methods\n",
      "stop\n",
      "{\n",
      "    \"policy_gradient_methods\": {\n",
      "        \"title\": \"Policy Gradient Methods\",\n",
      "        \"prerequisites\": [\"reinforcement_learning\", \"markov_decision_processes\", \"value_iteration\", \"q_learning\", \"monte_carlo_methods\", \"stochastic_processes\", \"dynamic_programming\", \"gradient_descent\", \"neural_networks\", \"backpropagation\"],\n",
      "        \"further_readings\": [\"actor_critic_methods\", \"proximal_policy_optimization\", \"deep_deterministic_policy_gradient\", \"trust_region_policy_optimization\", \"asynchronous_advantage_actor_critic\", \"policy_gradient_theorem\", \"off_policy_reinforcement_learning\", \"multi_agent_reinforcement_learning\", \"imitation_learning\", \"exploration_vs_exploitation\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "# Policy Gradient Methods\n",
      "\n",
      "Policy Gradient Methods is a set of reinforcement learning algorithms used to optimize the policy of an agent in a Markov decision process. The policy is a mapping from states to actions and is optimized to maximize the expected reward. Unlike value-based methods that try to learn the optimal value function and derive the policy from it, policy gradient methods directly optimize the policy. \n",
      "\n",
      "Policy Gradient Methods are important because they can handle stochastic policies and continuous action spaces, which cannot be handled easily by value-based methods. They can also be used for problems where the reward function is not explicitly defined, such as in inverse reinforcement learning.\n",
      "\n",
      "## How it works\n",
      "\n",
      "Policy Gradient Methods use gradient ascent to update the policy parameters. The gradient of the expected reward with respect to the policy parameters is computed using the policy gradient theorem. The policy is then updated by taking a step in the direction of the gradient.\n",
      "\n",
      "$$ \\Delta \\theta = \\alpha \\nabla_\\theta J(\\pi_\\theta) $$\n",
      "\n",
      "where $\\theta$ are the policy parameters, $\\pi_\\theta$ is the policy, $J(\\pi_\\theta)$ is the expected reward, and $\\alpha$ is the learning rate.\n",
      "\n",
      "There are several variants of policy gradient methods, such as:\n",
      "\n",
      "- REINFORCE: The simplest policy gradient method that uses Monte Carlo estimation to estimate the expected reward.\n",
      "- Actor-Critic: A combination of policy-based and value-based methods that uses a separate value function to reduce the variance of the policy gradient.\n",
      "- Proximal Policy Optimization (PPO): A recent algorithm that uses a surrogate objective function to prevent large policy updates and improve stability.\n",
      "- Deep Deterministic Policy Gradient (DDPG): An off-policy algorithm that uses a deterministic policy and a replay buffer to improve sample efficiency.\n",
      "- Trust Region Policy Optimization (TRPO): An algorithm that constrains the policy updates to ensure that the new policy is not too different from the old policy.\n",
      "\n",
      "## Applications\n",
      "\n",
      "Policy Gradient Methods have been successfully applied to a wide range of problems, such as:\n",
      "\n",
      "- Game playing: AlphaGo, AlphaZero, OpenAI Five\n",
      "- Robotics: locomotion, manipulation, grasping\n",
      "- Natural language processing: dialogue systems, machine translation\n",
      "- Finance: portfolio optimization, algorithmic trading\n",
      "\n",
      "## Limitations\n",
      "\n",
      "Policy Gradient Methods suffer from high variance and slow convergence compared to value-based methods. They also require careful tuning of the learning rate and other hyperparameters. Training can be computationally expensive, especially when using deep neural networks.\n",
      "\n",
      "## Further Readings\n",
      "\n",
      "- Actor-Critic Methods\n",
      "- Proximal Policy Optimization\n",
      "- Deep Deterministic Policy Gradient\n",
      "- Trust Region Policy Optimization\n",
      "- Asynchronous Advantage Actor-Critic\n",
      "- Policy Gradient Theorem\n",
      "- Off-Policy Reinforcement Learning\n",
      "- Multi-Agent Reinforcement Learning\n",
      "- Imitation Learning\n",
      "- Exploration vs Exploitation\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "generate_json() missing 1 required positional argument: 'topic'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mif\u001b[39;00m finish_reason \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mstop\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     17\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m generate_json(message)\n\u001b[1;32m     20\u001b[0m generate_markdown(message, topic)\n\u001b[1;32m     21\u001b[0m generate_js(topic)\n",
      "\u001b[0;31mTypeError\u001b[0m: generate_json() missing 1 required positional argument: 'topic'"
     ]
    }
   ],
   "source": [
    "visited_pages = load_visited_pages()\n",
    "queue = ['policy_gradient_methods']\n",
    "\n",
    "while queue:\n",
    "    topic = queue.pop(0)\n",
    "    topic = topic.lower()\n",
    "    if topic in visited_pages:\n",
    "        continue\n",
    "\n",
    "    print('NOW GENERATING:', topic)\n",
    "    prompt = generate_prompt(topic)\n",
    "    finish_reason, message, completion = generate_completion(prompt)\n",
    "    print(finish_reason)\n",
    "    print(message)\n",
    "\n",
    "    if finish_reason != 'stop':\n",
    "        continue\n",
    "    \n",
    "    generate_json(message, topic)\n",
    "    generate_markdown(message, topic)\n",
    "    generate_js(topic)\n",
    "\n",
    "    visited_pages.add(topic)\n",
    "    save_visited_pages(visited_pages)\n",
    "\n",
    "    with open('wiki-connections.json', 'r') as file:\n",
    "        wiki_connections = json.load(file)\n",
    "        queue += wiki_connections[topic]['prerequisites']\n",
    "        queue += wiki_connections[topic]['further_readings']\n",
    "\n",
    "    print('DONE GENERATING:', topic)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
