{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/alaney2/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import shutil\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import nltk\n",
    "from dotenv import load_dotenv\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "load_dotenv('.env.local')\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(topic):\n",
    "    parts = topic.split('_')\n",
    "    parts = [part.capitalize() for part in parts]\n",
    "    topic = ' '.join(parts)\n",
    "    prompt = f'Topic: {topic}\\n' + '''\n",
    "    You are a world-renowned AI and ML expert.\n",
    "    Provide a JSON object containing the topic, a list of 1-12 prerequisite topics, and a list of 1-12 further readings related to AI, ML, and DL.\n",
    "    The name of the JSON object must match exactly with the given topic.\n",
    "    Ensure that the prerequisites and further readings are specifically relevant to the given, rather than broad topics like calculus or statistics.\n",
    "    When generating topics, prefer the singular form of the topic, such as \"convolutional_neural_network\" instead of \"convolutional_neural_networks\" but use the plural form when it makes more sense to (\"policy_gradient_methods\").\n",
    "    Ensure that the title field is properly capitalized and spaced and has the right punctuation (such as Q-Learning).\n",
    "    Also ensure that the topic, prerequisites, and further readings are in snake_case (no dashes).\n",
    "    Use a similar format to the example provided below.:\n",
    "\n",
    "    Example:\n",
    "    {\n",
    "        \"generative_adversarial_network\": {\n",
    "            \"title\": \"Generative Adversarial Network\",\n",
    "            \"prerequisites\": [\"expectation_maximization_algorithm\", \"probability_distributions\", \"convolutional_neural_networks\", \"backpropagation\", \"stochastic_gradient_descent\", \"loss_functions\", \"optimization_algorithms\", \"deep_learning_frameworks\", \"regularization_techniques\", \"unsupervised_learning\"],\n",
    "            \"further_readings\": [\"conditional_gans\", \"cycle_gans\", \"stylegan_and_stylegan2\", \"wasserstein_gans\", \"domain_adaptation\", \"image_to_image_translation\", \"semi_supervised_learning\", \"adversarial_training\", \"adversarial_attacks_and_defenses\", \"transfer_learning\"]\n",
    "        }\n",
    "    }\n",
    "    Next, write a detailed wiki page about the given topic in Markdown format. Always write from a third-person perspective and remain unopinionated.\n",
    "    Ensure that this wiki page is explicitly in code format. \n",
    "    Do not include a \"Contents\" section. \n",
    "    Do not include a \"Further Readings\" nor a \"Prerequisites\" section if they just include related topics.\n",
    "    Use a neutral, unbiased tone without exclamation marks. \n",
    "    Ensure that the heading is the same as the title in the JSON object.\n",
    "    Follow Markdown syntax for headings and formatting, and use LaTeX for equations, with inline equations in pairs of $ and multiline equations in $$.\n",
    "    Ensure the entire output is less than 3600 tokens long and does not include an extra line at the end of the Markdown.\n",
    "    '''\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def generate_completion(prompt):\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    finish_reason = completion.choices[0]['finish_reason']\n",
    "    message = completion.choices[0].message.content\n",
    "    return finish_reason, message, completion\n",
    "\n",
    "\n",
    "def generate_json(message, topic):\n",
    "    message = message.strip()\n",
    "    json_string = re.search(r'(?s){\\s*\\\"[^\"]+\\\":\\s*{.*?}\\s*}', message, re.DOTALL)\n",
    "    \n",
    "    if json_string:\n",
    "        json_string = json_string.group()\n",
    "        # json_string = json_string.lower()\n",
    "        json_string = re.sub(r',\\s*([\\]}])', r'\\1', json_string)\n",
    "        json_object = json.loads(json_string)\n",
    "        # title_words = json_object[topic]['title'].split()\n",
    "        # result_title = [word.capitalize() if word.lower() not in stop_words else word for word in title_words]\n",
    "        # result_title = \" \".join(result_title)\n",
    "        # json_object[topic]['title'] = result_title\n",
    "        # print(json_object[topic]['title'])\n",
    "\n",
    "        # if topic not in json_string:\n",
    "        #     print(\"Error: Could not find topic in JSON.\")\n",
    "        #     exit(1)\n",
    "        with open('wiki-connections.json', 'r') as file:\n",
    "            existing_data = json.load(file)\n",
    "        existing_data.update(json_object)\n",
    "        with open('wiki-connections.json', 'w') as file:\n",
    "            json.dump(existing_data, file, indent=4)\n",
    "    else:\n",
    "        print(\"Error: Could not extract JSON from message.\")\n",
    "        exit(1)\n",
    "\n",
    "\n",
    "def generate_markdown(message, topic):\n",
    "    message = message.strip()\n",
    "    markdown_start_pos = message.find('#')\n",
    "    markdown_content = message[markdown_start_pos:].strip() + '\\n'\n",
    "\n",
    "    md_filename = topic + '.md'\n",
    "    with open(md_filename, 'w') as file:\n",
    "        file.write(markdown_content)\n",
    "\n",
    "    destination_folder = 'data'\n",
    "    shutil.move(md_filename, destination_folder)\n",
    "\n",
    "\n",
    "def generate_js(topic):\n",
    "    md_filename = topic + '.md'\n",
    "    js_string = f'''\n",
    "    import React from 'react';\n",
    "    import path from 'path';\n",
    "    import fs from 'fs';\n",
    "    import PageContent from '@/components/PageContent/PageContent';\n",
    "\n",
    "    const filename = '{md_filename}';\n",
    "\n",
    "    export default function MarkdownPage({{ markdownContent }}) {{\n",
    "    return <PageContent content={{markdownContent}} filename={{filename}} />;\n",
    "    }}\n",
    "\n",
    "    export async function getStaticProps() {{\n",
    "    const filePath = path.join(process.cwd(), 'data', filename);\n",
    "    const markdownContent = fs.readFileSync(filePath, 'utf8');\n",
    "    return {{\n",
    "        props: {{\n",
    "        markdownContent,\n",
    "        }},\n",
    "    }};\n",
    "    }}\n",
    "    '''\n",
    "    js_filename = topic + '.js'\n",
    "    with open(js_filename, 'w') as file:\n",
    "        file.write(js_string)\n",
    "\n",
    "    destination_folder = 'pages'\n",
    "    shutil.move(js_filename, destination_folder)\n",
    "\n",
    "\n",
    "def extract_markdown(message):\n",
    "    markdown_start = message.find('```')\n",
    "    markdown_end = message.rfind('```')\n",
    "    markdown_string = message[markdown_start:markdown_end+3]\n",
    "    return markdown_string\n",
    "\n",
    "\n",
    "def save_visited_pages(visited_pages, file_name='visited_pages.pickle'):\n",
    "    with open(file_name, 'wb') as handle:\n",
    "        pickle.dump(visited_pages, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def load_visited_pages(file_name='visited_pages.pickle'):\n",
    "    try:\n",
    "        with open(file_name, 'rb') as handle:\n",
    "            visited_pages = pickle.load(handle)\n",
    "        return visited_pages\n",
    "    except FileNotFoundError:\n",
    "        return set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "visited_pages = load_visited_pages()\n",
    "visited_pages.update([''])\n",
    "save_visited_pages(visited_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW GENERATING: bayesian_optimization\n",
      "FINISH_REASON: stop\n",
      "{\n",
      "    \"bayesian_optimization\": {\n",
      "        \"title\": \"Bayesian Optimization\",\n",
      "        \"prerequisites\": [\"probability_distributions\", \"stochastic_processes\", \"surrogate_modeling\", \"gaussian_processes\", \"optimization_algorithms\", \"machine_learning_algorithms\", \"hyperparameter_tuning\", \"gradient_descent\", \"convex_optimization\"],\n",
      "        \"further_readings\": [\"bayesian_optimization_with_deep_learning\", \"multi_fidelity_bayesian_optimization\", \"bayesian_optimization_for_reinforcement_learning\", \"bayesian_optimization_for_neural_network_architecture_search\", \"bayesian_global_optimization\", \"bayesian_optimization_for_simulator_based_inference\", \"bayesian_optimization_for_transfer_learning\", \"bayesian_optimization_for_active_learning\", \"differential_evolution\", \"particle_swarm_optimization\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "# Bayesian Optimization\n",
      "\n",
      "Bayesian Optimization is a probabilistic approach to the optimization of black-box functions with expensive evaluations. It is widely used in machine learning research and has been applied to hyperparameter tuning in deep learning. The method involves constructing a probabilistic model of the function and using it to guide the search for the optimum. \n",
      "\n",
      "## Methodology\n",
      "\n",
      "Bayesian Optimization typically involves the following steps:\n",
      "\n",
      "1. Choose a probabilistic model to represent the unknown function. Gaussian Processes are a popular choice due to their flexibility and ability to model uncertainty.\n",
      "\n",
      "2. Define an acquisition function, which is a heuristic that measures the expected utility of evaluating the function at a particular point in the search space. Common acquisition functions include Expected Improvement, Probability of Improvement, and Upper Confidence Bound.\n",
      "\n",
      "3. Optimize the acquisition function to obtain the next point to evaluate.\n",
      "\n",
      "4. Evaluate the function at the chosen point and update the probabilistic model.\n",
      "\n",
      "5. Repeat steps 2-4 until a stopping criterion is met.\n",
      "\n",
      "## Advantages\n",
      "\n",
      "Bayesian Optimization has several advantages over other optimization methods:\n",
      "\n",
      "- It is efficient at finding global optima in high-dimensional search spaces with limited evaluations.\n",
      "- It can handle noisy and non-smooth functions, as well as non-convex and non-linear constraints.\n",
      "- It can incorporate prior knowledge and constraints into the model.\n",
      "- It can be used with different types of functions, such as black-box simulations, machine learning algorithms, and physical experiments.\n",
      "\n",
      "## Applications\n",
      "\n",
      "Bayesian Optimization has a wide range of applications in machine learning, engineering, and science:\n",
      "\n",
      "- Hyperparameter tuning for deep learning models, such as convolutional neural networks and recurrent neural networks.\n",
      "- Model selection and validation for machine learning algorithms, such as support vector machines and decision trees.\n",
      "- Parameter estimation and optimization for simulation-based inference, such as Bayesian inference and Markov Chain Monte Carlo.\n",
      "- Control and optimization for physical systems, such as robotics and aerospace engineering.\n",
      "- Experimental design and optimization for scientific experiments, such as drug discovery and material science.\n",
      "\n",
      "## Limitations\n",
      "\n",
      "Bayesian Optimization also has some limitations and challenges:\n",
      "\n",
      "- It requires careful selection and tuning of the probabilistic model, the acquisition function, and the optimization algorithm.\n",
      "- It can be computationally expensive and time-consuming, especially for complex functions and high-dimensional search spaces.\n",
      "- It can suffer from local optima and suboptimal solutions if the model is misspecified or the search space is too narrow.\n",
      "- It can be sensitive to the initial conditions and the choice of the prior distribution.\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "Bayesian Optimization is a powerful and versatile method for optimizing black-box functions with expensive evaluations. It combines probabilistic modeling, acquisition function optimization, and surrogate modeling to efficiently explore the search space and find the global optimum. Its applications range from hyperparameter tuning to experimental design, and it has the potential to revolutionize optimization in machine learning and beyond.\n",
      "DONE GENERATING: bayesian_optimization\n",
      "NOW GENERATING: stochastic_processes\n",
      "FINISH_REASON: stop\n",
      "{\n",
      "    \"stochastic_processes\": {\n",
      "        \"title\": \"Stochastic Processes\",\n",
      "        \"prerequisites\": [\"probability_theory\", \"random_variables\", \"probability_distributions\", \"markov_chains\", \"martingales\", \"brownian_motion\", \"time_series_analysis\", \"statistics\", \"linear_algebra\"],\n",
      "        \"further_readings\": [\"stochastic_calculus\", \"point_processes\", \"queueing_theory\", \"random_walks\", \"renewal_theory\", \"monte_carlo_simulation\", \"hidden_markov_models\", \"bayesian_inference\", \"machine_learning\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "# Stochastic Processes\n",
      "\n",
      "A stochastic process is a mathematical model that describes the evolution of a system over time, where the system is subject to random fluctuations. Stochastic processes are used in various fields, including physics, finance, engineering, and computer science. In AI, ML, and DL, stochastic processes are widely used for modeling uncertainty and making predictions.\n",
      "\n",
      "## Probability Theory\n",
      "\n",
      "Probability theory is the foundation of stochastic processes. It deals with the study of random events and the likelihood of their occurrence. A good understanding of probability theory is essential for understanding stochastic processes.\n",
      "\n",
      "## Random Variables\n",
      "\n",
      "Random variables are variables that take on different values according to the outcome of a random event. They are used to model the uncertainty in a system. A stochastic process can be thought of as a collection of random variables indexed by time.\n",
      "\n",
      "## Probability Distributions\n",
      "\n",
      "Probability distributions describe the likelihood of different outcomes in a random event. They play a crucial role in stochastic processes as they determine the behavior of the random variables.\n",
      "\n",
      "## Markov Chains\n",
      "\n",
      "A Markov chain is a type of stochastic process where the future state of the system depends only on the current state and not on the past states. Markov chains are widely used in AI, ML, and DL for modeling sequential data.\n",
      "\n",
      "## Martingales\n",
      "\n",
      "Martingales are a type of stochastic process where the expected value of the next value in the sequence is equal to the current value, given the current information available. Martingales are used in finance and gambling for modeling fair games.\n",
      "\n",
      "## Brownian Motion\n",
      "\n",
      "Brownian motion is a type of stochastic process where the system undergoes random fluctuations in a continuous manner. Brownian motion is used in physics and finance for modeling the movement of particles and stock prices, respectively.\n",
      "\n",
      "## Time Series Analysis\n",
      "\n",
      "Time series analysis is the study of data that is collected over time. Stochastic processes are used in time series analysis for modeling the underlying dynamics of the data.\n",
      "\n",
      "## Statistics\n",
      "\n",
      "Statistics is the study of collecting, analyzing, and interpreting data. Stochastic processes are used in statistics for modeling the uncertainty in the data.\n",
      "\n",
      "## Linear Algebra\n",
      "\n",
      "Linear algebra is the study of linear equations and their properties. It is used extensively in stochastic processes for representing and manipulating matrices and vectors.\n",
      "\n",
      "## Further Readings\n",
      "\n",
      "- Stochastic Calculus\n",
      "- Point Processes\n",
      "- Queueing Theory\n",
      "- Random Walks\n",
      "- Renewal Theory\n",
      "- Monte Carlo Simulation\n",
      "- Hidden Markov Models\n",
      "- Bayesian Inference\n",
      "- Machine Learning\n",
      "DONE GENERATING: stochastic_processes\n",
      "NOW GENERATING: surrogate_modeling\n",
      "FINISH_REASON: stop\n",
      "{\n",
      "    \"surrogate_modeling\": {\n",
      "        \"title\": \"Surrogate Modeling\",\n",
      "        \"prerequisites\": [\"regression_analysis\", \"design_of_experiments\", \"response_surface_methodology\", \"optimization_algorithms\", \"machine_learning_algorithms\"],\n",
      "        \"further_readings\": [\"kriging\", \"support_vector_regression\", \"neural_networks_for_surrogate_modeling\", \"genetic_algorithms_for_surrogate_modeling\", \"ensemble_methods_for_surrogate_modeling\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "# Surrogate Modeling\n",
      "\n",
      "Surrogate modeling, also known as metamodeling, is a technique used to construct a simplified model of a complex, computationally expensive system. This model, known as a surrogate model, can be used to predict the behavior of the original system, often at a fraction of the computational cost required to run the original model. The surrogate model is usually constructed by training a machine learning algorithm on a set of inputs and outputs obtained from the original model.\n",
      "\n",
      "## Applications\n",
      "\n",
      "Surrogate modeling has applications in a variety of fields, including engineering, finance, and environmental science. In engineering, surrogate models are often used to optimize the design of complex systems, such as aircraft engines or wind turbines. In finance, surrogate models can be used to predict the behavior of financial markets or to optimize investment portfolios. In environmental science, surrogate models can be used to predict the impact of climate change on ecosystems.\n",
      "\n",
      "## Techniques\n",
      "\n",
      "There are several techniques that can be used to construct surrogate models, including regression analysis, design of experiments, response surface methodology, and optimization algorithms. Regression analysis involves fitting a mathematical function to a set of data points. Design of experiments involves selecting a set of input values to test the original model and using the results to construct the surrogate model. Response surface methodology involves fitting a mathematical function to the results of a series of experiments. Optimization algorithms involve searching for the optimal set of input values that maximize or minimize a particular output variable.\n",
      "\n",
      "## Machine Learning Algorithms\n",
      "\n",
      "Machine learning algorithms, such as support vector regression, neural networks, and genetic algorithms, can also be used to construct surrogate models. Support vector regression involves finding a hyperplane that maximizes the distance between the data points and the hyperplane. Neural networks use a network of interconnected nodes to learn the relationship between the input and output variables. Genetic algorithms use a process of natural selection and mutation to optimize the surrogate model.\n",
      "\n",
      "## Further Readings\n",
      "\n",
      "For more information on surrogate modeling, the following topics are recommended: kriging, support vector regression, neural networks for surrogate modeling, genetic algorithms for surrogate modeling, and ensemble methods for surrogate modeling.\n",
      "DONE GENERATING: surrogate_modeling\n",
      "NOW GENERATING: machine_learning_algorithms\n",
      "FINISH_REASON: stop\n",
      "{\n",
      "    \"machine_learning_algorithms\": {\n",
      "        \"title\": \"Machine Learning Algorithms\",\n",
      "        \"prerequisites\": [\"linear_algebra\", \"probability_theory\", \"statistics\", \"calculus\", \"data_structures\", \"algorithms\", \"programming_languages\", \"databases\", \"data_preprocessing\", \"feature_selection\"],\n",
      "        \"further_readings\": [\"deep_learning_algorithms\", \"reinforcement_learning_algorithms\", \"unsupervised_learning_algorithms\", \"supervised_learning_algorithms\", \"ensemble_learning_algorithms\", \"transfer_learning_algorithms\", \"online_learning_algorithms\", \"semi_supervised_learning_algorithms\", \"multi_task_learning_algorithms\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "# Machine Learning Algorithms\n",
      "\n",
      "Machine learning is a branch of artificial intelligence that utilizes statistical methods to enable machines to improve their performance on a specific task over time. Machine learning algorithms are the foundation of machine learning and are the set of procedures that enable machines to learn from data and make predictions or decisions.\n",
      "\n",
      "## Types of Machine Learning Algorithms\n",
      "\n",
      "There are three types of machine learning algorithms: supervised learning, unsupervised learning, and reinforcement learning.\n",
      "\n",
      "### Supervised Learning Algorithms\n",
      "\n",
      "Supervised learning algorithms are used when the data has labeled examples. In this type of learning, the algorithm learns from labeled data to predict outcomes for new, unseen data. Some popular supervised learning algorithms include decision trees, support vector machines, and neural networks.\n",
      "\n",
      "### Unsupervised Learning Algorithms\n",
      "\n",
      "Unsupervised learning algorithms are used when the data has no labeled examples. In this type of learning, the algorithm finds patterns and relationships in the data to group similar data together. Some popular unsupervised learning algorithms include k-means clustering, principal component analysis, and autoencoders.\n",
      "\n",
      "### Reinforcement Learning Algorithms\n",
      "\n",
      "Reinforcement learning algorithms are used in scenarios where the machine interacts with an environment and learns from the rewards obtained. The algorithm learns through trial and error to maximize the rewards it receives. Some popular reinforcement learning algorithms include Q-learning, policy gradient methods, and deep reinforcement learning.\n",
      "\n",
      "## Applications of Machine Learning Algorithms\n",
      "\n",
      "Machine learning algorithms have a wide range of applications in various fields, including but not limited to:\n",
      "\n",
      "- Image and video classification\n",
      "- Speech recognition\n",
      "- Natural language processing\n",
      "- Fraud detection\n",
      "- Recommender systems\n",
      "- Medical diagnosis\n",
      "- Autonomous vehicles\n",
      "- Robotics\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "Machine learning algorithms enable machines to learn and improve from data, making them an essential component of artificial intelligence. Supervised learning, unsupervised learning, and reinforcement learning are the three main types of machine learning algorithms, each with its own set of applications. As machine learning continues to evolve, so will the capabilities of machine learning algorithms, opening up new opportunities for innovation and discovery.\n",
      "DONE GENERATING: machine_learning_algorithms\n",
      "NOW GENERATING: hyperparameter_tuning\n",
      "FINISH_REASON: stop\n",
      "{\n",
      "    \"hyperparameter_tuning\": {\n",
      "        \"title\": \"Hyperparameter Tuning\",\n",
      "        \"prerequisites\": [\"machine_learning_algorithms\", \"optimization_algorithms\", \"cross_validation\", \"grid_search\", \"random_search\", \"bayesian_optimization\"],\n",
      "        \"further_readings\": [\"ensemble_methods\", \"neural_architecture_search\", \"transfer_learning\", \"meta_learning\", \"automl\", \"hyperband\", \"population_based_training\", \"evolutionary_algorithms\", \"multi_objective_optimization\", \"hyperparameter_importance\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "# Hyperparameter Tuning\n",
      "\n",
      "Hyperparameter tuning is the process of selecting the best hyperparameters for a machine learning algorithm to achieve the best possible performance. Hyperparameters are parameters that are set prior to the training of the model, such as learning rate, number of hidden layers, and regularization strength, and cannot be learned from the data. Therefore, it is crucial to choose the best set of hyperparameters to achieve the best performance of the model.\n",
      "\n",
      "## Techniques for Hyperparameter Tuning\n",
      "\n",
      "There are several techniques for hyperparameter tuning, including:\n",
      "\n",
      "### Grid Search\n",
      "\n",
      "Grid search is a technique where a predefined set of hyperparameters is tested exhaustively to find the best combination of hyperparameters. This technique can be computationally expensive, especially when the search space is large.\n",
      "\n",
      "### Random Search\n",
      "\n",
      "Random search is a technique where a random set of hyperparameters is tested to find the best combination of hyperparameters. This technique is less computationally expensive than grid search and can achieve similar results.\n",
      "\n",
      "### Bayesian Optimization\n",
      "\n",
      "Bayesian optimization is a technique that builds a probabilistic model of the function that maps hyperparameters to the performance of the model. The model is used to suggest the next set of hyperparameters to evaluate, based on the expected improvement in performance. This technique can achieve better results with fewer evaluations than grid search and random search.\n",
      "\n",
      "## Cross-Validation for Hyperparameter Tuning\n",
      "\n",
      "Cross-validation is a technique used to evaluate the performance of a model by partitioning the data into training and validation sets. Cross-validation is used in hyperparameter tuning to estimate the generalization performance of the model for a given set of hyperparameters. \n",
      "\n",
      "## Conclusion\n",
      "\n",
      "Hyperparameter tuning is an important step in machine learning to achieve the best performance of the model. Grid search, random search, and Bayesian optimization are popular techniques for hyperparameter tuning. Cross-validation is used to estimate the generalization performance of the model for a given set of hyperparameters.\n",
      "DONE GENERATING: hyperparameter_tuning\n",
      "NOW GENERATING: convex_optimization\n",
      "FINISH_REASON: stop\n",
      "{\n",
      "    \"convex_optimization\": {\n",
      "        \"title\": \"Convex Optimization\",\n",
      "        \"prerequisites\": [\"linear_algebra\", \"calculus\", \"optimization_algorithms\", \"gradient_descent\", \"lagrange_multipliers\"],\n",
      "        \"further_readings\": [\"non_convex_optimization\", \"semidefinite_programming\", \"interior_point_methods\", \"stochastic_gradient_descent\", \"proximal_gradient_methods\", \"duality_in_optimization\", \"online_learning\", \"matrix_completion\", \"compressed_sensing\", \"robust_optimization\", \"distributed_optimization\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "# Convex Optimization\n",
      "\n",
      "Convex optimization is a subfield of optimization that deals with minimizing convex functions over convex sets. It is a powerful tool in machine learning, as many problems in this field can be formulated as convex optimization problems. This wiki page will cover the basics of convex optimization, including definitions, examples, and algorithms.\n",
      "\n",
      "## Convexity\n",
      "\n",
      "A set $C\\subseteq\\mathbb{R}^n$ is convex if, for any two points $x,y\\in C$ and any $\\theta\\in[0,1]$, the point $\\theta x+(1-\\theta)y$ is also in $C$. In other words, a set is convex if the line segment connecting any two points in the set is entirely contained within the set.\n",
      "\n",
      "A function $f\\colon\\mathbb{R}^n\\to\\mathbb{R}$ is convex if, for any $x,y\\in\\mathbb{R}^n$ and any $\\theta\\in[0,1]$, we have $f(\\theta x+(1-\\theta)y)\\leq\\theta f(x)+(1-\\theta)f(y)$. In other words, a function is convex if the line segment connecting any two points on or above the graph of the function is entirely contained above the graph.\n",
      "\n",
      "## Convex Optimization Problems\n",
      "\n",
      "A convex optimization problem is an optimization problem of the form\n",
      "\n",
      "$$\n",
      "\\min_{x\\in\\mathbb{R}^n}f(x)\\quad\\text{s.t.}\\quad g_i(x)\\leq0,\\quad i=1,\\ldots,m,\\quad Ax=b,\n",
      "$$\n",
      "\n",
      "where $f$ is a convex function, $g_i$ are convex functions, and $A$ is a matrix. The inequality constraints $g_i(x)\\leq0$ define a convex set $C$, and the equality constraints $Ax=b$ define an affine set.\n",
      "\n",
      "Convex optimization problems have several desirable properties. For example, any local minimum of a convex function is also a global minimum, and any local minimum of a convex optimization problem is also a global minimum.\n",
      "\n",
      "## Algorithms\n",
      "\n",
      "There are several algorithms for solving convex optimization problems. One of the most basic algorithms is gradient descent, which iteratively updates the solution $x$ as\n",
      "\n",
      "$$\n",
      "x^{(k+1)}=x^{(k)}-\\alpha_k\\nabla f(x^{(k)}),\n",
      "$$\n",
      "\n",
      "where $\\alpha_k$ is a step size parameter and $\\nabla f(x)$ is the gradient of $f$ evaluated at $x$. Gradient descent can be slow for problems with highly curved surfaces, so other algorithms such as Newton's method or quasi-Newton methods may be more appropriate.\n",
      "\n",
      "Another popular algorithm is the method of Lagrange multipliers, which converts a constrained optimization problem into an unconstrained optimization problem by introducing Lagrange multipliers. The resulting unconstrained problem can then be solved using gradient descent or other algorithms.\n",
      "\n",
      "## Applications\n",
      "\n",
      "Convex optimization has many applications in machine learning, including linear regression, logistic regression, support vector machines, and neural network training. In these applications, the objective function is typically a loss function that measures the difference between the predicted output and the true output, and the constraints are often regularization terms that encourage certain properties of the model, such as sparsity or smoothness.\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "Convex optimization is a powerful tool in machine learning for solving optimization problems with convex objectives and constraints. It has many applications in various fields, including computer vision, natural language processing, and robotics. By understanding the basics of convex optimization and the algorithms used to solve it, researchers and practitioners can develop more effective and efficient machine learning models.\n"
     ]
    },
    {
     "ename": "Error",
     "evalue": "Destination path 'data/convex_optimization.md' already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m     19\u001b[0m generate_json(message, topic)\n\u001b[0;32m---> 20\u001b[0m generate_markdown(message, topic)\n\u001b[1;32m     21\u001b[0m generate_js(topic)\n\u001b[1;32m     23\u001b[0m visited_pages\u001b[39m.\u001b[39madd(topic)\n",
      "Cell \u001b[0;32mIn[44], line 86\u001b[0m, in \u001b[0;36mgenerate_markdown\u001b[0;34m(message, topic)\u001b[0m\n\u001b[1;32m     83\u001b[0m     file\u001b[39m.\u001b[39mwrite(markdown_content)\n\u001b[1;32m     85\u001b[0m destination_folder \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> 86\u001b[0m shutil\u001b[39m.\u001b[39;49mmove(md_filename, destination_folder)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/shutil.py:814\u001b[0m, in \u001b[0;36mmove\u001b[0;34m(src, dst, copy_function)\u001b[0m\n\u001b[1;32m    811\u001b[0m     real_dst \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(dst, _basename(src))\n\u001b[1;32m    813\u001b[0m     \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(real_dst):\n\u001b[0;32m--> 814\u001b[0m         \u001b[39mraise\u001b[39;00m Error(\u001b[39m\"\u001b[39m\u001b[39mDestination path \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m already exists\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m real_dst)\n\u001b[1;32m    815\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    816\u001b[0m     os\u001b[39m.\u001b[39mrename(src, real_dst)\n",
      "\u001b[0;31mError\u001b[0m: Destination path 'data/convex_optimization.md' already exists"
     ]
    }
   ],
   "source": [
    "visited_pages = load_visited_pages()\n",
    "queue = []\n",
    "with open('wiki-connections.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "    for key in data:\n",
    "        for new_topic in data[key]['prerequisites']:\n",
    "            if new_topic not in visited_pages and new_topic not in queue and new_topic != topic and new_topic not in data:\n",
    "                queue.append(new_topic)\n",
    "        for new_topic in data[key]['further_readings']:\n",
    "            if new_topic not in visited_pages and new_topic not in queue and new_topic != topic and new_topic not in data:\n",
    "                queue.append(new_topic)\n",
    "        if len(queue) > 0:\n",
    "            break\n",
    "\n",
    "# queue = ['bayesian_optimization']\n",
    "# visited_pages.remove('bayesian_optimization')\n",
    "\n",
    "while queue:\n",
    "    topic = queue.pop(0)\n",
    "    topic = topic.lower()\n",
    "\n",
    "    if topic in visited_pages:\n",
    "        continue\n",
    "\n",
    "    with open('wiki-connections.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "        if topic in data:\n",
    "            continue\n",
    "        while len(queue) == 0:\n",
    "            for key in data:\n",
    "                for new_topic in data[key]['prerequisites']:\n",
    "                    if new_topic not in visited_pages and new_topic not in queue and new_topic != topic and new_topic not in data:\n",
    "                        queue.append(new_topic)\n",
    "                for new_topic in data[key]['further_readings']:\n",
    "                    if new_topic not in visited_pages and new_topic not in queue and new_topic != topic and new_topic not in data:\n",
    "                        queue.append(new_topic)\n",
    "\n",
    "    print('NOW GENERATING:', topic)\n",
    "    prompt = generate_prompt(topic)\n",
    "    finish_reason, message, completion = generate_completion(prompt)\n",
    "    print(\"FINISH_REASON:\", finish_reason)\n",
    "    print(message)\n",
    "\n",
    "    if finish_reason != 'stop':\n",
    "        print(\"Error: Did not finish generating.\")\n",
    "        exit(1)\n",
    "    \n",
    "    generate_json(message, topic)\n",
    "    generate_markdown(message, topic)\n",
    "    generate_js(topic)\n",
    "\n",
    "    visited_pages.add(topic)\n",
    "    save_visited_pages(visited_pages)\n",
    "\n",
    "    with open('wiki-connections.json', 'r') as file:\n",
    "        wiki_connections = json.load(file)\n",
    "        queue += wiki_connections[topic]['prerequisites']\n",
    "        queue += wiki_connections[topic]['further_readings']\n",
    "\n",
    "    print('DONE GENERATING:', topic)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
