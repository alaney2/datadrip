{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/alaney2/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import shutil\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import nltk\n",
    "from dotenv import load_dotenv\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "load_dotenv('.env.local')\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(topic):\n",
    "    parts = topic.split('_')\n",
    "    parts = [part.capitalize() for part in parts]\n",
    "    topic = ' '.join(parts)\n",
    "    prompt = f'Topic: {topic}\\n' + '''\n",
    "    You are a world-renowned AI and ML expert.\n",
    "    Provide a JSON object containing the topic, a list of 0-8 prerequisite topics, and a list of 0-8 further readings related to AI, ML, and DL.\n",
    "    Ensure that the prerequisites and further readings are specifically relevant to the given, rather than broad topics like calculus or statistics.\n",
    "    When generating topics, prefer the singular form of the topic, such as \"convolutional_neural_network\" instead of \"convolutional_neural_networks\" but use the plural form when it makes more sense to (\"policy_gradient_methods\").\n",
    "    The name of the JSON object must match exactly with the given topic.\n",
    "    Ensure that the title field is properly capitalized and spaced and has the right punctuation (such as Q-Learning).\n",
    "    Also ensure that the topic, prerequisites, and further readings are in snake_case. Do not put single quotes anywhere in the JSON object.\n",
    "    Use a similar format to the example provided below and ensure that the JSON object is valid.:\n",
    "\n",
    "    Example:\n",
    "    {\n",
    "        \"topic_example\": {\n",
    "            \"title\": \"Topic Example\",\n",
    "            \"prerequisites\": [\"page_a\", \"page_b\", \"page_d\"],\n",
    "            \"further_readings\": [\"page_c\", \"page_f\", \"page_z\", \"page_s\"]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    Next, write a detailed wiki page about the given topic in Markdown format. Always write from a third-person perspective and remain unopinionated.\n",
    "    Ensure that this wiki page is explicitly in code format. \n",
    "    Do not include a \"Contents\" section. \n",
    "    Do not include a \"Further Readings\" nor a \"Prerequisites\" section if they just include related topics.\n",
    "    Use a neutral, unbiased tone without exclamation marks. \n",
    "    Ensure that the heading is the same as the title in the JSON object.\n",
    "    Follow Markdown syntax for headings and formatting, and use LaTeX for equations, with inline equations in pairs of $ and multiline equations in $$.\n",
    "    Ensure the entire output is less than 3600 tokens long and does not include an extra line at the end of the Markdown.\n",
    "    '''\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def generate_completion(prompt):\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        # model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.2,\n",
    "    )\n",
    "    finish_reason = completion.choices[0]['finish_reason']\n",
    "    message = completion.choices[0].message.content\n",
    "    return finish_reason, message, completion\n",
    "\n",
    "\n",
    "def generate_json(message, topic):\n",
    "    message = message.strip()\n",
    "    json_string = re.search(r'(?s){\\s*\\\"[^\"]+\\\":\\s*{.*?}\\s*}', message, re.DOTALL)\n",
    "    \n",
    "    if json_string:\n",
    "        json_string = json_string.group()\n",
    "        # json_string = json_string.lower()\n",
    "        json_string = re.sub(r',\\s*([\\]}])', r'\\1', json_string)\n",
    "        json_object = json.loads(json_string)\n",
    "        # title_words = json_object[topic]['title'].split()\n",
    "        # result_title = [word.capitalize() if word.lower() not in stop_words else word for word in title_words]\n",
    "        # result_title = \" \".join(result_title)\n",
    "        # json_object[topic]['title'] = result_title\n",
    "        # print(json_object[topic]['title'])\n",
    "\n",
    "        # if topic not in json_string:\n",
    "        #     print(\"Error: Could not find topic in JSON.\")\n",
    "        #     exit(1)\n",
    "        with open('wiki-connections.json', 'r') as file:\n",
    "            existing_data = json.load(file)\n",
    "        existing_data.update(json_object)\n",
    "        with open('wiki-connections.json', 'w') as file:\n",
    "            json.dump(existing_data, file, indent=4)\n",
    "        return True\n",
    "    else:\n",
    "        print(\"Error: Could not extract JSON from message.\")\n",
    "        return False\n",
    "        exit(1)\n",
    "\n",
    "\n",
    "def generate_markdown(message, topic):\n",
    "    if os.path.exists('data/' + topic + '.md'):\n",
    "        print(\"Error: Markdown file already exists.\")\n",
    "        return False\n",
    "\n",
    "    message = message.strip()\n",
    "    markdown_start_pos = message.find('#')\n",
    "    markdown_content = message[markdown_start_pos:].strip() + '\\n'\n",
    "\n",
    "    md_filename = topic + '.md'\n",
    "    with open(md_filename, 'w') as file:\n",
    "        file.write(markdown_content)\n",
    "\n",
    "    destination_folder = 'data'\n",
    "    shutil.move(md_filename, destination_folder)\n",
    "    return True\n",
    "\n",
    "def generate_js(topic):\n",
    "    if os.path.exists('pages/' + topic + '.js'):\n",
    "        print(\"Error: JS file already exists.\")\n",
    "        return False\n",
    "\n",
    "    md_filename = topic + '.md'\n",
    "    js_string = f'''\n",
    "    import React from 'react';\n",
    "    import path from 'path';\n",
    "    import fs from 'fs';\n",
    "    import PageContent from '@/components/PageContent/PageContent';\n",
    "\n",
    "    const filename = '{md_filename}';\n",
    "\n",
    "    export default function MarkdownPage({{ markdownContent }}) {{\n",
    "    return <PageContent content={{markdownContent}} filename={{filename}} />;\n",
    "    }}\n",
    "\n",
    "    export async function getStaticProps() {{\n",
    "    const filePath = path.join(process.cwd(), 'data', filename);\n",
    "    const markdownContent = fs.readFileSync(filePath, 'utf8');\n",
    "    return {{\n",
    "        props: {{\n",
    "        markdownContent,\n",
    "        }},\n",
    "    }};\n",
    "    }}\n",
    "    '''\n",
    "    js_filename = topic + '.js'\n",
    "    with open(js_filename, 'w') as file:\n",
    "        file.write(js_string)\n",
    "\n",
    "    destination_folder = 'pages'\n",
    "    shutil.move(js_filename, destination_folder)\n",
    "    return True\n",
    "\n",
    "\n",
    "def extract_markdown(message):\n",
    "    markdown_start = message.find('```')\n",
    "    markdown_end = message.rfind('```')\n",
    "    markdown_string = message[markdown_start:markdown_end+3]\n",
    "    return markdown_string\n",
    "\n",
    "\n",
    "def save_visited_pages(visited_pages, file_name='visited_pages.pickle'):\n",
    "    with open(file_name, 'wb') as handle:\n",
    "        pickle.dump(visited_pages, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def load_visited_pages(file_name='visited_pages.pickle'):\n",
    "    try:\n",
    "        with open(file_name, 'rb') as handle:\n",
    "            visited_pages = pickle.load(handle)\n",
    "        return visited_pages\n",
    "    except FileNotFoundError:\n",
    "        return set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "visited_pages = load_visited_pages()\n",
    "visited_pages.update([''])\n",
    "save_visited_pages(visited_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['imitation_learning_in_rl', 'off_policy_rl']\n",
      "NOW GENERATING: imitation_learning_in_rl\n",
      "FINISH_REASON: stop\n",
      "{\n",
      "    \"imitation_learning_in_rl\": {\n",
      "        \"title\": \"Imitation Learning in RL\",\n",
      "        \"prerequisites\": [\"reinforcement_learning\", \"supervised_learning\", \"neural_networks\"],\n",
      "        \"further_readings\": [\"inverse_reinforcement_learning\", \"imitation_learning_in_cv\", \"imitation_learning_in_nlp\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "# Imitation Learning in RL\n",
      "\n",
      "Imitation learning is a type of machine learning where an agent learns to perform a task by imitating the behavior of an expert. In reinforcement learning (RL), imitation learning is used to train an agent to perform a task by learning from the actions of an expert. This is done by providing the agent with a set of expert demonstrations, which it can use to learn a policy that mimics the expert's behavior.\n",
      "\n",
      "## Reinforcement Learning\n",
      "\n",
      "Reinforcement learning is a type of machine learning where an agent learns to perform a task by interacting with an environment. The agent receives rewards or punishments based on its actions, and its goal is to learn a policy that maximizes its long-term reward. Reinforcement learning is used in many applications, such as game playing, robotics, and autonomous driving.\n",
      "\n",
      "## Supervised Learning\n",
      "\n",
      "Supervised learning is a type of machine learning where an agent learns to perform a task by learning from labeled examples. The agent is provided with a set of input-output pairs, and its goal is to learn a function that maps inputs to outputs. Supervised learning is used in many applications, such as image classification, speech recognition, and natural language processing.\n",
      "\n",
      "## Neural Networks\n",
      "\n",
      "Neural networks are a type of machine learning model that are inspired by the structure and function of the brain. They consist of layers of interconnected nodes, where each node performs a simple computation. Neural networks are used in many applications, such as image recognition, speech synthesis, and natural language understanding.\n",
      "\n",
      "## Inverse Reinforcement Learning\n",
      "\n",
      "Inverse reinforcement learning is a type of machine learning where an agent learns the reward function of an expert by observing its behavior. This is done by assuming that the expert is optimizing a reward function, and then inferring that reward function from the expert's behavior. Inverse reinforcement learning is used in many applications, such as autonomous driving, robotics, and game playing.\n",
      "\n",
      "## Imitation Learning in Computer Vision\n",
      "\n",
      "Imitation learning is used in computer vision to train agents to perform tasks such as object detection, segmentation, and tracking. This is done by providing the agent with a set of expert demonstrations, which it can use to learn a policy that mimics the expert's behavior. Imitation learning is used in many applications, such as autonomous driving, robotics, and surveillance.\n",
      "\n",
      "## Imitation Learning in Natural Language Processing\n",
      "\n",
      "Imitation learning is used in natural language processing to train agents to perform tasks such as language translation, question answering, and text summarization. This is done by providing the agent with a set of expert demonstrations, which it can use to learn a policy that mimics the expert's behavior. Imitation learning is used in many applications, such as chatbots, virtual assistants, and customer service. \n",
      "\n",
      "In conclusion, imitation learning in RL is a powerful technique for training agents to perform tasks by imitating the behavior of an expert. It is used in many applications, such as game playing, robotics, and autonomous driving. By providing the agent with a set of expert demonstrations, it can learn a policy that mimics the expert's behavior and achieve high performance on the task.\n",
      "DONE GENERATING: imitation_learning_in_rl\n",
      "NOW GENERATING: off_policy_rl\n",
      "FINISH_REASON: stop\n",
      "{\n",
      "    \"off_policy_rl\": {\n",
      "        \"title\": \"Off Policy RL\",\n",
      "        \"prerequisites\": [\"reinforcement_learning\", \"markov_decision_process\", \"q_learning\"],\n",
      "        \"further_readings\": [\"importance_sampling\", \"monte_carlo_tree_search\", \"deep_reinforcement_learning\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "# Off Policy RL\n",
      "\n",
      "Off-policy reinforcement learning (RL) is a type of RL where the agent learns from a policy that is different from the one it is currently following. In other words, the agent learns from the experiences generated by a different policy than the one it is currently using to interact with the environment. This is in contrast to on-policy RL, where the agent learns from the experiences generated by the same policy it is currently following.\n",
      "\n",
      "Off-policy RL is useful in situations where the optimal policy is difficult to learn directly, or where the agent needs to explore different policies to learn more about the environment. It is also useful in situations where the agent needs to learn from a human expert or from a pre-existing dataset.\n",
      "\n",
      "One of the main challenges in off-policy RL is the problem of importance sampling. Importance sampling is a technique used to estimate the value of a function under one distribution using samples generated from a different distribution. In off-policy RL, importance sampling is used to estimate the value of the target policy using samples generated by the behavior policy.\n",
      "\n",
      "One popular off-policy RL algorithm is Q-learning. Q-learning is a model-free RL algorithm that learns the optimal action-value function for a given policy. The action-value function represents the expected reward for taking a particular action in a particular state and following a particular policy. Q-learning uses the Bellman equation to update the action-value function at each time step.\n",
      "\n",
      "Another popular off-policy RL algorithm is Monte Carlo Tree Search (MCTS). MCTS is a tree-based search algorithm that is commonly used in games such as Go and Chess. MCTS uses a combination of tree search and Monte Carlo simulation to find the best move to make in a given state.\n",
      "\n",
      "Deep reinforcement learning (DRL) is a subfield of RL that combines deep learning with RL. DRL has been used to solve a wide range of complex problems, including playing Atari games and controlling robots. DRL algorithms such as Deep Q-Networks (DQN) and Asynchronous Advantage Actor-Critic (A3C) can be used for off-policy RL.\n",
      "\n",
      "Off-policy RL is a powerful technique that can be used to solve a wide range of problems in RL. However, it also comes with its own set of challenges, such as the problem of importance sampling. Researchers continue to explore new algorithms and techniques to overcome these challenges and improve the performance of off-policy RL algorithms.\n",
      "DONE GENERATING: off_policy_rl\n",
      "NOW GENERATING: game_theory\n",
      "FINISH_REASON: stop\n",
      "{\n",
      "    \"game_theory\": {\n",
      "        \"title\": \"Game Theory\",\n",
      "        \"prerequisites\": [\"nash_equilibrium\", \"mixed_strategy_nash_equilibrium\", \"dominant_strategy\"],\n",
      "        \"further_readings\": [\"minimax_theorem\", \"iterated_elimination_of_dominated_strategies\", \"auction_theory\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "# Game Theory\n",
      "\n",
      "Game theory is a branch of mathematics that studies decision-making in situations where multiple individuals or entities are involved. It provides a framework for analyzing the behavior of rational agents in strategic interactions, where the outcome of each agent's decision depends on the decisions of others.\n",
      "\n",
      "Game theory has applications in various fields, including economics, political science, psychology, biology, and computer science. It is used to model and analyze a wide range of phenomena, such as market competition, voting systems, bargaining, social dilemmas, and military conflicts.\n",
      "\n",
      "## Nash Equilibrium\n",
      "\n",
      "One of the central concepts in game theory is the Nash equilibrium, named after John Nash, who introduced it in his seminal paper \"Non-Cooperative Games\" in 1950. A Nash equilibrium is a set of strategies, one for each player, such that no player can improve their payoff by unilaterally changing their strategy, given the strategies of the other players.\n",
      "\n",
      "For example, consider the classic game of Prisoner's Dilemma, where two suspects are arrested and interrogated separately. If both confess, they each get a moderate sentence. If one confesses and the other remains silent, the confessor goes free while the other gets a harsh sentence. If both remain silent, they each get a light sentence.\n",
      "\n",
      "The Nash equilibrium of this game is for both suspects to confess, even though it leads to a worse outcome for both than if they both remained silent. This is because each suspect has an incentive to confess, regardless of what the other does, as confessing always yields a higher payoff than remaining silent.\n",
      "\n",
      "## Mixed Strategy Nash Equilibrium\n",
      "\n",
      "In some games, there may not be a pure strategy Nash equilibrium, where each player chooses a single strategy with certainty. Instead, there may be a mixed strategy Nash equilibrium, where each player randomizes over a set of strategies with certain probabilities.\n",
      "\n",
      "For example, consider the game of Matching Pennies, where two players simultaneously choose heads or tails. If the choices match, player 1 wins, otherwise player 2 wins. There is no pure strategy Nash equilibrium in this game, as each player's best response depends on the other's choice. However, there is a mixed strategy Nash equilibrium where each player chooses heads and tails with equal probability.\n",
      "\n",
      "## Dominant Strategy\n",
      "\n",
      "A dominant strategy is a strategy that yields a higher payoff than any other strategy, regardless of what the other players do. If a player has a dominant strategy, they will always choose it, regardless of what the other players do.\n",
      "\n",
      "For example, in the game of Rock-Paper-Scissors, each player has a dominant strategy of choosing their move randomly with equal probability, as this ensures that they cannot be exploited by the other player.\n",
      "\n",
      "## Further Readings\n",
      "\n",
      "- Minimax Theorem\n",
      "- Iterated Elimination of Dominated Strategies\n",
      "- Auction Theory\n",
      "DONE GENERATING: game_theory\n",
      "NOW GENERATING: decision_trees\n",
      "FINISH_REASON: stop\n",
      "{\n",
      "    \"decision_trees\": {\n",
      "        \"title\": \"Decision Trees\",\n",
      "        \"prerequisites\": [\"supervised_learning\", \"classification_algorithms\", \"information_gain\"],\n",
      "        \"further_readings\": [\"random_forests\", \"gradient_boosting\", \"ensemble_learning\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "# Decision Trees\n",
      "\n",
      "Decision Trees are a popular machine learning algorithm used for both classification and regression tasks. They are a type of supervised learning algorithm that can be used for both categorical and continuous input and output variables. Decision Trees are easy to understand and interpret, making them a popular choice for data scientists.\n",
      "\n",
      "## How Decision Trees Work\n",
      "\n",
      "A Decision Tree is a tree-like model where each internal node represents a test on an attribute, each branch represents the outcome of the test, and each leaf node represents a class label. The tree is built by recursively splitting the data into subsets based on the value of a single attribute. The attribute that provides the most information gain is chosen as the splitting criterion.\n",
      "\n",
      "Information gain is a measure of the reduction in entropy achieved by partitioning the data based on a given attribute. Entropy is a measure of the impurity of a set of examples. A set with all examples belonging to the same class has an entropy of 0, while a set with an equal number of examples from each class has an entropy of 1.\n",
      "\n",
      "## Advantages and Disadvantages of Decision Trees\n",
      "\n",
      "One advantage of Decision Trees is that they are easy to understand and interpret. They can also handle both categorical and continuous input and output variables. Decision Trees can be used for both classification and regression tasks, and they can handle missing values and noisy data.\n",
      "\n",
      "However, Decision Trees can be prone to overfitting, especially when the tree is deep. Overfitting occurs when the model is too complex and fits the training data too closely, resulting in poor generalization to new data. Decision Trees can also be sensitive to small changes in the data, which can result in different trees being generated.\n",
      "\n",
      "## Further Readings\n",
      "\n",
      "- Random Forests: A popular ensemble learning method that uses multiple Decision Trees to improve performance and reduce overfitting.\n",
      "- Gradient Boosting: Another ensemble learning method that combines multiple weak learners to create a strong learner.\n",
      "- Ensemble Learning: A general technique for combining multiple models to improve performance and reduce overfitting.\n",
      "DONE GENERATING: decision_trees\n",
      "NOW GENERATING: heuristics\n",
      "FINISH_REASON: stop\n",
      "{\n",
      "    \"heuristics\": {\n",
      "        \"title\": \"Heuristics\",\n",
      "        \"prerequisites\": [\"search_algorithms\", \"optimization_algorithms\"],\n",
      "        \"further_readings\": [\"decision_trees\", \"genetic_algorithms\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "# Heuristics\n",
      "\n",
      "Heuristics are problem-solving techniques that use practical methods to find solutions that are not guaranteed to be optimal but are good enough for the given problem. In artificial intelligence (AI), heuristics are used to solve complex problems that are difficult to solve using traditional methods. \n",
      "\n",
      "## Types of Heuristics\n",
      "\n",
      "There are several types of heuristics used in AI, including:\n",
      "\n",
      "- **Greedy Heuristics**: These heuristics make decisions based on the best available option at the time, without considering the long-term consequences. Greedy heuristics are often used in optimization problems.\n",
      "\n",
      "- **Admissible Heuristics**: These heuristics provide an estimate of the cost to reach the goal state from the current state. Admissible heuristics are used in search algorithms such as A*.\n",
      "\n",
      "- **Consistent Heuristics**: These heuristics ensure that the estimated cost to reach the goal state is always less than or equal to the cost of any successor state plus the estimated cost of reaching the goal state from that successor state. Consistent heuristics are also used in search algorithms such as A*.\n",
      "\n",
      "- **Dominance Heuristics**: These heuristics eliminate options that are dominated by other options. Dominance heuristics are often used in multi-objective optimization problems.\n",
      "\n",
      "## Applications of Heuristics\n",
      "\n",
      "Heuristics are used in a variety of AI applications, including:\n",
      "\n",
      "- **Search Algorithms**: Heuristics are used to guide search algorithms such as A* and IDA* to find the optimal path to the goal state.\n",
      "\n",
      "- **Optimization Algorithms**: Heuristics are used to find good solutions to optimization problems such as the traveling salesman problem and the knapsack problem.\n",
      "\n",
      "- **Game Playing**: Heuristics are used to evaluate game states in games such as chess and Go to determine the best move to make.\n",
      "\n",
      "- **Machine Learning**: Heuristics are used to preprocess data and feature selection in machine learning algorithms.\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "Heuristics are an important tool in AI for solving complex problems that are difficult to solve using traditional methods. By using practical methods to find good solutions, heuristics enable AI systems to make decisions quickly and efficiently.\n",
      "DONE GENERATING: heuristics\n",
      "NOW GENERATING: search_algorithms\n",
      "FINISH_REASON: stop\n",
      "{\n",
      "    \"search_algorithms\": {\n",
      "        \"title\": \"Search Algorithms\",\n",
      "        \"prerequisites\": [\"graph_theory\", \"data_structures\", \"sorting_algorithms\"],\n",
      "        \"further_readings\": [\"heuristic_search\", \"genetic_algorithms\", \"simulated_annealing\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "# Search Algorithms\n",
      "\n",
      "Search algorithms are a fundamental concept in computer science and artificial intelligence. These algorithms are used to find a solution to a problem by exploring a search space. A search space is a collection of possible solutions to a problem. The goal of a search algorithm is to find the best solution in the search space.\n",
      "\n",
      "## Types of Search Algorithms\n",
      "\n",
      "There are several types of search algorithms, including:\n",
      "\n",
      "- **Breadth-First Search (BFS)**: This algorithm explores all the nodes at the current depth before moving on to the next depth level. BFS is guaranteed to find the shortest path to a solution in an unweighted graph.\n",
      "\n",
      "- **Depth-First Search (DFS)**: This algorithm explores as far as possible along each branch before backtracking. DFS is often used in conjunction with backtracking algorithms.\n",
      "\n",
      "- **Uniform-Cost Search (UCS)**: This algorithm expands the node with the lowest cost. UCS is guaranteed to find the optimal solution in a weighted graph.\n",
      "\n",
      "- **A* Search**: This algorithm combines the best features of BFS and UCS. A* uses a heuristic function to estimate the cost of reaching the goal node. A* is guaranteed to find the optimal solution in a weighted graph.\n",
      "\n",
      "- **Greedy Best-First Search**: This algorithm expands the node that is closest to the goal node according to a heuristic function. Greedy Best-First Search is not guaranteed to find the optimal solution.\n",
      "\n",
      "## Heuristic Search\n",
      "\n",
      "Heuristic search is a type of search algorithm that uses a heuristic function to estimate the cost of reaching the goal node. Heuristic functions are used to guide the search algorithm towards the goal node. Heuristic search algorithms include:\n",
      "\n",
      "- **Hill Climbing**: This algorithm starts at a random point and moves in the direction of the steepest ascent. Hill Climbing is not guaranteed to find the optimal solution.\n",
      "\n",
      "- **Simulated Annealing**: This algorithm is similar to Hill Climbing, but it allows for occasional moves in the opposite direction to avoid getting stuck in local optima.\n",
      "\n",
      "- **Genetic Algorithms**: This algorithm uses a population of candidate solutions and applies genetic operators such as mutation and crossover to generate new solutions. Genetic Algorithms are often used in optimization problems.\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "Search algorithms are a fundamental concept in computer science and artificial intelligence. They are used to find a solution to a problem by exploring a search space. There are several types of search algorithms, including BFS, DFS, UCS, A* Search, and Greedy Best-First Search. Heuristic search algorithms use a heuristic function to estimate the cost of reaching the goal node. Heuristic search algorithms include Hill Climbing, Simulated Annealing, and Genetic Algorithms.\n",
      "DONE GENERATING: search_algorithms\n",
      "NOW GENERATING: alpha_zero\n",
      "FINISH_REASON: stop\n",
      "{\n",
      "    \"alpha_zero\": {\n",
      "        \"title\": \"Alpha Zero\",\n",
      "        \"prerequisites\": [\"reinforcement_learning\", \"monte_carlo_tree_search\", \"convolutional_neural_networks\"],\n",
      "        \"further_readings\": [\"deep_reinforcement_learning\", \"policy_gradient_methods\", \"value_iteration\", \"neural_network_architectures\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "# Alpha Zero\n",
      "\n",
      "Alpha Zero is an artificial intelligence program developed by DeepMind Technologies that is capable of learning and mastering complex games such as chess, shogi, and Go. It uses a combination of deep reinforcement learning and Monte Carlo tree search to achieve superhuman performance in these games.\n",
      "\n",
      "## Reinforcement Learning\n",
      "\n",
      "Reinforcement learning is a type of machine learning where an agent learns to make decisions by interacting with an environment and receiving rewards or punishments based on its actions. In the case of Alpha Zero, the environment is the game board and the rewards are based on whether the agent wins or loses the game.\n",
      "\n",
      "## Monte Carlo Tree Search\n",
      "\n",
      "Monte Carlo tree search is a decision-making algorithm that is used to find the best move in a game. It works by simulating many possible moves and outcomes and selecting the move that leads to the best outcome on average. Alpha Zero uses Monte Carlo tree search to explore the game tree and select the best move.\n",
      "\n",
      "## Convolutional Neural Networks\n",
      "\n",
      "Convolutional neural networks are a type of neural network that are commonly used in image recognition tasks. They are designed to recognize patterns in images by using filters that scan the image and extract features. Alpha Zero uses convolutional neural networks to evaluate the game board and predict the outcome of a game.\n",
      "\n",
      "## Deep Reinforcement Learning\n",
      "\n",
      "Deep reinforcement learning is a combination of reinforcement learning and deep learning. It involves training a neural network to make decisions based on the rewards received from the environment. Alpha Zero uses deep reinforcement learning to learn how to play games and improve its performance over time.\n",
      "\n",
      "## Policy Gradient Methods\n",
      "\n",
      "Policy gradient methods are a type of reinforcement learning algorithm that directly optimize the policy of an agent. They work by adjusting the parameters of the policy to maximize the expected reward. Alpha Zero uses policy gradient methods to improve its decision-making ability.\n",
      "\n",
      "## Value Iteration\n",
      "\n",
      "Value iteration is a dynamic programming algorithm that is used to find the optimal policy for a Markov decision process. It works by iteratively updating the value function until it converges to the optimal value function. Alpha Zero uses value iteration to improve its decision-making ability.\n",
      "\n",
      "## Neural Network Architectures\n",
      "\n",
      "Neural network architectures are the structure of a neural network. They determine how the neurons are connected and how the information flows through the network. Alpha Zero uses a specific neural network architecture that is designed to evaluate game boards and predict the outcome of a game.\n",
      "\n",
      "In conclusion, Alpha Zero is a groundbreaking artificial intelligence program that uses a combination of deep reinforcement learning and Monte Carlo tree search to achieve superhuman performance in complex games such as chess, shogi, and Go. It relies on a variety of AI, ML, and DL techniques such as convolutional neural networks, policy gradient methods, and value iteration to improve its decision-making ability.\n",
      "DONE GENERATING: alpha_zero\n",
      "NOW GENERATING: uct_algorithm\n",
      "FINISH_REASON: stop\n",
      "{\n",
      "    \"uct_algorithm\": {\n",
      "        \"title\": \"UCT Algorithm\",\n",
      "        \"prerequisites\": [\"monte_carlo_tree_search\", \"bandit_algorithms\"],\n",
      "        \"further_readings\": [\"upper_confidence_bounds\", \"multi-armed_bandits\", \"reinforcement_learning\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "# UCT Algorithm\n",
      "\n",
      "The Upper Confidence Bound applied to Trees (UCT) algorithm is a popular method for solving decision-making problems in artificial intelligence (AI). It is a variant of the Monte Carlo Tree Search (MCTS) algorithm that uses a bandit algorithm to balance exploration and exploitation of the search space.\n",
      "\n",
      "## Background\n",
      "\n",
      "The UCT algorithm was first introduced in 2006 by Kocsis and Szepesv√°ri as a way to solve the game of Go. It has since been applied to a wide range of decision-making problems, including robotics, finance, and gaming.\n",
      "\n",
      "## Algorithm\n",
      "\n",
      "The UCT algorithm is a variant of the MCTS algorithm that uses a bandit algorithm to balance exploration and exploitation of the search space. It works by building a tree of possible actions and their outcomes, and then selecting the most promising action based on a combination of the expected reward and the uncertainty of that reward.\n",
      "\n",
      "At each step of the algorithm, the UCT algorithm selects a node in the tree to expand. It does this by choosing the node with the highest Upper Confidence Bound (UCB) value, which is calculated as follows:\n",
      "\n",
      "$$\n",
      "UCB_i = \\frac{w_i}{n_i} + c \\sqrt{\\frac{\\ln N}{n_i}}\n",
      "$$\n",
      "\n",
      "where $w_i$ is the total reward of node $i$, $n_i$ is the number of times node $i$ has been visited, $N$ is the total number of times the parent node has been visited, and $c$ is a constant that controls the balance between exploration and exploitation.\n",
      "\n",
      "Once a node has been selected for expansion, the UCT algorithm simulates a random play-out from that node to the end of the game or a predetermined depth. The result of the play-out is then back-propagated up the tree, updating the reward and visit count of each node along the way.\n",
      "\n",
      "The UCT algorithm continues to select and expand nodes until a predetermined stopping criterion is met, such as a maximum number of iterations or a time limit.\n",
      "\n",
      "## Applications\n",
      "\n",
      "The UCT algorithm has been successfully applied to a wide range of decision-making problems, including:\n",
      "\n",
      "- Game playing: The UCT algorithm has been used to solve games such as Go, chess, and poker.\n",
      "- Robotics: The UCT algorithm has been used to plan robot trajectories and control robot behavior.\n",
      "- Finance: The UCT algorithm has been used to optimize investment portfolios and predict stock prices.\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "The UCT algorithm is a powerful method for solving decision-making problems in AI. It combines the strengths of MCTS and bandit algorithms to balance exploration and exploitation of the search space. The UCT algorithm has been successfully applied to a wide range of problems, from game playing to finance, and is likely to continue to be an important tool in the AI toolbox.\n",
      "DONE GENERATING: uct_algorithm\n",
      "NOW GENERATING: bandit_algorithms\n",
      "FINISH_REASON: stop\n",
      "{\n",
      "    \"bandit_algorithms\": {\n",
      "        \"title\": \"Bandit Algorithms\",\n",
      "        \"prerequisites\": [\"reinforcement_learning\", \"multi-armed_bandit\"],\n",
      "        \"further_readings\": [\"upper_confidence_bound\", \"thompson_sampling\", \"epsilon_greedy\", \"contextual_bandit\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "# Bandit Algorithms\n",
      "\n",
      "Bandit algorithms are a class of online learning algorithms that are used to solve the multi-armed bandit problem. The multi-armed bandit problem is a classic problem in probability theory and decision theory, where a gambler has to decide which of several slot machines to play, each with a different payout probability. The gambler's goal is to maximize his or her total winnings over a number of plays.\n",
      "\n",
      "In the context of machine learning, the multi-armed bandit problem is a simplified version of the reinforcement learning problem, where an agent has to learn to take actions in an environment to maximize a reward signal. In the multi-armed bandit problem, the agent has to choose between a set of actions, each with an unknown reward distribution, and has to learn to maximize its total reward over a number of trials.\n",
      "\n",
      "Bandit algorithms are designed to balance exploration and exploitation in the multi-armed bandit problem. Exploration refers to the process of trying out different actions to learn about their reward distributions, while exploitation refers to the process of choosing the action with the highest expected reward based on the current knowledge. The goal of bandit algorithms is to find the optimal balance between exploration and exploitation to maximize the total reward.\n",
      "\n",
      "There are several types of bandit algorithms, each with its own exploration-exploitation trade-off strategy. Some of the most popular bandit algorithms include:\n",
      "\n",
      "- **Epsilon-greedy algorithm**: This algorithm chooses the action with the highest expected reward with probability 1-epsilon, and chooses a random action with probability epsilon. This algorithm is simple and effective, but can be suboptimal if the optimal action has a low probability of being chosen.\n",
      "- **Upper confidence bound (UCB) algorithm**: This algorithm chooses the action with the highest upper confidence bound, which is a measure of the uncertainty in the estimated reward distribution. This algorithm is more exploratory than the epsilon-greedy algorithm, but can be more computationally expensive.\n",
      "- **Thompson sampling algorithm**: This algorithm samples the reward distribution for each action from a Bayesian posterior distribution, and chooses the action with the highest expected reward based on the samples. This algorithm is more computationally expensive than the epsilon-greedy algorithm, but can be more effective in complex environments.\n",
      "- **Contextual bandit algorithm**: This algorithm takes into account the context or state of the environment when choosing an action, and learns a separate reward distribution for each context. This algorithm is more complex than the other bandit algorithms, but can be more effective in environments with complex state spaces.\n",
      "\n",
      "Bandit algorithms have many applications in machine learning, such as online advertising, recommendation systems, and clinical trials. They are particularly useful in situations where the reward distributions are unknown or change over time, and where exploration is necessary to learn about the environment.\n",
      "\n",
      "In summary, bandit algorithms are a class of online learning algorithms that are used to solve the multi-armed bandit problem. They balance exploration and exploitation to maximize the total reward over a number of trials. There are several types of bandit algorithms, each with its own exploration-exploitation trade-off strategy. Bandit algorithms have many applications in machine learning, particularly in situations where the reward distributions are unknown or change over time.\n",
      "DONE GENERATING: bandit_algorithms\n",
      "NOW GENERATING: multi_agent_mcts\n",
      "FINISH_REASON: stop\n",
      "{\n",
      "    \"multi_agent_mcts\": {\n",
      "        \"title\": \"Multi Agent Mcts\",\n",
      "        \"prerequisites\": [\"monte_carlo_tree_search\", \"multi_agent_systems\"],\n",
      "        \"further_readings\": [\"distributed_mcts\", \"multi_agent_reinforcement_learning\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "# Multi Agent Mcts\n",
      "\n",
      "Multi Agent Monte Carlo Tree Search (MAMCTS) is an extension of Monte Carlo Tree Search (MCTS) that is designed for multi-agent systems. MCTS is a popular algorithm for decision-making in single-agent systems, but it can be extended to multi-agent systems by using MAMCTS. \n",
      "\n",
      "MAMCTS is a game tree search algorithm that is used to find the optimal action for each agent in a multi-agent system. It works by simulating the game tree for each agent separately and then combining the results to determine the best action for each agent. \n",
      "\n",
      "## How it Works\n",
      "\n",
      "MAMCTS works by simulating the game tree for each agent separately. Each agent has its own MCTS tree that is used to determine the best action for that agent. The trees are updated in parallel, and the results are combined to determine the best action for each agent.\n",
      "\n",
      "The algorithm works as follows:\n",
      "\n",
      "1. Each agent initializes its own MCTS tree with the current state of the game.\n",
      "\n",
      "2. Each agent selects an action to take based on the current state of its MCTS tree.\n",
      "\n",
      "3. The actions are executed simultaneously, and the new state of the game is observed.\n",
      "\n",
      "4. Each agent updates its MCTS tree with the new state of the game.\n",
      "\n",
      "5. Steps 2-4 are repeated until a terminal state is reached.\n",
      "\n",
      "6. The results of each agent's MCTS tree are combined to determine the best action for each agent.\n",
      "\n",
      "## Advantages\n",
      "\n",
      "MAMCTS has several advantages over other multi-agent algorithms. \n",
      "\n",
      "First, it is a model-free algorithm, which means that it does not require a model of the environment. This makes it more flexible and adaptable to different environments.\n",
      "\n",
      "Second, it is a distributed algorithm, which means that it can be run on multiple machines in parallel. This makes it scalable and efficient for large-scale multi-agent systems.\n",
      "\n",
      "Finally, it is a general-purpose algorithm, which means that it can be applied to a wide range of multi-agent problems.\n",
      "\n",
      "## Applications\n",
      "\n",
      "MAMCTS has been applied to a variety of multi-agent problems, including:\n",
      "\n",
      "- RoboCup soccer\n",
      "- Trading agents\n",
      "- Multi-robot coordination\n",
      "- Multi-player games\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "Multi Agent Monte Carlo Tree Search (MAMCTS) is an extension of Monte Carlo Tree Search (MCTS) that is designed for multi-agent systems. It works by simulating the game tree for each agent separately and then combining the results to determine the best action for each agent. MAMCTS has several advantages over other multi-agent algorithms, including being model-free, distributed, and general-purpose. It has been applied to a variety of multi-agent problems, including RoboCup soccer, trading agents, multi-robot coordination, and multi-player games.\n",
      "DONE GENERATING: multi_agent_mcts\n",
      "NOW GENERATING: deep_mcts\n",
      "FINISH_REASON: stop\n",
      "{\n",
      "    \"deep_mcts\": {\n",
      "        \"title\": \"Deep Monte Carlo Tree Search\",\n",
      "        \"prerequisites\": [\"monte_carlo_tree_search\", \"neural_networks\"],\n",
      "        \"further_readings\": [\"alpha_go\", \"deep_reinforcement_learning\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "# Deep Monte Carlo Tree Search\n",
      "\n",
      "Deep Monte Carlo Tree Search (Deep MCTS) is a variant of Monte Carlo Tree Search (MCTS) that incorporates neural networks to improve the efficiency and effectiveness of the search algorithm. MCTS is a popular algorithm used in artificial intelligence for decision-making problems, particularly in games. It is a heuristic search algorithm that builds a search tree by simulating the game from the current state and selecting the best move based on the results of the simulations.\n",
      "\n",
      "## Monte Carlo Tree Search\n",
      "\n",
      "MCTS is a four-step process that involves selection, expansion, simulation, and backpropagation. The algorithm starts with a root node that represents the current state of the game. The selection step involves traversing the tree from the root node to a leaf node based on a selection policy, such as the Upper Confidence Bound (UCB) algorithm. The expansion step involves adding one or more child nodes to the selected leaf node, representing the possible moves from the current state. The simulation step involves playing out the game from the newly added child node to the end of the game using a rollout policy, such as a random policy. The backpropagation step involves updating the statistics of the nodes in the path from the selected leaf node to the root node based on the outcome of the simulation.\n",
      "\n",
      "MCTS is known for its ability to handle large state spaces and imperfect information games, such as Go and Poker. However, it can be computationally expensive, especially for games with long horizons and complex dynamics. Deep MCTS aims to address this issue by incorporating neural networks to guide the search process.\n",
      "\n",
      "## Neural Networks\n",
      "\n",
      "Neural networks are a type of machine learning algorithm that can learn complex patterns in data. They consist of layers of interconnected nodes that perform mathematical operations on the input data to produce an output. Deep neural networks are neural networks with multiple layers, allowing them to learn hierarchical representations of the input data.\n",
      "\n",
      "In Deep MCTS, neural networks are used to estimate the value and policy of the game states. The value network estimates the expected outcome of the game from a given state, while the policy network estimates the probability distribution over the possible moves from the state. These estimates are used to guide the selection and expansion steps of the MCTS algorithm, improving its efficiency and effectiveness.\n",
      "\n",
      "## Applications\n",
      "\n",
      "Deep MCTS has been successfully applied to various games, including Go, Chess, and Shogi. In 2016, AlphaGo, a computer program that uses Deep MCTS, defeated the world champion in Go, a game considered to be one of the most complex board games in the world. Deep MCTS has also been used in robotics and autonomous driving, where decision-making under uncertainty is a critical challenge.\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "Deep Monte Carlo Tree Search is a variant of Monte Carlo Tree Search that incorporates neural networks to improve the efficiency and effectiveness of the search algorithm. It has been successfully applied to various games and decision-making problems, demonstrating its potential for real-world applications.\n",
      "DONE GENERATING: deep_mcts\n",
      "NOW GENERATING: monte_carlo_simulation\n",
      "FINISH_REASON: stop\n",
      "{\n",
      "    \"monte_carlo_simulation\": {\n",
      "        \"title\": \"Monte Carlo Simulation\",\n",
      "        \"prerequisites\": [\"probability_theory\", \"random_variables\", \"statistical_inference\"],\n",
      "        \"further_readings\": [\"markov_chain_monte_carlo\", \"importance_sampling\", \"reinforcement_learning\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "# Monte Carlo Simulation\n",
      "\n",
      "Monte Carlo simulation is a computational technique used to estimate the probability of complex events by generating random samples. It is named after the famous casino in Monaco, which is known for its gambling and games of chance. Monte Carlo simulation is widely used in various fields, including finance, engineering, physics, and computer science.\n",
      "\n",
      "## Overview\n",
      "\n",
      "Monte Carlo simulation involves generating a large number of random samples to estimate the probability of an event. The basic steps involved in Monte Carlo simulation are as follows:\n",
      "\n",
      "1. Define the problem and identify the variables involved.\n",
      "2. Generate random samples for each variable based on its probability distribution.\n",
      "3. Simulate the system using the generated samples and calculate the outcome.\n",
      "4. Repeat steps 2 and 3 many times to obtain a distribution of outcomes.\n",
      "5. Analyze the distribution of outcomes to estimate the probability of the event.\n",
      "\n",
      "Monte Carlo simulation is particularly useful when the problem involves a large number of variables or when the analytical solution is difficult or impossible to obtain. It can also be used to evaluate the sensitivity of the outcome to changes in the input variables.\n",
      "\n",
      "## Applications\n",
      "\n",
      "Monte Carlo simulation has a wide range of applications in various fields. Some examples include:\n",
      "\n",
      "- Finance: Monte Carlo simulation is used to estimate the value-at-risk (VaR) of a portfolio of financial assets. It can also be used to simulate the behavior of stock prices and other financial variables.\n",
      "- Engineering: Monte Carlo simulation is used to evaluate the reliability of complex systems, such as aircraft engines and nuclear power plants. It can also be used to optimize the design of products and processes.\n",
      "- Physics: Monte Carlo simulation is used to simulate the behavior of particles in high-energy physics experiments. It can also be used to model the behavior of materials under extreme conditions.\n",
      "- Computer Science: Monte Carlo simulation is used in machine learning and artificial intelligence to estimate the performance of algorithms and models. It can also be used to simulate the behavior of complex systems, such as traffic flow and social networks.\n",
      "\n",
      "## Limitations\n",
      "\n",
      "Monte Carlo simulation has some limitations that should be taken into account when using it. Some of these limitations include:\n",
      "\n",
      "- Sample size: The accuracy of the estimate depends on the number of samples generated. A larger sample size generally leads to a more accurate estimate.\n",
      "- Input distributions: The accuracy of the estimate depends on the accuracy of the input distributions. If the input distributions are not accurate, the estimate may be biased or unreliable.\n",
      "- Computation time: Monte Carlo simulation can be computationally intensive, especially when the problem involves a large number of variables or when the simulation needs to be repeated many times.\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "Monte Carlo simulation is a powerful computational technique that can be used to estimate the probability of complex events. It has a wide range of applications in various fields, including finance, engineering, physics, and computer science. However, it also has some limitations that should be taken into account when using it. Overall, Monte Carlo simulation is a valuable tool for decision-making and risk analysis.\n",
      "DONE GENERATING: monte_carlo_simulation\n",
      "NOW GENERATING: bayesian_mcts\n",
      "FINISH_REASON: stop\n",
      "{\n",
      "    \"bayesian_mcts\": {\n",
      "        \"title\": \"Bayesian MCTS\",\n",
      "        \"prerequisites\": [\"monte_carlo_tree_search\", \"bayesian_inference\", \"reinforcement_learning\"],\n",
      "        \"further_readings\": [\"bayesian_optimization\", \"markov_decision_processes\", \"deep_reinforcement_learning\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "# Bayesian MCTS\n",
      "\n",
      "Bayesian Monte Carlo Tree Search (Bayesian MCTS) is a variant of Monte Carlo Tree Search (MCTS) that incorporates Bayesian inference to improve decision-making in reinforcement learning problems. It is a popular algorithm in the field of artificial intelligence, particularly in game AI.\n",
      "\n",
      "## Monte Carlo Tree Search\n",
      "\n",
      "Monte Carlo Tree Search is a decision-making algorithm that is commonly used in games and other decision-making problems. It is a heuristic search algorithm that builds a tree of possible actions and their outcomes, and uses Monte Carlo simulations to evaluate the value of each action. The algorithm selects the best action based on the results of the simulations.\n",
      "\n",
      "## Bayesian Inference\n",
      "\n",
      "Bayesian inference is a statistical technique that uses Bayes' theorem to update the probability of a hypothesis based on new evidence. It is commonly used in machine learning and artificial intelligence to make predictions and decisions based on uncertain data.\n",
      "\n",
      "## Reinforcement Learning\n",
      "\n",
      "Reinforcement learning is a type of machine learning that involves an agent learning to make decisions in an environment in order to maximize a reward signal. It is commonly used in game AI and robotics.\n",
      "\n",
      "## Bayesian MCTS Algorithm\n",
      "\n",
      "Bayesian MCTS is a variant of MCTS that incorporates Bayesian inference to improve decision-making. The algorithm works by maintaining a distribution over the values of each action in the tree, rather than just a single value. This distribution is updated using Bayesian inference after each simulation.\n",
      "\n",
      "The algorithm can be summarized in the following steps:\n",
      "\n",
      "1. Initialize a tree with a single root node.\n",
      "2. Select a leaf node to expand based on the UCB1 algorithm.\n",
      "3. Simulate a game from the selected node to a terminal state.\n",
      "4. Update the value distribution of each action in the path from the root to the selected node using Bayesian inference.\n",
      "5. Backpropagate the result of the simulation up the tree, updating the value distribution of each action in the path from the root to the selected node.\n",
      "\n",
      "The algorithm can be used in a variety of reinforcement learning problems, including games and robotics.\n",
      "\n",
      "## Advantages of Bayesian MCTS\n",
      "\n",
      "Bayesian MCTS has several advantages over traditional MCTS:\n",
      "\n",
      "1. It can handle uncertainty more effectively, allowing it to make better decisions in complex environments.\n",
      "2. It can learn from fewer simulations, making it more efficient in terms of computational resources.\n",
      "3. It can handle non-stationary environments, where the distribution of rewards may change over time.\n",
      "\n",
      "## Further Readings\n",
      "\n",
      "- [Bayesian Optimization](bayesian_optimization): A related technique for optimizing expensive black-box functions.\n",
      "- [Markov Decision Processes](markov_decision_processes): A framework for modeling decision-making problems.\n",
      "- [Deep Reinforcement Learning](deep_reinforcement_learning): A subfield of reinforcement learning that uses deep neural networks to learn policies.\n",
      "DONE GENERATING: bayesian_mcts\n",
      "NOW GENERATING: online_mcts\n",
      "FINISH_REASON: stop\n",
      "{\n",
      "    \"online_mcts\": {\n",
      "        \"title\": \"Online Monte Carlo Tree Search\",\n",
      "        \"prerequisites\": [\"monte_carlo_tree_search\", \"reinforcement_learning\", \"bandit_algorithms\"],\n",
      "        \"further_readings\": [\"upper_confidence_bounds\", \"multi_armed_bandits\", \"temporal_difference_learning\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "# Online Monte Carlo Tree Search\n",
      "\n",
      "Online Monte Carlo Tree Search (MCTS) is a variant of Monte Carlo Tree Search that is used in decision-making problems with large state spaces. It is a popular algorithm in the field of artificial intelligence, particularly in the subfields of reinforcement learning and game theory. MCTS is used to find the optimal decision in a given state by simulating the game or problem from that state and selecting the best action based on the results of the simulation.\n",
      "\n",
      "## Monte Carlo Tree Search\n",
      "\n",
      "Monte Carlo Tree Search (MCTS) is a decision-making algorithm that is used in problems with large state spaces. It is a heuristic search algorithm that uses random sampling to build a search tree of the game or problem. The algorithm is based on the principle of Monte Carlo simulation, which involves generating random samples to estimate the outcome of a problem. MCTS is used in a variety of applications, including game playing, robotics, and optimization.\n",
      "\n",
      "## Reinforcement Learning\n",
      "\n",
      "Reinforcement learning is a subfield of machine learning that is concerned with how agents learn to make decisions in an environment. The goal of reinforcement learning is to learn a policy that maximizes the expected cumulative reward over time. Reinforcement learning algorithms are used in a variety of applications, including robotics, game playing, and control systems.\n",
      "\n",
      "## Bandit Algorithms\n",
      "\n",
      "Bandit algorithms are a class of algorithms used in decision-making problems with limited information. The name \"bandit\" comes from the idea of a slot machine, where a player must decide which machine to play without knowing the payout probabilities. Bandit algorithms are used in a variety of applications, including online advertising, recommendation systems, and clinical trials.\n",
      "\n",
      "## Upper Confidence Bounds\n",
      "\n",
      "Upper Confidence Bounds (UCB) is a class of algorithms used in decision-making problems with limited information. The UCB algorithm is based on the principle of balancing exploration and exploitation. The algorithm selects the action with the highest upper confidence bound, which is a measure of how uncertain the algorithm is about the value of the action.\n",
      "\n",
      "## Multi-Armed Bandits\n",
      "\n",
      "Multi-Armed Bandits (MAB) is a class of problems in which a player must decide which arm of a bandit to pull in order to maximize their reward. The problem is similar to a slot machine, where the player must decide which machine to play without knowing the payout probabilities. MAB problems are used in a variety of applications, including online advertising, recommendation systems, and clinical trials.\n",
      "\n",
      "## Temporal Difference Learning\n",
      "\n",
      "Temporal Difference (TD) learning is a reinforcement learning algorithm that is used to learn a policy that maximizes the expected cumulative reward over time. The algorithm is based on the principle of bootstrapping, which involves estimating the value of a state based on the estimated value of the next state. TD learning is used in a variety of applications, including robotics, game playing, and control systems. \n",
      "\n",
      "In conclusion, Online Monte Carlo Tree Search is a powerful algorithm that is used in decision-making problems with large state spaces. It is a variant of Monte Carlo Tree Search that is particularly useful in the field of artificial intelligence, particularly in the subfields of reinforcement learning and game theory. By simulating the game or problem from a given state and selecting the best action based on the results of the simulation, the algorithm is able to find the optimal decision in a given state.\n",
      "DONE GENERATING: online_mcts\n",
      "NOW GENERATING: sampling_based_planning\n",
      "FINISH_REASON: stop\n",
      "{\n",
      "    \"sampling_based_planning\": {\n",
      "        \"title\": \"Sampling Based Planning\",\n",
      "        \"prerequisites\": [\"motion_planning\", \"probabilistic_robotics\"],\n",
      "        \"further_readings\": [\"rapidly_exploring_random_tree\", \"probabilistic_roadmap\", \"stochastic_optimization\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "# Sampling Based Planning\n",
      "\n",
      "Sampling based planning is a subfield of motion planning in robotics that involves generating feasible paths for a robot to move from its current position to a desired goal position. It is a popular approach due to its ability to handle complex environments and high-dimensional state spaces. \n",
      "\n",
      "## Overview\n",
      "\n",
      "Sampling based planning algorithms generate a set of random samples in the state space and use them to construct a graph that represents the connectivity of the space. The graph is then searched for a path from the start state to the goal state. The two main types of sampling based planning algorithms are:\n",
      "\n",
      "- Probabilistic Roadmap (PRM): PRM constructs a graph by connecting samples that are close to each other and collision-free. It then searches the graph for a path from the start to the goal state. PRM is particularly useful for high-dimensional state spaces.\n",
      "\n",
      "- Rapidly Exploring Random Tree (RRT): RRT constructs a tree by growing it from the start state towards the goal state. It randomly samples the state space and extends the tree towards the sample that is closest to it and collision-free. RRT is particularly useful for environments with narrow passages or obstacles.\n",
      "\n",
      "## Applications\n",
      "\n",
      "Sampling based planning algorithms have been successfully applied to a variety of robotics applications, including:\n",
      "\n",
      "- Autonomous driving: Sampling based planning algorithms are used to plan trajectories for autonomous vehicles to navigate through complex environments.\n",
      "\n",
      "- Robotic manipulation: Sampling based planning algorithms are used to plan the motion of robotic arms to manipulate objects in cluttered environments.\n",
      "\n",
      "- Unmanned aerial vehicles (UAVs): Sampling based planning algorithms are used to plan the trajectory of UAVs to perform tasks such as surveillance and inspection.\n",
      "\n",
      "## Further Readings\n",
      "\n",
      "- Rapidly Exploring Random Tree (RRT)\n",
      "- Probabilistic Roadmap (PRM)\n",
      "- Stochastic Optimization\n",
      "DONE GENERATING: sampling_based_planning\n",
      "NOW GENERATING: stochastic_search\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 54\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mNOW GENERATING:\u001b[39m\u001b[39m'\u001b[39m, topic)\n\u001b[1;32m     53\u001b[0m prompt \u001b[39m=\u001b[39m generate_prompt(topic)\n\u001b[0;32m---> 54\u001b[0m finish_reason, message, completion \u001b[39m=\u001b[39m generate_completion(prompt)\n\u001b[1;32m     55\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFINISH_REASON:\u001b[39m\u001b[39m\"\u001b[39m, finish_reason)\n\u001b[1;32m     56\u001b[0m \u001b[39mprint\u001b[39m(message)\n",
      "Cell \u001b[0;32mIn[2], line 37\u001b[0m, in \u001b[0;36mgenerate_completion\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_completion\u001b[39m(prompt):\n\u001b[0;32m---> 37\u001b[0m     completion \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m     38\u001b[0m         model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgpt-3.5-turbo\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     39\u001b[0m         \u001b[39m# model=\"gpt-4\",\u001b[39;49;00m\n\u001b[1;32m     40\u001b[0m         messages\u001b[39m=\u001b[39;49m[\n\u001b[1;32m     41\u001b[0m             {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: prompt}\n\u001b[1;32m     42\u001b[0m         ],\n\u001b[1;32m     43\u001b[0m         temperature\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m,\n\u001b[1;32m     44\u001b[0m     )\n\u001b[1;32m     45\u001b[0m     finish_reason \u001b[39m=\u001b[39m completion\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mfinish_reason\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     46\u001b[0m     message \u001b[39m=\u001b[39m completion\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mmessage\u001b[39m.\u001b[39mcontent\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/openai/api_requestor.py:216\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    206\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    207\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    214\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    215\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m--> 216\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest_raw(\n\u001b[1;32m    217\u001b[0m         method\u001b[39m.\u001b[39;49mlower(),\n\u001b[1;32m    218\u001b[0m         url,\n\u001b[1;32m    219\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    220\u001b[0m         supplied_headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    221\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[1;32m    222\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    223\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    224\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    225\u001b[0m     )\n\u001b[1;32m    226\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response(result, stream)\n\u001b[1;32m    227\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/openai/api_requestor.py:516\u001b[0m, in \u001b[0;36mAPIRequestor.request_raw\u001b[0;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    514\u001b[0m     _thread_context\u001b[39m.\u001b[39msession \u001b[39m=\u001b[39m _make_session()\n\u001b[1;32m    515\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 516\u001b[0m     result \u001b[39m=\u001b[39m _thread_context\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    517\u001b[0m         method,\n\u001b[1;32m    518\u001b[0m         abs_url,\n\u001b[1;32m    519\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    520\u001b[0m         data\u001b[39m=\u001b[39;49mdata,\n\u001b[1;32m    521\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[1;32m    522\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    523\u001b[0m         timeout\u001b[39m=\u001b[39;49mrequest_timeout \u001b[39mif\u001b[39;49;00m request_timeout \u001b[39melse\u001b[39;49;00m TIMEOUT_SECS,\n\u001b[1;32m    524\u001b[0m         proxies\u001b[39m=\u001b[39;49m_thread_context\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mproxies,\n\u001b[1;32m    525\u001b[0m     )\n\u001b[1;32m    526\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mTimeout \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    527\u001b[0m     \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mTimeout(\u001b[39m\"\u001b[39m\u001b[39mRequest timed out: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(e)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    704\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/requests/adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    488\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 489\u001b[0m         resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    490\u001b[0m             method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    491\u001b[0m             url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    492\u001b[0m             body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    493\u001b[0m             headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    494\u001b[0m             redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m             assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    496\u001b[0m             preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    497\u001b[0m             decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    498\u001b[0m             retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    499\u001b[0m             timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    500\u001b[0m         )\n\u001b[1;32m    502\u001b[0m     \u001b[39m# Send the request.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39mproxy_pool\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    704\u001b[0m     conn,\n\u001b[1;32m    705\u001b[0m     method,\n\u001b[1;32m    706\u001b[0m     url,\n\u001b[1;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    711\u001b[0m )\n\u001b[1;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/urllib3/connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    444\u001b[0m             httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[1;32m    445\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    450\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    451\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/urllib3/connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n\u001b[1;32m    443\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m         httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    445\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m         \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m         \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m         \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    449\u001b[0m         six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/http/client.py:1374\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1372\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1373\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1374\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1375\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[1;32m   1376\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1271\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1272\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1273\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1274\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1275\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1276\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1128\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1129\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1130\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1131\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1132\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "visited_pages = load_visited_pages()\n",
    "queue = []\n",
    "visited_pages.add('voxel-based_method')\n",
    "\n",
    "if not queue:\n",
    "    with open('wiki-connections.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "        for key in data:\n",
    "            for new_topic in data[key]['prerequisites']:\n",
    "                if os.path.exists('data/' + new_topic + '.md'):\n",
    "                    visited_pages.add(new_topic)\n",
    "                    continue\n",
    "                if new_topic not in visited_pages and new_topic not in queue and new_topic not in data:\n",
    "                    queue.append(new_topic)\n",
    "            for new_topic in data[key]['further_readings']:\n",
    "                if os.path.exists('data/' + new_topic + '.md'):\n",
    "                    visited_pages.add(new_topic)\n",
    "                    continue\n",
    "                if new_topic not in visited_pages and new_topic not in queue and new_topic not in data:\n",
    "                    queue.append(new_topic)\n",
    "            if len(queue) > 0:\n",
    "                break\n",
    "print(queue)\n",
    "\n",
    "while queue:\n",
    "    topic = queue.pop(0)\n",
    "    topic = topic.lower()\n",
    "    topic = topic.replace(\"'\", \"\")\n",
    "\n",
    "    if topic in visited_pages:\n",
    "        continue\n",
    "\n",
    "    with open('wiki-connections.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "        if topic in data:\n",
    "            continue\n",
    "        while len(queue) == 0:\n",
    "            for key in data:\n",
    "                for new_topic in data[key]['prerequisites']:\n",
    "                    if os.path.exists('data/' + new_topic + '.md'):\n",
    "                        visited_pages.add(new_topic)\n",
    "                        continue\n",
    "                    if new_topic not in visited_pages and new_topic not in queue and new_topic != topic and new_topic not in data:\n",
    "                        queue.append(new_topic)\n",
    "                for new_topic in data[key]['further_readings']:\n",
    "                    if os.path.exists('data/' + new_topic + '.md'):\n",
    "                        visited_pages.add(new_topic)\n",
    "                        continue\n",
    "                    if new_topic not in visited_pages and new_topic not in queue and new_topic != topic and new_topic not in data:\n",
    "                        queue.append(new_topic)\n",
    "\n",
    "    print('NOW GENERATING:', topic)\n",
    "    prompt = generate_prompt(topic)\n",
    "    finish_reason, message, completion = generate_completion(prompt)\n",
    "    print(\"FINISH_REASON:\", finish_reason)\n",
    "    print(message)\n",
    "\n",
    "    if finish_reason != 'stop':\n",
    "        print(\"Error: Did not finish generating.\")\n",
    "        exit(1)\n",
    "    \n",
    "    has_generated_json = generate_json(message, topic)\n",
    "    has_generated_markdown = generate_markdown(message, topic)\n",
    "    has_generated_js = generate_js(topic)\n",
    "\n",
    "    visited_pages.add(topic)\n",
    "    save_visited_pages(visited_pages)\n",
    "\n",
    "    if not has_generated_json or not has_generated_markdown or not has_generated_js:\n",
    "        exit(1)\n",
    "\n",
    "    # with open('wiki-connections.json', 'r') as file:\n",
    "    #     wiki_connections = json.load(file)\n",
    "    #     queue += wiki_connections[topic]['prerequisites']\n",
    "    #     queue += wiki_connections[topic]['further_readings']\n",
    "\n",
    "    print('DONE GENERATING:', topic)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
