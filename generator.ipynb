{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/alaney2/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import shutil\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import nltk\n",
    "from dotenv import load_dotenv\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "load_dotenv('.env.local')\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(topic):\n",
    "    parts = topic.split('_')\n",
    "    parts = [part.capitalize() for part in parts]\n",
    "    topic = ' '.join(parts)\n",
    "    prompt = f'Topic: {topic}\\n' + '''\n",
    "    You are a world-renowned AI and ML expert.\n",
    "    Provide a JSON object containing the topic, a list of 0-8 prerequisite topics, and a list of 0-8 further readings related to AI, ML, and DL.\n",
    "    The name of the JSON object must match exactly with the given topic.\n",
    "    Ensure that the prerequisites and further readings are specifically relevant to the given, rather than broad topics like calculus or statistics.\n",
    "    When generating topics, prefer the singular form of the topic, such as \"convolutional_neural_network\" instead of \"convolutional_neural_networks\" but use the plural form when it makes more sense to (\"policy_gradient_methods\").\n",
    "    Ensure that the title field is properly capitalized and spaced and has the right punctuation (such as Q-Learning).\n",
    "    Also ensure that the topic, prerequisites, and further readings are in snake_case. Do not put single quotes anywhere in the JSON object.\n",
    "    Use a similar format to the example provided below and ensure that the JSON object is valid.\n",
    "\n",
    "    Example:\n",
    "    {\n",
    "        \"generative_adversarial_network\": {\n",
    "            \"title\": \"Generative Adversarial Network\",\n",
    "            \"prerequisites\": [\"expectation_maximization_algorithm\", \"probability_distributions\", \"convolutional_neural_networks\", \"backpropagation\", \"stochastic_gradient_descent\", \"loss_functions\", \"optimization_algorithms\", \"deep_learning_frameworks\", \"regularization_techniques\", \"unsupervised_learning\"],\n",
    "            \"further_readings\": [\"conditional_gans\", \"cycle_gans\", \"stylegan_and_stylegan2\", \"wasserstein_gans\", \"domain_adaptation\", \"image_to_image_translation\", \"semi_supervised_learning\", \"adversarial_training\", \"adversarial_attacks_and_defenses\", \"transfer_learning\"]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    Next, write a detailed wiki page about the given topic in Markdown format. Always write from a third-person perspective and remain unopinionated.\n",
    "    Ensure that this wiki page is explicitly in code format. \n",
    "    Do not include a \"Contents\" section. \n",
    "    Do not include a \"Further Readings\" nor a \"Prerequisites\" section if they just include related topics.\n",
    "    Use a neutral, unbiased tone without exclamation marks. \n",
    "    Ensure that the heading is the same as the title in the JSON object.\n",
    "    Follow Markdown syntax for headings and formatting, and use LaTeX for equations, with inline equations in pairs of $ and multiline equations in $$.\n",
    "    Ensure the entire output is less than 3600 tokens long and does not include an extra line at the end of the Markdown.\n",
    "    '''\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def generate_completion(prompt):\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        # model=\"gpt-3.5-turbo\",\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    finish_reason = completion.choices[0]['finish_reason']\n",
    "    message = completion.choices[0].message.content\n",
    "    return finish_reason, message, completion\n",
    "\n",
    "\n",
    "def generate_json(message, topic):\n",
    "    message = message.strip()\n",
    "    json_string = re.search(r'(?s){\\s*\\\"[^\"]+\\\":\\s*{.*?}\\s*}', message, re.DOTALL)\n",
    "    \n",
    "    if json_string:\n",
    "        json_string = json_string.group()\n",
    "        # json_string = json_string.lower()\n",
    "        json_string = re.sub(r',\\s*([\\]}])', r'\\1', json_string)\n",
    "        json_object = json.loads(json_string)\n",
    "        # title_words = json_object[topic]['title'].split()\n",
    "        # result_title = [word.capitalize() if word.lower() not in stop_words else word for word in title_words]\n",
    "        # result_title = \" \".join(result_title)\n",
    "        # json_object[topic]['title'] = result_title\n",
    "        # print(json_object[topic]['title'])\n",
    "\n",
    "        # if topic not in json_string:\n",
    "        #     print(\"Error: Could not find topic in JSON.\")\n",
    "        #     exit(1)\n",
    "        with open('wiki-connections.json', 'r') as file:\n",
    "            existing_data = json.load(file)\n",
    "        existing_data.update(json_object)\n",
    "        with open('wiki-connections.json', 'w') as file:\n",
    "            json.dump(existing_data, file, indent=4)\n",
    "    else:\n",
    "        print(\"Error: Could not extract JSON from message.\")\n",
    "        exit(1)\n",
    "\n",
    "\n",
    "def generate_markdown(message, topic):\n",
    "    message = message.strip()\n",
    "    markdown_start_pos = message.find('#')\n",
    "    markdown_content = message[markdown_start_pos:].strip() + '\\n'\n",
    "\n",
    "    md_filename = topic + '.md'\n",
    "    with open(md_filename, 'w') as file:\n",
    "        file.write(markdown_content)\n",
    "\n",
    "    destination_folder = 'data'\n",
    "    shutil.move(md_filename, destination_folder)\n",
    "\n",
    "\n",
    "def generate_js(topic):\n",
    "    md_filename = topic + '.md'\n",
    "    js_string = f'''\n",
    "    import React from 'react';\n",
    "    import path from 'path';\n",
    "    import fs from 'fs';\n",
    "    import PageContent from '@/components/PageContent/PageContent';\n",
    "\n",
    "    const filename = '{md_filename}';\n",
    "\n",
    "    export default function MarkdownPage({{ markdownContent }}) {{\n",
    "    return <PageContent content={{markdownContent}} filename={{filename}} />;\n",
    "    }}\n",
    "\n",
    "    export async function getStaticProps() {{\n",
    "    const filePath = path.join(process.cwd(), 'data', filename);\n",
    "    const markdownContent = fs.readFileSync(filePath, 'utf8');\n",
    "    return {{\n",
    "        props: {{\n",
    "        markdownContent,\n",
    "        }},\n",
    "    }};\n",
    "    }}\n",
    "    '''\n",
    "    js_filename = topic + '.js'\n",
    "    with open(js_filename, 'w') as file:\n",
    "        file.write(js_string)\n",
    "\n",
    "    destination_folder = 'pages'\n",
    "    shutil.move(js_filename, destination_folder)\n",
    "\n",
    "\n",
    "def extract_markdown(message):\n",
    "    markdown_start = message.find('```')\n",
    "    markdown_end = message.rfind('```')\n",
    "    markdown_string = message[markdown_start:markdown_end+3]\n",
    "    return markdown_string\n",
    "\n",
    "\n",
    "def save_visited_pages(visited_pages, file_name='visited_pages.pickle'):\n",
    "    with open(file_name, 'wb') as handle:\n",
    "        pickle.dump(visited_pages, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def load_visited_pages(file_name='visited_pages.pickle'):\n",
    "    try:\n",
    "        with open(file_name, 'rb') as handle:\n",
    "            visited_pages = pickle.load(handle)\n",
    "        return visited_pages\n",
    "    except FileNotFoundError:\n",
    "        return set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "visited_pages = load_visited_pages()\n",
    "visited_pages.update([''])\n",
    "save_visited_pages(visited_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW GENERATING: confusion_matrix\n",
      "FINISH_REASON: stop\n",
      "{\n",
      "    \"confusion_matrix\": {\n",
      "        \"title\": \"Confusion Matrix\",\n",
      "        \"prerequisites\": [\"classification_algorithms\", \"binary_classification\", \"multiclass_classification\", \"evaluation_metrics\", \"true_positive_rate\", \"false_positive_rate\", \"precision\", \"recall\"],\n",
      "        \"further_readings\": [\"roc_curve\", \"auc_score\", \"f1_score\", \"balanced_accuracy\", \"cohen_kappa\", \"matthews_correlation_coefficient\", \"multilabel_classification\", \"imbalanced_classification\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "# Confusion Matrix\n",
      "\n",
      "A confusion matrix is a table that is used to describe the performance of a classification algorithm, also known as a classifier, on a set of data for which the true values are known. It is particularly useful for measuring the performance of binary and multiclass classification problems. In this representation, each row of the matrix represents the instances in a predicted class, while each column represents the instances in an actual class.\n",
      "\n",
      "## Binary Classification\n",
      "\n",
      "In binary classification, there are two possible outcomes: positive and negative. A confusion matrix for a binary classifier consists of four values:\n",
      "\n",
      "1. **True Positive (TP)**: The number of true positive instances, i.e., the number of positive instances that were correctly classified as positive by the classifier.\n",
      "2. **False Positive (FP)**: The number of false positive instances, i.e., the number of negative instances that were incorrectly classified as positive by the classifier.\n",
      "3. **True Negative (TN)**: The number of true negative instances, i.e., the number of negative instances that were correctly classified as negative by the classifier.\n",
      "4. **False Negative (FN)**: The number of false negative instances, i.e., the number of positive instances that were incorrectly classified as negative by the classifier.\n",
      "\n",
      "The confusion matrix for a binary classification problem is represented in the following format:\n",
      "\n",
      "```\n",
      "         Actual\n",
      "         P   N\n",
      "Predicted TP  FP\n",
      "         FN  TN\n",
      "```\n",
      "\n",
      "## Multiclass Classification\n",
      "\n",
      "In multiclass classification, there are more than two possible outcomes. A confusion matrix for a multiclass classifier consists of multiple values for each class. The confusion matrix for a multiclass classification problem is represented in the following format:\n",
      "\n",
      "```\n",
      "         Actual\n",
      "         C1  C2  ...  Cn\n",
      "Predicted\n",
      "C1        a   b   ...  c\n",
      "C2        d   e   ...  f\n",
      "...       ... ... ... ...\n",
      "Cn        g   h   ...  i\n",
      "```\n",
      "\n",
      "Where `C1`, `C2`, ..., `Cn` represent the different classes, and `a`, `b`, ..., `i` are the counts of instances for each predicted-actual class combination.\n",
      "\n",
      "## Evaluation Metrics\n",
      "\n",
      "A confusion matrix can be used to derive several evaluation metrics that provide different perspectives on the performance of a classifier. Some common evaluation metrics derived from a confusion matrix are:\n",
      "\n",
      "1. **Accuracy**: The proportion of correctly classified instances out of the total instances. It is calculated as $\\frac{TP + TN}{TP + FP + FN + TN}$.\n",
      "2. **Precision**: The proportion of true positive instances out of the total predicted positive instances. It is calculated as $\\frac{TP}{TP + FP}$.\n",
      "3. **Recall**: The proportion of true positive instances out of the total actual positive instances. It is also known as sensitivity or the true positive rate. It is calculated as $\\frac{TP}{TP + FN}$.\n",
      "4. **Specificity**: The proportion of true negative instances out of the total actual negative instances. It is also known as the true negative rate. It is calculated as $\\frac{TN}{TN + FP}$.\n",
      "5. **F1 Score**: The harmonic mean of precision and recall, providing a balanced measure of both metrics. It is calculated as $\\frac{2 * (Precision * Recall)}{Precision + Recall}$.\n",
      "\n",
      "## Interpretation\n",
      "\n",
      "A confusion matrix provides a detailed view of a classifier's performance, allowing for the identification of potential issues such as misclassification, overfitting, or class imbalance. By analyzing the confusion matrix, one can identify patterns in which the classifier is struggling and make adjustments accordingly. For example, if the classifier is misclassifying a specific class consistently, additional training data or feature engineering for that class might be necessary.\n",
      "\n",
      "It is important to note that a confusion matrix is not a standalone evaluation metric but rather a starting point for deriving other performance metrics. Depending on the specific problem and the goals of the classification task, different evaluation metrics might be more appropriate for measuring the performance of a classifier.\n",
      "DONE GENERATING: confusion_matrix\n",
      "NOW GENERATING: roc_curve\n",
      "FINISH_REASON: stop\n",
      "{\n",
      "    \"roc_curve\": {\n",
      "        \"title\": \"ROC Curve\",\n",
      "        \"prerequisites\": [\"binary_classification\", \"confusion_matrix\", \"true_positive_rate\", \"false_positive_rate\", \"probability_thresholds\", \"model_evaluation\"],\n",
      "        \"further_readings\": [\"precision_recall_curve\", \"area_under_the_curve\", \"receiver_operating_characteristic\", \"cost_sensitive_learning\", \"model_comparison\", \"cross_validation\", \"imbalanced_data\", \"classifier_calibration\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "# ROC Curve\n",
      "\n",
      "ROC curve, or Receiver Operating Characteristic curve, is a commonly used graphical technique to evaluate the performance of binary classification models by plotting the true positive rate (sensitivity/recall) against the false positive rate (1-specificity). The curve is created by varying the decision threshold of the classifier and measuring the corresponding true positive rate (TPR) and false positive rate (FPR) at each threshold.\n",
      "\n",
      "The main purpose of the ROC curve is to visualize and compare the trade-offs between the sensitivity and specificity of the classifier. It provides a single, easy-to-interpret graph that can help in selecting the optimal decision threshold based on the performance and cost requirements of a given application.\n",
      "\n",
      "## True Positive Rate and False Positive Rate\n",
      "\n",
      "Before diving into ROC curve, it's essential to understand the concepts of true positive rate (TPR) and false positive rate (FPR). Both TPR and FPR are derived from the confusion matrix, which is a tabular representation of the predicted and actual binary class labels.\n",
      "\n",
      "True Positive Rate (TPR) is the proportion of actual positive instances that are correctly predicted by the classifier:\n",
      "\n",
      "$$\n",
      "TPR = \\frac{TP}{TP + FN}\n",
      "$$\n",
      "\n",
      "where $TP$ is the number of true positives and $FN$ is the number of false negatives.\n",
      "\n",
      "False Positive Rate (FPR) is the proportion of actual negative instances that are incorrectly predicted as positive by the classifier:\n",
      "\n",
      "$$\n",
      "FPR = \\frac{FP}{FP + TN}\n",
      "$$\n",
      "\n",
      "where $FP$ is the number of false positives and $TN$ is the number of true negatives.\n",
      "\n",
      "## Construction of the ROC Curve\n",
      "\n",
      "To construct an ROC curve, the following steps are performed:\n",
      "\n",
      "1. Sort the predicted probabilities of the positive class in descending order.\n",
      "\n",
      "2. For each unique probability value, set it as the decision threshold.\n",
      "\n",
      "3. Calculate the TPR and FPR at the current threshold.\n",
      "\n",
      "4. Plot the TPR on the y-axis and FPR on the x-axis.\n",
      "\n",
      "5. Connect the points to form the ROC curve.\n",
      "\n",
      "The curve starts from the origin (0,0) and ends at (1,1). The diagonal line from the origin to (1,1) represents a random classifier, which has an equal chance of predicting the correct and incorrect class labels. An ideal classifier would have a curve that passes through the point (0,1), representing a perfect classification with no false positives or false negatives.\n",
      "\n",
      "## Area Under the ROC Curve (AUC-ROC)\n",
      "\n",
      "The area under the ROC curve (AUC-ROC) is a popular metric used to quantify the performance of a binary classifier. It measures the classifier's ability to discriminate between the positive and negative classes. A higher AUC-ROC value indicates better classification performance. The AUC-ROC value ranges from 0 to 1, with 0.5 representing a random classifier and 1 representing a perfect classifier.\n",
      "\n",
      "The AUC-ROC can be interpreted as the probability that a randomly chosen positive example will have a higher predicted probability than a randomly chosen negative example. This interpretation makes AUC-ROC a useful measure for comparing classifiers, especially when the class distribution is imbalanced.\n",
      "\n",
      "## Interpretation and Use Cases\n",
      "\n",
      "The ROC curve is widely used in various machine learning applications, including medical diagnosis, credit scoring, and spam detection. By analyzing the ROC curve, one can select an appropriate decision threshold that balances the trade-off between true positive rate and false positive rate based on the specific application.\n",
      "\n",
      "For example, in a medical diagnosis application, it might be more crucial to have a high true positive rate to ensure that patients with the disease are identified, even at the expense of a higher false positive rate. On the other hand, in a spam detection system, it might be more important to maintain a low false positive rate to prevent legitimate emails from being misclassified as spam, even if it means a lower true positive rate.\n",
      "\n",
      "In conclusion, the ROC curve is a powerful tool for evaluating and comparing the performance of binary classification models. It provides a graphical representation of the trade-offs between sensitivity and specificity, allowing for the selection of an optimal decision threshold based on the performance and cost requirements of a given application.\n",
      "DONE GENERATING: roc_curve\n",
      "NOW GENERATING: area_under_the_curve\n",
      "FINISH_REASON: stop\n",
      "{\n",
      "    \"area_under_the_curve\": {\n",
      "        \"title\": \"Area Under The Curve\",\n",
      "        \"prerequisites\": [\"probability_distributions\", \"random_variables\", \"cumulative_distribution_function\", \"probability_density_function\", \"integration\", \"roc_curve\"],\n",
      "        \"further_readings\": [\"precision_recall_curve\", \"receiver_operating_characteristic\", \"model_evaluation_metrics\", \"threshold_selection\", \"classification_models\", \"regression_models\", \"multiclass_classification\", \"performance_measure\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "# Area Under The Curve\n",
      "\n",
      "Area Under The Curve (AUC) is a popular metric used in machine learning, statistics, and data analysis to evaluate the performance of classification models. It is often used in conjunction with the Receiver Operating Characteristic (ROC) curve, which is a graphical representation of the true positive rate (sensitivity) against the false positive rate (1-specificity) at various threshold settings. The AUC represents the probability that a randomly chosen positive instance will be ranked higher than a randomly chosen negative instance by the classifier.\n",
      "\n",
      "## Background\n",
      "\n",
      "In order to understand the AUC, it is important to understand the concepts of true positive rate (TPR) and false positive rate (FPR). The TPR is the proportion of positive instances that are correctly identified by the classifier, while the FPR is the proportion of negative instances that are incorrectly identified as positive. The ROC curve plots the TPR against the FPR at various threshold settings, providing a visual representation of the trade-off between sensitivity and specificity.\n",
      "\n",
      "The AUC is calculated by finding the area under the ROC curve. A perfect classifier would have an AUC of 1, indicating that it can perfectly separate positive and negative instances at all threshold settings. An AUC of 0.5 corresponds to a classifier that performs no better than random chance, while an AUC less than 0.5 indicates a classifier that performs worse than random chance. In general, a higher AUC value indicates better classifier performance.\n",
      "\n",
      "## Calculation\n",
      "\n",
      "The AUC can be calculated using different methods, including numerical integration and the trapezoidal rule. The numerical integration method involves dividing the area under the curve into small rectangles and summing the areas of these rectangles. The trapezoidal rule is a more accurate method that approximates the area by dividing the curve into trapezoids and summing their areas. The trapezoidal rule can be expressed mathematically as:\n",
      "\n",
      "$$\n",
      "AUC = \\sum_{i=1}^{n-1} \\frac{(FPR_{i+1} - FPR_i) \\times (TPR_{i+1} + TPR_i)}{2}\n",
      "$$\n",
      "\n",
      "where $n$ is the number of threshold settings, and $FPR_i$ and $TPR_i$ are the false positive rate and true positive rate, respectively, at the $i$-th threshold setting.\n",
      "\n",
      "## Interpretation\n",
      "\n",
      "The AUC is an important metric for several reasons:\n",
      "\n",
      "1. It provides a single value that summarizes the performance of a classifier across all threshold settings, making it easier to compare different classifiers.\n",
      "2. It is insensitive to class imbalance, which is a common problem in machine learning. Class imbalance occurs when the number of instances in one class is much larger than the other class, leading to biased estimates of classifier performance.\n",
      "3. It can be used to evaluate the performance of binary classifiers as well as multi-class classifiers by considering the AUC for each class separately and averaging the values.\n",
      "\n",
      "However, it is important to note that the AUC does not provide information on the optimal threshold setting for a classifier. To determine the best threshold, other metrics such as precision, recall, and F1-score should be considered in conjunction with the AUC.\n",
      "\n",
      "## Limitations\n",
      "\n",
      "While the AUC is a widely used metric, it has its limitations:\n",
      "\n",
      "1. It does not provide information on the classifier's performance at a specific threshold setting. For some applications, the optimal threshold may be more important than the overall performance across all thresholds.\n",
      "2. The AUC can be sensitive to small changes in the ROC curve, leading to large changes in the AUC value. This can make it difficult to compare the performance of different classifiers when the differences in their ROC curves are small.\n",
      "3. In some cases, a high AUC value may not necessarily correspond to a good classifier. For example, a classifier with a high AUC value may have poor performance in the region of interest, such as the high-sensitivity or high-specificity region of the ROC curve.\n",
      "\n",
      "Despite these limitations, the AUC remains a valuable metric for evaluating classifier performance and is widely used in machine learning and data analysis.\n",
      "DONE GENERATING: area_under_the_curve\n",
      "NOW GENERATING: accuracy_score\n",
      "FINISH_REASON: stop\n",
      "```json\n",
      "{\n",
      "    \"accuracy_score\": {\n",
      "        \"title\": \"Accuracy Score\",\n",
      "        \"prerequisites\": [\"classification_algorithms\", \"confusion_matrix\", \"precision_recall_f1_score\", \"evaluation_metrics_ml\"],\n",
      "        \"further_readings\": [\"cross_validation\", \"roc_curve\", \"area_under_curve\", \"multiclass_classification_metrics\", \"imbalanced_data\", \"micro_macro_averaging\", \"k_fold_cross_validation\", \"stratified_k_fold_cross_validation\"]\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "# Accuracy Score\n",
      "\n",
      "The accuracy score is a widely used evaluation metric for classification problems in Machine Learning (ML). It is the ratio of the number of correct predictions to the total number of predictions. The accuracy score gives an overall idea of how well the classification algorithm is performing, but it may not be the best choice for evaluating the performance of a classifier in certain situations, especially when dealing with imbalanced datasets.\n",
      "\n",
      "## Definition\n",
      "\n",
      "The accuracy score is defined as the number of correct predictions (both true positives and true negatives) divided by the total number of predictions. Mathematically, the accuracy score can be represented as:\n",
      "\n",
      "$$\n",
      "\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
      "$$\n",
      "\n",
      "Where:\n",
      "- $TP$ - True Positives\n",
      "- $TN$ - True Negatives\n",
      "- $FP$ - False Positives\n",
      "- $FN$ - False Negatives\n",
      "\n",
      "## Interpretation\n",
      "\n",
      "The accuracy score can be interpreted as the probability that a classifier will correctly classify a given instance. A higher accuracy score indicates better performance of the classifier. However, there are some limitations to using the accuracy score as the sole evaluation metric for classification problems.\n",
      "\n",
      "## Limitations of Accuracy Score\n",
      "\n",
      "1. **Imbalanced dataset**: The accuracy score may be misleading when dealing with imbalanced datasets. In such cases, a classifier may achieve high accuracy by simply predicting the majority class for all instances, which may not be useful in practice.\n",
      "\n",
      "2. **Misclassification costs**: The accuracy score does not take into account the different costs of misclassifying instances. For example, in a medical diagnosis problem, misclassifying a patient with a severe illness as healthy might have far more severe consequences than misclassifying a healthy patient as ill.\n",
      "\n",
      "3. **Class-wise performance**: The accuracy score does not provide information on the performance of the classifier for each class. In some situations, it may be more important to focus on a specific class, such as the minority class in an imbalanced dataset.\n",
      "\n",
      "## Alternative Evaluation Metrics\n",
      "\n",
      "Due to the limitations of the accuracy score, other evaluation metrics are often used in combination with or instead of accuracy to measure the performance of classification algorithms. Some of these metrics include:\n",
      "\n",
      "1. **Precision**: The ratio of true positives to the sum of true positives and false positives. It measures the ability of a classifier to correctly identify positive instances.\n",
      "2. **Recall**: The ratio of true positives to the sum of true positives and false negatives. It measures the ability of a classifier to identify all positive instances.\n",
      "3. **F1-score**: The harmonic mean of precision and recall, providing a balance between the two metrics.\n",
      "4. **ROC Curve**: A plot of the true positive rate (sensitivity) versus the false positive rate (1-specificity) for different classification thresholds.\n",
      "5. **Area Under the ROC Curve (AUC-ROC)**: A scalar value representing the performance of a classifier based on the area under the ROC curve.\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "The accuracy score is a simple and widely used evaluation metric for classification problems in machine learning. However, it has limitations in certain situations, especially when dealing with imbalanced datasets or when different misclassification costs need to be considered. In such cases, alternative evaluation metrics such as precision, recall, F1-score, and AUC-ROC can provide a more comprehensive assessment of classifier performance.\n",
      "DONE GENERATING: accuracy_score\n",
      "NOW GENERATING: macro_f1_score\n",
      "FINISH_REASON: stop\n",
      "{\n",
      "    \"macro_f1_score\": {\n",
      "        \"title\": \"Macro F1 Score\",\n",
      "        \"prerequisites\": [\"precision_and_recall\", \"confusion_matrix\", \"classification_metrics\", \"multiclass_classification\"],\n",
      "        \"further_readings\": [\"micro_f1_score\", \"weighted_f1_score\", \"area_under_roc_curve\", \"hamming_loss\", \"jaccard_similarity_score\", \"classification_report\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "# Macro F1 Score\n",
      "\n",
      "Macro F1 Score is a performance metric used to evaluate the effectiveness of classification algorithms, particularly in multiclass classification problems. It is the harmonic mean of precision and recall, calculated separately for each class, and then averaged across all classes.\n",
      "\n",
      "## Overview\n",
      "\n",
      "In multiclass classification problems, where there are multiple classes to predict, it is essential to evaluate the performance of a classifier not only based on its overall accuracy but also on its ability to predict each class correctly. Macro F1 Score is one such metric that helps to assess the performance of a classifier in handling each class with equal importance.\n",
      "\n",
      "Macro F1 Score is calculated as follows:\n",
      "\n",
      "1. Compute the precision and recall for each class separately.\n",
      "2. Calculate the F1 Score for each class using the formula:\n",
      "\n",
      "   $$\n",
      "   F1_i = \\frac{2 \\times Precision_i \\times Recall_i}{Precision_i + Recall_i}\n",
      "   $$\n",
      "\n",
      "3. Calculate the Macro F1 Score by averaging the F1 Scores of all classes:\n",
      "\n",
      "   $$\n",
      "   Macro\\ F1\\ Score = \\frac{1}{n}\\sum_{i=1}^{n} F1_i\n",
      "   $$\n",
      "\n",
      "   where $n$ is the number of classes.\n",
      "\n",
      "Macro F1 Score is particularly useful when dealing with imbalanced datasets, where some classes have significantly fewer samples than others. By treating each class with equal importance, the Macro F1 Score provides a better understanding of the classifier's performance on minority classes.\n",
      "\n",
      "## Example\n",
      "\n",
      "Let's consider a multiclass classification problem with three classes, A, B, and C. The classifier's predictions are compared with the true labels, and the following confusion matrix is obtained:\n",
      "\n",
      "|      | Predicted A | Predicted B | Predicted C |\n",
      "|------|-------------|-------------|-------------|\n",
      "| True A | 40          | 10          | 5           |\n",
      "| True B | 5           | 35          | 15          |\n",
      "| True C | 10          | 15          | 60          |\n",
      "\n",
      "To calculate the Macro F1 Score, first, we need to compute the precision and recall for each class:\n",
      "\n",
      "- Class A\n",
      "  - Precision A = $\\frac{True\\ Positives\\ A}{True\\ Positives\\ A + False\\ Positives\\ A} = \\frac{40}{40+5+10} = \\frac{40}{55} = 0.727$\n",
      "  - Recall A = $\\frac{True\\ Positives\\ A}{True\\ Positives\\ A + False\\ Negatives\\ A} = \\frac{40}{40+10+5} = \\frac{40}{55} = 0.727$\n",
      "\n",
      "- Class B\n",
      "  - Precision B = $\\frac{True\\ Positives\\ B}{True\\ Positives\\ B + False\\ Positives\\ B} = \\frac{35}{10+35+15} = \\frac{35}{60} = 0.583$\n",
      "  - Recall B = $\\frac{True\\ Positives\\ B}{True\\ Positives\\ B + False\\ Negatives\\ B} = \\frac{35}{5+35+15} = \\frac{35}{55} = 0.636$\n",
      "\n",
      "- Class C\n",
      "  - Precision C = $\\frac{True\\ Positives\\ C}{True\\ Positives\\ C + False\\ Positives\\ C} = \\frac{60}{5+15+60} = \\frac{60}{80} = 0.75$\n",
      "  - Recall C = $\\frac{True\\ Positives\\ C}{True\\ Positives\\ C + False\\ Negatives\\ C} = \\frac{60}{10+15+60} = \\frac{60}{85} = 0.706$\n",
      "\n",
      "Now, we can calculate the F1 Score for each class:\n",
      "\n",
      "- Class A: $F1_A = \\frac{2 \\times Precision_A \\times Recall_A}{Precision_A + Recall_A} = \\frac{2 \\times 0.727 \\times 0.727}{0.727 + 0.727} = 0.727$\n",
      "- Class B: $F1_B = \\frac{2 \\times Precision_B \\times Recall_B}{Precision_B + Recall_B} = \\frac{2 \\times 0.583 \\times 0.636}{0.583 + 0.636} = 0.608$\n",
      "- Class C: $F1_C = \\frac{2 \\times Precision_C \\times Recall_C}{Precision_C + Recall_C} = \\frac{2 \\times 0.75 \\times 0.706}{0.75 + 0.706} = 0.727$\n",
      "\n",
      "Finally, the Macro F1 Score is the average of the F1 Scores of all classes:\n",
      "\n",
      "$$\n",
      "Macro\\ F1\\ Score = \\frac{1}{3}(F1_A + F1_B + F1_C) = \\frac{1}{3}(0.727 + 0.608 + 0.727) \\approx 0.687\n",
      "$$\n",
      "\n",
      "Thus, the Macro F1 Score for this classifier is approximately 0.687.\n",
      "\n",
      "## Interpretation\n",
      "\n",
      "A higher Macro F1 Score indicates better classification performance, particularly when dealing with imbalanced datasets. A Macro F1 Score of 1 indicates perfect classification, while a score of 0 signifies the worst possible performance.\n",
      "\n",
      "It is essential to note that the Macro F1 Score does not take the class distribution into account. This means that if a classifier performs poorly on a minority class, it will still have a significant impact on the overall Macro F1 Score, making it a suitable metric for evaluating classifiers in imbalanced datasets.\n",
      "\n",
      "However, when dealing with balanced datasets or when the class distribution is essential, other metrics like Micro F1 Score or Weighted F1 Score may be more appropriate.\n",
      "\n",
      "## Limitations\n",
      "\n",
      "One limitation of the Macro F1 Score is that it treats each class equally, regardless of the class distribution. This can lead to misleading results when dealing with imbalanced datasets where the class distribution is essential. In such cases, it is recommended to use other metrics like Micro F1 Score or Weighted F1 Score that consider the class distribution.\n",
      "\n",
      "Another limitation is that the Macro F1 Score is sensitive to the number of classes, making it challenging to compare classifiers with a different number of classes. To overcome this issue, one can use other metrics like the Jaccard similarity score or the Hamming loss that are not affected by the number of classes.\n",
      "DONE GENERATING: macro_f1_score\n",
      "NOW GENERATING: micro_f1_score\n",
      "FINISH_REASON: stop\n",
      "```\n",
      "{\n",
      "    \"micro_f1_score\": {\n",
      "        \"title\": \"Micro F1 Score\",\n",
      "        \"prerequisites\": [\"accuracy\", \"precision\", \"recall\", \"f1_score\", \"confusion_matrix\", \"classification_metrics\", \"multi_class_classification\", \"multi_label_classification\"],\n",
      "        \"further_readings\": [\"weighted_f1_score\", \"macro_f1_score\", \"average_precision_score\", \"roc_auc_score\", \"hamming_loss\", \"jaccard_similarity_score\", \"log_loss\", \"zero_one_loss\"]\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "# Micro F1 Score\n",
      "\n",
      "The Micro F1 Score is a performance metric used in classification problems, particularly in multi-class and multi-label classification tasks. It is a variant of the F1 score, which is a harmonic mean of precision and recall. The Micro F1 Score is calculated by aggregating the contributions of all classes or labels to compute the average precision, recall, and F1 score across all classes.\n",
      "\n",
      "## Background\n",
      "\n",
      "In classification problems, the goal is to assign an instance to one or more classes or labels based on its features. A common way to evaluate the performance of a classification model is to compute various metrics, such as accuracy, precision, recall, and F1 score. These metrics help to understand the trade-offs between true positive, false positive, true negative, and false negative predictions.\n",
      "\n",
      "In multi-class and multi-label classification problems, the number of classes or labels is more than two, and the classification performance must be evaluated across all these classes. Micro F1 Score is one such metric that provides a single measure of the model's performance across all classes.\n",
      "\n",
      "## Calculation\n",
      "\n",
      "The Micro F1 Score is calculated using the following steps:\n",
      "\n",
      "1. Compute the confusion matrix for each class or label. The confusion matrix is a table that shows the number of true positive (TP), false positive (FP), true negative (TN), and false negative (FN) predictions for each class or label.\n",
      "\n",
      "2. Calculate the precision and recall for each class or label using the TP, FP, TN, and FN values from the confusion matrix. The precision is the ratio of true positive predictions to the total number of positive predictions (TP + FP), while the recall is the ratio of true positive predictions to the total number of actual positive instances (TP + FN).\n",
      "\n",
      "3. Calculate the F1 score for each class or label using the precision and recall values. The F1 score is the harmonic mean of precision and recall and is given by the formula:\n",
      "\n",
      "$$\n",
      "F1 = 2 \\times \\frac{precision \\times recall}{precision + recall}\n",
      "$$\n",
      "\n",
      "4. Compute the Micro F1 Score by aggregating the TP, FP, and FN values across all classes or labels and then calculating the overall precision, recall, and F1 score. The Micro F1 Score is calculated as follows:\n",
      "\n",
      "$$\n",
      "Micro\\,F1 = 2 \\times \\frac{\\sum_{i=1}^{n} TP_i}{\\sum_{i=1}^{n} (TP_i + FP_i) + \\sum_{i=1}^{n} (TP_i + FN_i)}\n",
      "$$\n",
      "\n",
      "where $n$ is the number of classes or labels, and $TP_i$, $FP_i$, and $FN_i$ are the true positive, false positive, and false negative values for the $i$-th class or label, respectively.\n",
      "\n",
      "## Interpretation\n",
      "\n",
      "The Micro F1 Score ranges from 0 to 1, with higher values indicating better classification performance. A Micro F1 Score of 1 represents perfect classification, while a score of 0 indicates that the model fails to correctly classify any instance.\n",
      "\n",
      "The Micro F1 Score is particularly useful in cases where the class distribution is imbalanced, as it gives equal weight to the performance of each class or label. This can help to identify models that perform well on the majority class but poorly on minority classes.\n",
      "\n",
      "## Comparison with Other Metrics\n",
      "\n",
      "The Micro F1 Score is related to other classification performance metrics, such as the Macro F1 Score and the Weighted F1 Score. The Macro F1 Score computes the average F1 score across all classes, while the Weighted F1 Score computes the average F1 score weighted by the number of instances in each class. These metrics provide different perspectives on the overall performance of a classification model and can be used in conjunction with the Micro F1 Score to evaluate multi-class and multi-label classification tasks.\n",
      "DONE GENERATING: micro_f1_score\n",
      "NOW GENERATING: classification_algorithms\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mNOW GENERATING:\u001b[39m\u001b[39m'\u001b[39m, topic)\n\u001b[1;32m     42\u001b[0m prompt \u001b[39m=\u001b[39m generate_prompt(topic)\n\u001b[0;32m---> 43\u001b[0m finish_reason, message, completion \u001b[39m=\u001b[39m generate_completion(prompt)\n\u001b[1;32m     44\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFINISH_REASON:\u001b[39m\u001b[39m\"\u001b[39m, finish_reason)\n\u001b[1;32m     45\u001b[0m \u001b[39mprint\u001b[39m(message)\n",
      "Cell \u001b[0;32mIn[2], line 37\u001b[0m, in \u001b[0;36mgenerate_completion\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_completion\u001b[39m(prompt):\n\u001b[0;32m---> 37\u001b[0m     completion \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m     38\u001b[0m         \u001b[39m# model=\"gpt-3.5-turbo\",\u001b[39;49;00m\n\u001b[1;32m     39\u001b[0m         model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgpt-4\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     40\u001b[0m         messages\u001b[39m=\u001b[39;49m[\n\u001b[1;32m     41\u001b[0m             {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: prompt}\n\u001b[1;32m     42\u001b[0m         ],\n\u001b[1;32m     43\u001b[0m         temperature\u001b[39m=\u001b[39;49m\u001b[39m0.7\u001b[39;49m,\n\u001b[1;32m     44\u001b[0m     )\n\u001b[1;32m     45\u001b[0m     finish_reason \u001b[39m=\u001b[39m completion\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mfinish_reason\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     46\u001b[0m     message \u001b[39m=\u001b[39m completion\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mmessage\u001b[39m.\u001b[39mcontent\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/openai/api_requestor.py:216\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    206\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    207\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    214\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    215\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m--> 216\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest_raw(\n\u001b[1;32m    217\u001b[0m         method\u001b[39m.\u001b[39;49mlower(),\n\u001b[1;32m    218\u001b[0m         url,\n\u001b[1;32m    219\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    220\u001b[0m         supplied_headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    221\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[1;32m    222\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    223\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    224\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    225\u001b[0m     )\n\u001b[1;32m    226\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response(result, stream)\n\u001b[1;32m    227\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/openai/api_requestor.py:516\u001b[0m, in \u001b[0;36mAPIRequestor.request_raw\u001b[0;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    514\u001b[0m     _thread_context\u001b[39m.\u001b[39msession \u001b[39m=\u001b[39m _make_session()\n\u001b[1;32m    515\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 516\u001b[0m     result \u001b[39m=\u001b[39m _thread_context\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    517\u001b[0m         method,\n\u001b[1;32m    518\u001b[0m         abs_url,\n\u001b[1;32m    519\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    520\u001b[0m         data\u001b[39m=\u001b[39;49mdata,\n\u001b[1;32m    521\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[1;32m    522\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    523\u001b[0m         timeout\u001b[39m=\u001b[39;49mrequest_timeout \u001b[39mif\u001b[39;49;00m request_timeout \u001b[39melse\u001b[39;49;00m TIMEOUT_SECS,\n\u001b[1;32m    524\u001b[0m         proxies\u001b[39m=\u001b[39;49m_thread_context\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mproxies,\n\u001b[1;32m    525\u001b[0m     )\n\u001b[1;32m    526\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mTimeout \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    527\u001b[0m     \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mTimeout(\u001b[39m\"\u001b[39m\u001b[39mRequest timed out: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(e)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    704\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/requests/adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    488\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 489\u001b[0m         resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    490\u001b[0m             method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    491\u001b[0m             url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    492\u001b[0m             body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    493\u001b[0m             headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    494\u001b[0m             redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m             assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    496\u001b[0m             preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    497\u001b[0m             decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    498\u001b[0m             retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    499\u001b[0m             timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    500\u001b[0m         )\n\u001b[1;32m    502\u001b[0m     \u001b[39m# Send the request.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39mproxy_pool\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    704\u001b[0m     conn,\n\u001b[1;32m    705\u001b[0m     method,\n\u001b[1;32m    706\u001b[0m     url,\n\u001b[1;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    711\u001b[0m )\n\u001b[1;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/urllib3/connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    444\u001b[0m             httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[1;32m    445\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    450\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    451\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/urllib3/connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n\u001b[1;32m    443\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m         httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    445\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m         \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m         \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m         \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    449\u001b[0m         six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/http/client.py:1374\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1372\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1373\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1374\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1375\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[1;32m   1376\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1271\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1272\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1273\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1274\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1275\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1276\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1128\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1129\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1130\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1131\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1132\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "visited_pages = load_visited_pages()\n",
    "queue = []\n",
    "# visited_pages.remove('recall')\n",
    "\n",
    "if not queue:\n",
    "    with open('wiki-connections.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "        # for key in data:\n",
    "        key = 'f1_score'\n",
    "        for new_topic in data[key]['prerequisites']:\n",
    "            if new_topic not in visited_pages and new_topic not in queue and new_topic not in data:\n",
    "                queue.append(new_topic)\n",
    "        for new_topic in data[key]['further_readings']:\n",
    "            if new_topic not in visited_pages and new_topic not in queue and new_topic not in data:\n",
    "                queue.append(new_topic)\n",
    "        # if len(queue) > 0:\n",
    "        #     break\n",
    "\n",
    "\n",
    "while queue:\n",
    "    topic = queue.pop(0)\n",
    "    topic = topic.lower()\n",
    "    topic = topic.replace(\"'\", \"\")\n",
    "\n",
    "    if topic in visited_pages:\n",
    "        continue\n",
    "\n",
    "    with open('wiki-connections.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "        if topic in data:\n",
    "            continue\n",
    "        while len(queue) == 0:\n",
    "            for key in data:\n",
    "                for new_topic in data[key]['prerequisites']:\n",
    "                    if new_topic not in visited_pages and new_topic not in queue and new_topic != topic and new_topic not in data:\n",
    "                        queue.append(new_topic)\n",
    "                for new_topic in data[key]['further_readings']:\n",
    "                    if new_topic not in visited_pages and new_topic not in queue and new_topic != topic and new_topic not in data:\n",
    "                        queue.append(new_topic)\n",
    "\n",
    "    print('NOW GENERATING:', topic)\n",
    "    prompt = generate_prompt(topic)\n",
    "    finish_reason, message, completion = generate_completion(prompt)\n",
    "    print(\"FINISH_REASON:\", finish_reason)\n",
    "    print(message)\n",
    "\n",
    "    if finish_reason != 'stop':\n",
    "        print(\"Error: Did not finish generating.\")\n",
    "        exit(1)\n",
    "    \n",
    "    generate_json(message, topic)\n",
    "    generate_markdown(message, topic)\n",
    "    generate_js(topic)\n",
    "\n",
    "    visited_pages.add(topic)\n",
    "    save_visited_pages(visited_pages)\n",
    "\n",
    "    with open('wiki-connections.json', 'r') as file:\n",
    "        wiki_connections = json.load(file)\n",
    "        queue += wiki_connections[topic]['prerequisites']\n",
    "        queue += wiki_connections[topic]['further_readings']\n",
    "\n",
    "    print('DONE GENERATING:', topic)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
