{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/alaney2/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import shutil\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import nltk\n",
    "from dotenv import load_dotenv\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "load_dotenv('.env.local')\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(topic):\n",
    "    parts = topic.split('_')\n",
    "    parts = [part.capitalize() for part in parts]\n",
    "    topic = ' '.join(parts)\n",
    "    prompt = f'Topic: {topic}\\n' + '''\n",
    "    You are a world-renowned AI and ML expert.\n",
    "    Provide a JSON object containing the topic, a list of 0-8 prerequisite topics, and a list of 0-8 further readings related to AI, ML, and DL.\n",
    "    The name of the JSON object must match exactly with the given topic.\n",
    "    Ensure that the prerequisites and further readings are specifically relevant to the given, rather than broad topics like calculus or statistics.\n",
    "    When generating topics, prefer the singular form of the topic, such as \"convolutional_neural_network\" instead of \"convolutional_neural_networks\" but use the plural form when it makes more sense to (\"policy_gradient_methods\").\n",
    "    Ensure that the title field is properly capitalized and spaced and has the right punctuation (such as Q-Learning).\n",
    "    Also ensure that the topic, prerequisites, and further readings are in snake_case. Do not put single quotes anywhere in the JSON object.\n",
    "    Use a similar format to the example provided below and ensure that the JSON object is valid.\n",
    "\n",
    "    Example:\n",
    "    {\n",
    "        \"generative_adversarial_network\": {\n",
    "            \"title\": \"Generative Adversarial Network\",\n",
    "            \"prerequisites\": [\"expectation_maximization_algorithm\", \"probability_distributions\", \"convolutional_neural_networks\", \"backpropagation\", \"stochastic_gradient_descent\", \"loss_functions\", \"optimization_algorithms\", \"deep_learning_frameworks\", \"regularization_techniques\", \"unsupervised_learning\"],\n",
    "            \"further_readings\": [\"conditional_gans\", \"cycle_gans\", \"stylegan_and_stylegan2\", \"wasserstein_gans\", \"domain_adaptation\", \"image_to_image_translation\", \"semi_supervised_learning\", \"adversarial_training\", \"adversarial_attacks_and_defenses\", \"transfer_learning\"]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    Next, write a detailed wiki page about the given topic in Markdown format. Always write from a third-person perspective and remain unopinionated.\n",
    "    Ensure that this wiki page is explicitly in code format. \n",
    "    Do not include a \"Contents\" section. \n",
    "    Do not include a \"Further Readings\" nor a \"Prerequisites\" section if they just include related topics.\n",
    "    Use a neutral, unbiased tone without exclamation marks. \n",
    "    Ensure that the heading is the same as the title in the JSON object.\n",
    "    Follow Markdown syntax for headings and formatting, and use LaTeX for equations, with inline equations in pairs of $ and multiline equations in $$.\n",
    "    Ensure the entire output is less than 3600 tokens long and does not include an extra line at the end of the Markdown.\n",
    "    '''\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def generate_completion(prompt):\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        # model=\"gpt-3.5-turbo\",\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    finish_reason = completion.choices[0]['finish_reason']\n",
    "    message = completion.choices[0].message.content\n",
    "    return finish_reason, message, completion\n",
    "\n",
    "\n",
    "def generate_json(message, topic):\n",
    "    message = message.strip()\n",
    "    json_string = re.search(r'(?s){\\s*\\\"[^\"]+\\\":\\s*{.*?}\\s*}', message, re.DOTALL)\n",
    "    \n",
    "    if json_string:\n",
    "        json_string = json_string.group()\n",
    "        # json_string = json_string.lower()\n",
    "        json_string = re.sub(r',\\s*([\\]}])', r'\\1', json_string)\n",
    "        json_object = json.loads(json_string)\n",
    "        # title_words = json_object[topic]['title'].split()\n",
    "        # result_title = [word.capitalize() if word.lower() not in stop_words else word for word in title_words]\n",
    "        # result_title = \" \".join(result_title)\n",
    "        # json_object[topic]['title'] = result_title\n",
    "        # print(json_object[topic]['title'])\n",
    "\n",
    "        # if topic not in json_string:\n",
    "        #     print(\"Error: Could not find topic in JSON.\")\n",
    "        #     exit(1)\n",
    "        with open('wiki-connections.json', 'r') as file:\n",
    "            existing_data = json.load(file)\n",
    "        existing_data.update(json_object)\n",
    "        with open('wiki-connections.json', 'w') as file:\n",
    "            json.dump(existing_data, file, indent=4)\n",
    "    else:\n",
    "        print(\"Error: Could not extract JSON from message.\")\n",
    "        exit(1)\n",
    "\n",
    "\n",
    "def generate_markdown(message, topic):\n",
    "    message = message.strip()\n",
    "    markdown_start_pos = message.find('#')\n",
    "    markdown_content = message[markdown_start_pos:].strip() + '\\n'\n",
    "\n",
    "    md_filename = topic + '.md'\n",
    "    with open(md_filename, 'w') as file:\n",
    "        file.write(markdown_content)\n",
    "\n",
    "    destination_folder = 'data'\n",
    "    shutil.move(md_filename, destination_folder)\n",
    "\n",
    "\n",
    "def generate_js(topic):\n",
    "    md_filename = topic + '.md'\n",
    "    js_string = f'''\n",
    "    import React from 'react';\n",
    "    import path from 'path';\n",
    "    import fs from 'fs';\n",
    "    import PageContent from '@/components/PageContent/PageContent';\n",
    "\n",
    "    const filename = '{md_filename}';\n",
    "\n",
    "    export default function MarkdownPage({{ markdownContent }}) {{\n",
    "    return <PageContent content={{markdownContent}} filename={{filename}} />;\n",
    "    }}\n",
    "\n",
    "    export async function getStaticProps() {{\n",
    "    const filePath = path.join(process.cwd(), 'data', filename);\n",
    "    const markdownContent = fs.readFileSync(filePath, 'utf8');\n",
    "    return {{\n",
    "        props: {{\n",
    "        markdownContent,\n",
    "        }},\n",
    "    }};\n",
    "    }}\n",
    "    '''\n",
    "    js_filename = topic + '.js'\n",
    "    with open(js_filename, 'w') as file:\n",
    "        file.write(js_string)\n",
    "\n",
    "    destination_folder = 'pages'\n",
    "    shutil.move(js_filename, destination_folder)\n",
    "\n",
    "\n",
    "def extract_markdown(message):\n",
    "    markdown_start = message.find('```')\n",
    "    markdown_end = message.rfind('```')\n",
    "    markdown_string = message[markdown_start:markdown_end+3]\n",
    "    return markdown_string\n",
    "\n",
    "\n",
    "def save_visited_pages(visited_pages, file_name='visited_pages.pickle'):\n",
    "    with open(file_name, 'wb') as handle:\n",
    "        pickle.dump(visited_pages, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def load_visited_pages(file_name='visited_pages.pickle'):\n",
    "    try:\n",
    "        with open(file_name, 'rb') as handle:\n",
    "            visited_pages = pickle.load(handle)\n",
    "        return visited_pages\n",
    "    except FileNotFoundError:\n",
    "        return set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "visited_pages = load_visited_pages()\n",
    "visited_pages.update([''])\n",
    "save_visited_pages(visited_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW GENERATING: recall\n",
      "FINISH_REASON: stop\n",
      "{\n",
      "    \"recall\": {\n",
      "        \"title\": \"Recall\",\n",
      "        \"prerequisites\": [\"classification_metrics\", \"confusion_matrix\", \"precision\", \"true_positive_rate\", \"false_negative_rate\", \"binary_classification\"],\n",
      "        \"further_readings\": [\"f1_score\", \"precision_recall_curve\", \"roc_curve\", \"area_under_the_curve\", \"average_precision\", \"balanced_accuracy\", \"multiclass_classification_metrics\", \"weighted_metrics\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "# Recall\n",
      "\n",
      "Recall, also known as sensitivity, hit rate, or true positive rate (TPR), is a measure used in binary classification tasks to evaluate the performance of a classifier. It is the proportion of true positive instances among all the instances that were actually positive. In other words, recall measures the ability of a classifier to correctly identify positive examples as positive.\n",
      "\n",
      "## Definition\n",
      "\n",
      "Given a confusion matrix, where $tp$ (true positives) represents the number of positive instances correctly classified, and $fn$ (false negatives) represents the number of positive instances misclassified as negative, recall can be calculated as follows:\n",
      "\n",
      "$$\n",
      "Recall = \\frac{tp}{tp + fn}\n",
      "$$\n",
      "\n",
      "A recall of 1 indicates that all positive instances were correctly identified, while a recall of 0 means that none of the positive instances were identified correctly. In general, a higher recall indicates better performance of the classifier in identifying positive instances.\n",
      "\n",
      "## Interpretation\n",
      "\n",
      "Recall is particularly useful when the cost of false negatives is high, such as in medical diagnosis, where failing to identify a disease could potentially have severe consequences. In such cases, a high recall classifier would ensure that most of the actual positive cases are identified, even if it may lead to some false positives.\n",
      "\n",
      "It is important to note that recall alone may not provide a complete picture of the classifier's performance, as it does not take into account false positives (instances incorrectly classified as positive). This is why other measures, such as precision and F1-score, are also used to assess the performance of binary classifiers.\n",
      "\n",
      "## Relation to Precision\n",
      "\n",
      "Precision is another performance measure for binary classification tasks, which measures the proportion of true positive instances among all the instances predicted as positive. It can be calculated using the confusion matrix, where $fp$ (false positives) represents the number of negative instances misclassified as positive:\n",
      "\n",
      "$$\n",
      "Precision = \\frac{tp}{tp + fp}\n",
      "$$\n",
      "\n",
      "While recall focuses on the classifier's ability to correctly identify positive instances, precision focuses on the classifier's ability to avoid false positives. In some cases, a trade-off between recall and precision may be necessary, as increasing one may lead to a decrease in the other. This trade-off can be visualized using a precision-recall curve, which plots the values of precision and recall for different decision thresholds.\n",
      "\n",
      "## F1-Score\n",
      "\n",
      "The F1-score is a harmonic mean of precision and recall and can be used as a single metric to evaluate the performance of a binary classifier, especially when dealing with imbalanced datasets. It is defined as follows:\n",
      "\n",
      "$$\n",
      "F1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}\n",
      "$$\n",
      "\n",
      "The F1-score ranges from 0 to 1, with 1 indicating perfect classification and 0 indicating the worst possible performance. A higher F1-score represents a better balance between precision and recall.\n",
      "\n",
      "## Extension to Multi-class Classification\n",
      "\n",
      "Recall can be extended to multi-class classification tasks by calculating the recall for each class individually and then combining the results. There are several ways to do this, such as micro-averaging, macro-averaging, and weighted averaging. Micro-averaging calculates the overall recall by summing up the true positives and false negatives across all classes, while macro-averaging calculates the recall for each class separately and then takes the average. Weighted averaging also calculates the recall for each class separately, but then takes a weighted average based on the number of instances in each class.\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "Recall is an important performance measure for binary classification tasks, as it evaluates the classifier's ability to correctly identify positive instances. It is particularly useful in situations where the cost of false negatives is high. However, recall should be used in conjunction with other measures, such as precision and F1-score, to provide a more comprehensive evaluation of the classifier's performance.\n",
      "DONE GENERATING: recall\n",
      "NOW GENERATING: activation_functions\n",
      "FINISH_REASON: stop\n",
      "{\n",
      "    \"activation_functions\": {\n",
      "        \"title\": \"Activation Functions\",\n",
      "        \"prerequisites\": [\"artificial_neural_networks\", \"deep_learning\", \"gradient_descent\", \"backpropagation\"],\n",
      "        \"further_readings\": [\"vanishing_and_exploding_gradients\", \"rectified_linear_units\", \"softmax_function\", \"sigmoid_function\", \"tanh_function\", \"swish_function\", \"leaky_relu\", \"maxout_function\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "# Activation Functions\n",
      "\n",
      "An activation function, also known as a transfer function or non-linearity, is a fundamental component of artificial neural networks (ANNs) and deep learning (DL). It is a mathematical function that introduces non-linearity into the network, enabling it to learn complex, non-linear patterns in the data. Activation functions are applied to the weighted sum of inputs at each neuron in a network, transforming the input signal into an output signal that is passed to the next layer of neurons.\n",
      "\n",
      "## Purpose of Activation Functions\n",
      "\n",
      "The primary purpose of activation functions is to introduce non-linearity into the network. Without non-linearity, a neural network would be equivalent to a linear regression model, incapable of learning complex patterns in the data. By applying non-linear activation functions at each neuron, the network can learn to represent a wide range of non-linear functions and solve complex tasks, such as image recognition, natural language processing, and reinforcement learning.\n",
      "\n",
      "In addition to introducing non-linearity, activation functions also help to control the output range of neurons, which can improve learning dynamics and convergence during training. Some activation functions, like the sigmoid and hyperbolic tangent (tanh), squash their input to a fixed range, which can help to mitigate the exploding gradient problem. Other activation functions, like the rectified linear unit (ReLU), impose sparsity on the activations, which can lead to more efficient and interpretable models.\n",
      "\n",
      "## Common Activation Functions\n",
      "\n",
      "Several activation functions have been proposed and widely used in the literature. Some of the most common activation functions include:\n",
      "\n",
      "### Sigmoid Function\n",
      "\n",
      "The sigmoid function, also known as the logistic function, is one of the earliest and most widely used activation functions in neural networks. It is defined as:\n",
      "\n",
      "$$\n",
      "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
      "$$\n",
      "\n",
      "The sigmoid function squashes its input to the range $(0, 1)$, making it particularly useful for binary classification problems. However, the sigmoid function suffers from the vanishing gradient problem, where the gradients become very small for large inputs, leading to slow convergence during training.\n",
      "\n",
      "### Hyperbolic Tangent (tanh) Function\n",
      "\n",
      "The hyperbolic tangent (tanh) function is defined as:\n",
      "\n",
      "$$\n",
      "\\tanh(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}\n",
      "$$\n",
      "\n",
      "The tanh function is similar to the sigmoid function but squashes its input to the range $(-1, 1)$. This centered output can lead to faster convergence during training compared to the sigmoid function. However, the tanh function also suffers from the vanishing gradient problem.\n",
      "\n",
      "### Rectified Linear Unit (ReLU)\n",
      "\n",
      "The rectified linear unit (ReLU) is a popular activation function for deep learning, defined as:\n",
      "\n",
      "$$\n",
      "\\text{ReLU}(x) = \\max(0, x)\n",
      "$$\n",
      "\n",
      "ReLU is computationally efficient and has been shown to alleviate the vanishing gradient problem. However, ReLU can suffer from a phenomenon known as \"dead neurons\" or \"dying ReLU,\" where a neuron becomes inactive and stops contributing to the learning process due to its output being consistently zero.\n",
      "\n",
      "### Leaky ReLU\n",
      "\n",
      "Leaky ReLU is a variant of the ReLU function designed to address the dead neuron problem. It is defined as:\n",
      "\n",
      "$$\n",
      "\\text{LeakyReLU}(x) = \\max(\\alpha x, x)\n",
      "$$\n",
      "\n",
      "where $\\alpha$ is a small positive constant, typically set to 0.01. By allowing a small, non-zero gradient for negative inputs, Leaky ReLU mitigates the dead neuron issue.\n",
      "\n",
      "### Softmax Function\n",
      "\n",
      "The softmax function is used primarily in the output layer of neural networks for multi-class classification problems. It is defined as:\n",
      "\n",
      "$$\n",
      "\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^K e^{x_j}}\n",
      "$$\n",
      "\n",
      "where $x_i$ is the input value for class $i$, and $K$ is the total number of classes. The softmax function normalizes the input values, producing a probability distribution over the classes.\n",
      "\n",
      "### Other Activation Functions\n",
      "\n",
      "Several other activation functions have been proposed, such as the exponential linear unit (ELU), the scaled exponential linear unit (SELU), the maxout function, and the recently introduced Swish function. Each activation function has its advantages and disadvantages, and the choice of activation function depends on the specific problem and architecture of the neural network.\n",
      "\n",
      "## Choosing an Activation Function\n",
      "\n",
      "There is no one-size-fits-all answer to the question of which activation function to use in a neural network. The choice of activation function depends on the specific problem, the architecture of the network, and the desired properties of the learned model. In general, ReLU and its variants have become the default choice for most deep learning applications, due to their computational efficiency and ability to mitigate the vanishing gradient problem. However, other activation functions may be more suitable for specific tasks, such as the sigmoid function for binary classification problems, or the tanh function for tasks that require a centered output range. Ultimately, the choice of activation function should be guided by experimentation and an understanding of the properties and limitations of each function.\n",
      "DONE GENERATING: activation_functions\n",
      "NOW GENERATING: alexnet\n",
      "FINISH_REASON: stop\n",
      "{\n",
      "    \"alexnet\": {\n",
      "        \"title\": \"AlexNet\",\n",
      "        \"prerequisites\": [\"convolutional_neural_networks\", \"backpropagation\", \"stochastic_gradient_descent\", \"loss_functions\", \"optimization_algorithms\", \"deep_learning_frameworks\", \"regularization_techniques\"],\n",
      "        \"further_readings\": [\"vgg\", \"googlenet\", \"resnet\", \"inception\", \"mobilenet\", \"efficientnet\", \"image_classification\", \"transfer_learning\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "# AlexNet\n",
      "\n",
      "AlexNet is a deep convolutional neural network (CNN) architecture proposed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton in 2012. It significantly outperformed all the prior competitors and won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), a prestigious image classification competition. AlexNet is considered a breakthrough in the field of deep learning, as it demonstrated the potential of deep CNNs for image recognition tasks and paved the way for the development of more advanced architectures.\n",
      "\n",
      "## Architecture\n",
      "\n",
      "The AlexNet architecture consists of eight layers: five convolutional layers followed by three fully connected layers. The network ends with a softmax layer to produce the final class probabilities. The architecture is as follows:\n",
      "\n",
      "1. **Convolutional Layer 1**: 96 filters of size 11x11 with a stride of 4 and padding of 0, followed by a ReLU activation function.\n",
      "2. **Max Pooling Layer 1**: 3x3 filters with a stride of 2 and padding of 0.\n",
      "3. **Convolutional Layer 2**: 256 filters of size 5x5 with a stride of 1 and padding of 2, followed by a ReLU activation function.\n",
      "4. **Max Pooling Layer 2**: 3x3 filters with a stride of 2 and padding of 0.\n",
      "5. **Convolutional Layer 3**: 384 filters of size 3x3 with a stride of 1 and padding of 1, followed by a ReLU activation function.\n",
      "6. **Convolutional Layer 4**: 384 filters of size 3x3 with a stride of 1 and padding of 1, followed by a ReLU activation function.\n",
      "7. **Convolutional Layer 5**: 256 filters of size 3x3 with a stride of 1 and padding of 1, followed by a ReLU activation function.\n",
      "8. **Max Pooling Layer 3**: 3x3 filters with a stride of 2 and padding of 0.\n",
      "9. **Fully Connected Layer 1**: 4096 neurons with a ReLU activation function.\n",
      "10. **Fully Connected Layer 2**: 4096 neurons with a ReLU activation function.\n",
      "11. **Fully Connected Layer 3**: 1000 neurons with a softmax activation function.\n",
      "\n",
      "## Key Features\n",
      "\n",
      "### ReLU Activation Function\n",
      "\n",
      "AlexNet employs the Rectified Linear Unit (ReLU) activation function, which is defined as $f(x) = max(0, x)$. ReLUs are computationally more efficient than other activation functions, such as sigmoid or hyperbolic tangent, and help mitigate the vanishing gradient problem. The use of ReLU activation functions significantly accelerated the training process.\n",
      "\n",
      "### Local Response Normalization\n",
      "\n",
      "AlexNet introduces a normalization technique called local response normalization (LRN), which is applied after the first and second convolutional layers. LRN is a form of lateral inhibition that encourages competition among neurons within a local neighborhood. The output of LRN is calculated as:\n",
      "\n",
      "$$\n",
      "b_{x,y}^i = a_{x,y}^i \\left( k + \\frac{\\alpha}{n} \\sum_{j=\\max(0,i-\\frac{n}{2})}^{\\min(N-1,i+\\frac{n}{2})} (a_{x,y}^j)^2 \\right)^{-\\beta}\n",
      "$$\n",
      "\n",
      "Where $a_{x,y}^i$ is the input activation, $b_{x,y}^i$ is the normalized output, $k$, $\\alpha$, and $\\beta$ are hyperparameters, $n$ is the size of the local neighborhood, and $N$ is the total number of feature maps in the layer. LRN aims to enhance the generalization capability of the network.\n",
      "\n",
      "### Overlapping Pooling\n",
      "\n",
      "AlexNet uses overlapping max-pooling layers to reduce the spatial dimensions of the feature maps. Overlapping pooling involves using a stride smaller than the filter size, which results in neighboring pooled regions sharing some of the input values. This technique improves the robustness of the network by reducing overfitting.\n",
      "\n",
      "### Dropout\n",
      "\n",
      "To prevent overfitting, AlexNet incorporates dropout, a regularization technique, in the first two fully connected layers. During training, dropout randomly sets a fraction of the neurons to zero, ensuring that the network does not rely on any single neuron too much. The dropout rate used in AlexNet is 0.5.\n",
      "\n",
      "### Data Augmentation\n",
      "\n",
      "AlexNet uses data augmentation techniques, such as random cropping, flipping, and RGB channel intensity adjustments, to increase the size of the training dataset and improve the model's generalization capabilities.\n",
      "\n",
      "### Splitting the Network Across GPUs\n",
      "\n",
      "At the time of AlexNet's development, the available GPUs had limited memory. To accommodate the large size of the network, the authors split the network across two GPUs, with each GPU responsible for half of the feature maps at each layer. This allowed for faster training and larger batch sizes.\n",
      "\n",
      "## Impact\n",
      "\n",
      "AlexNet's success in the ILSVRC-2012 competition marked a turning point in the field of deep learning. It demonstrated that deep CNNs can achieve state-of-the-art performance in image classification tasks, leading to a surge of research in deep learning and the development of more advanced architectures, such as VGG, GoogLeNet, and ResNet. Today, AlexNet is often used as a baseline for comparing new CNN architectures and serves as a starting point for transfer learning tasks.\n",
      "DONE GENERATING: alexnet\n",
      "NOW GENERATING: vgg\n",
      "FINISH_REASON: stop\n",
      "```\n",
      "{\n",
      "    \"vgg\": {\n",
      "        \"title\": \"VGG\",\n",
      "        \"prerequisites\": [\"convolutional_neural_networks\", \"backpropagation\", \"stochastic_gradient_descent\", \"loss_functions\", \"optimization_algorithms\", \"deep_learning_frameworks\", \"regularization_techniques\"],\n",
      "        \"further_readings\": [\"resnet\", \"inception\", \"efficientnet\", \"densenet\", \"squeezenet\", \"mobile_net\", \"transfer_learning\", \"fine_tuning_and_pretraining\"]\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "# VGG\n",
      "\n",
      "VGG, which stands for Visual Geometry Group, is a deep convolutional neural network (CNN) architecture proposed by the VGG research group at the University of Oxford in 2014. It is known for its simplicity and effectiveness in image recognition tasks. VGG has become a popular choice for image classification, object detection, and other computer vision tasks due to its high performance and ease of implementation.\n",
      "\n",
      "## Architecture\n",
      "\n",
      "VGG consists of multiple convolutional layers followed by max-pooling layers and fully connected layers. The architecture is characterized by its use of small 3x3 convolutional filters, which are stacked in increasing depth to learn increasingly complex features. The VGG family includes the VGG-16 and VGG-19 models, which consist of 16 and 19 layers, respectively. The numbers indicate the total number of layers with weights, including convolutional layers and fully connected layers.\n",
      "\n",
      "The VGG architecture can be summarized as follows:\n",
      "\n",
      "1. Input layer: Receives the input image with a fixed size of 224x224 pixels and three color channels (RGB).\n",
      "\n",
      "2. Convolutional layers: Multiple convolutional layers with small 3x3 filters are used to learn local features. Each convolutional layer is followed by a rectified linear unit (ReLU) activation function to introduce non-linearity.\n",
      "\n",
      "3. Max-pooling layers: After every few convolutional layers, a max-pooling layer is added to reduce the spatial dimensions and increase the receptive field of the network.\n",
      "\n",
      "4. Fully connected layers: After the last max-pooling layer, there are three fully connected layers. The first two have 4096 neurons each, while the last one has the same number of neurons as the number of classes in the classification task.\n",
      "\n",
      "5. Softmax layer: The final layer is a softmax layer that outputs the probability distribution over the classes.\n",
      "\n",
      "## Training\n",
      "\n",
      "VGG is trained using stochastic gradient descent (SGD) with backpropagation. The loss function used is the cross-entropy loss, which measures the discrepancy between the predicted probabilities and the true labels. The optimizer updates the weights of the network to minimize this loss function. Various optimization algorithms, such as momentum and adaptive learning rates, can be used to speed up the training process.\n",
      "\n",
      "Regularization techniques, such as L2 regularization and dropout, are employed to reduce overfitting. Data augmentation, including random cropping, flipping, and color jittering, is used to increase the size and diversity of the training set, further reducing overfitting and improving generalization.\n",
      "\n",
      "## Applications\n",
      "\n",
      "VGG has proven to be highly effective in various computer vision tasks, such as:\n",
      "\n",
      "1. Image classification: VGG has achieved state-of-the-art results in image classification tasks, such as the ImageNet Large Scale Visual Recognition Challenge (ILSVRC).\n",
      "\n",
      "2. Object detection: VGG has been used as a backbone for object detection models like Faster R-CNN and Single Shot MultiBox Detector (SSD), achieving top performance.\n",
      "\n",
      "3. Semantic segmentation: VGG can be used as the encoder part of an encoder-decoder architecture for semantic segmentation tasks, like in the Fully Convolutional Network (FCN) model.\n",
      "\n",
      "4. Transfer learning: VGG can be used as a feature extractor for other tasks, such as fine-tuning the network for a specific domain or using the pre-trained weights as initialization for other models.\n",
      "\n",
      "## Limitations and Future Work\n",
      "\n",
      "While VGG has been highly successful, it has certain limitations, such as a large number of parameters and high computational requirements, making it less suitable for deployment on resource-constrained devices. More recent architectures, such as ResNet, Inception, and EfficientNet, have addressed these limitations by introducing novel techniques like skip connections, depthwise separable convolutions, and compound scaling.\n",
      "\n",
      "Despite these limitations, VGG continues to be a widely used and highly effective CNN architecture for various computer vision tasks. Its simplicity, ease of implementation, and high performance make it a popular choice for researchers and practitioners alike.\n",
      "DONE GENERATING: vgg\n",
      "NOW GENERATING: resnet\n",
      "FINISH_REASON: stop\n",
      "{\n",
      "    \"resnet\": {\n",
      "        \"title\": \"ResNet\",\n",
      "        \"prerequisites\": [\"convolutional_neural_networks\", \"backpropagation\", \"stochastic_gradient_descent\", \"loss_functions\", \"optimization_algorithms\", \"deep_learning_frameworks\", \"regularization_techniques\"],\n",
      "        \"further_readings\": [\"resnet_versions\", \"densenet\", \"inception_networks\", \"efficientnet\", \"resnext\", \"wide_resnet\", \"residual_attention_networks\", \"pretrained_resnet_models\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "# ResNet\n",
      "\n",
      "ResNet, short for Residual Networks, is a type of deep convolutional neural network (CNN) architecture designed to address the vanishing gradient problem in deep networks. It was introduced by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun in their 2015 paper, \"Deep Residual Learning for Image Recognition.\" ResNet has been widely adopted in various computer vision tasks, and it has won multiple awards in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) and the Common Objects in Context (COCO) competition.\n",
      "\n",
      "## Residual Learning\n",
      "\n",
      "The key idea behind ResNet is residual learning, which aims to alleviate the vanishing gradient problem by allowing the network to learn residual functions instead of directly learning the desired mapping. In a standard deep network, each layer learns a mapping function, and the output of the layer is passed through an activation function before being fed into the next layer. In a residual network, however, each layer learns a residual function, and the output is added to the input before being passed through the activation function.\n",
      "\n",
      "A residual block can be formally defined as:\n",
      "\n",
      "$$\n",
      "\\mathcal{F}(x) = H(x) - x\n",
      "$$\n",
      "\n",
      "where $x$ is the input to the residual block, $H(x)$ is the desired mapping, and $\\mathcal{F}(x)$ is the residual function. The output of the residual block is given by:\n",
      "\n",
      "$$\n",
      "y = \\mathcal{F}(x) + x = H(x)\n",
      "$$\n",
      "\n",
      "The addition operation, also called a shortcut connection, allows gradients to flow more freely through the network, which helps prevent the vanishing gradient problem.\n",
      "\n",
      "## ResNet Architecture\n",
      "\n",
      "The ResNet architecture consists of multiple residual blocks stacked on top of each other. Each residual block contains two or more convolutional layers, followed by batch normalization and a ReLU activation function. The input to the block is added to the output of the last convolutional layer before being passed through the activation function.\n",
      "\n",
      "The basic residual block can be extended to a bottleneck residual block, which is more efficient in terms of computation and memory. In a bottleneck residual block, the first and last layers are 1x1 convolutions, while the middle layer is a 3x3 convolution. The 1x1 convolutions are used to reduce and restore the number of channels, which reduces the total number of parameters and computations.\n",
      "\n",
      "ResNet can be divided into different versions based on the number of layers, such as ResNet-18, ResNet-34, ResNet-50, ResNet-101, and ResNet-152. The number in the name indicates the total number of weight layers in the network.\n",
      "\n",
      "## Training and Preprocessing\n",
      "\n",
      "ResNet is trained using stochastic gradient descent with momentum, and a weight decay regularization term is included to prevent overfitting. The learning rate is initially set to a high value and is reduced during training using a step decay schedule. Data augmentation techniques, such as random cropping, horizontal flipping, and color jittering, are applied to the input images to improve the model's generalization performance.\n",
      "\n",
      "## Applications\n",
      "\n",
      "ResNet has been successfully applied to various computer vision tasks, including image classification, object detection, and semantic segmentation. Some popular applications of ResNet are:\n",
      "\n",
      "- ImageNet classification: ResNet achieved state-of-the-art performance on the ImageNet dataset, significantly reducing the top-1 and top-5 error rates compared to previous models.\n",
      "- COCO object detection: ResNet has been used as a backbone network for object detection models, such as Faster R-CNN, and achieved top performance on the COCO dataset.\n",
      "- Transfer learning: Pretrained ResNet models can be fine-tuned on smaller datasets to achieve excellent performance with relatively fewer training samples.\n",
      "\n",
      "## Variants and Extensions\n",
      "\n",
      "Several research works have built upon the original ResNet architecture, proposing variants and extensions that improve its performance or efficiency. Some notable examples include:\n",
      "\n",
      "- ResNeXt: ResNeXt extends ResNet by introducing a split-transform-merge strategy, which adds cardinality as a new dimension of network design.\n",
      "- Wide ResNet: Wide ResNet increases the width of ResNet by increasing the number of channels in each layer while keeping the depth constant, resulting in improved performance and reduced computational complexity.\n",
      "- DenseNet: DenseNet connects each layer to every other layer in a feed-forward fashion, which strengthens feature propagation and encourages feature reuse.\n",
      "- EfficientNet: EfficientNet uses a compound scaling method to jointly scale the depth, width, and resolution of the network, resulting in more efficient models with better performance.\n",
      "- Residual Attention Networks: Residual Attention Networks combine ResNet with attention mechanisms to improve the model's ability to focus on relevant features and ignore irrelevant ones.\n",
      "DONE GENERATING: resnet\n",
      "NOW GENERATING: inception\n",
      "FINISH_REASON: stop\n",
      "{\n",
      "    \"inception\": {\n",
      "        \"title\": \"Inception\",\n",
      "        \"prerequisites\": [\"convolutional_neural_networks\", \"deep_learning_frameworks\", \"backpropagation\", \"stochastic_gradient_descent\", \"loss_functions\", \"optimization_algorithms\", \"regularization_techniques\"],\n",
      "        \"further_readings\": [\"inception_v2_and_v3\", \"inception_v4_and_inception_resnet\", \"xception\", \"googlenet\", \"resnet\", \"densenet\", \"efficientnet\", \"nasnet\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "# Inception\n",
      "\n",
      "Inception is a family of convolutional neural networks (CNNs) designed for image classification tasks, primarily for the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC), which is a popular benchmark in computer vision. The original Inception model, also known as GoogLeNet, was introduced by Christian Szegedy et al. in their paper \"Going Deeper with Convolutions\" in 2014. The Inception architecture achieved top performance in the ILSVRC-2014 competition, significantly reducing the error rate compared to previous state-of-the-art models.\n",
      "\n",
      "The key idea behind the Inception architecture is to efficiently increase the depth and width of the network while maintaining computational efficiency. This is achieved by using a novel module called the \"Inception module,\" which allows the network to learn various feature representations at different scales without increasing the computation cost significantly.\n",
      "\n",
      "## Inception Module\n",
      "\n",
      "The Inception module consists of several parallel convolutional layers with different filter sizes, followed by a concatenation operation. This design enables the network to learn various feature representations at different scales, helping it to capture both local and global information in the input image. The original Inception module has the following components:\n",
      "\n",
      "1. A 1x1 convolutional layer, which reduces the number of input channels and acts as a dimensionality reduction technique.\n",
      "2. A 3x3 convolutional layer, which captures local information in the input image.\n",
      "3. A 5x5 convolutional layer, which captures more global information in the input image.\n",
      "4. A 3x3 max-pooling layer, which reduces the spatial dimensions of the input and provides translation invariance.\n",
      "\n",
      "These components are applied in parallel, and their outputs are concatenated along the channel dimension to form the final output of the Inception module.\n",
      "\n",
      "## Inception Network Architecture\n",
      "\n",
      "The Inception network architecture is built by stacking multiple Inception modules, forming a deep and wide network. In addition to the Inception modules, the network also includes several other layers:\n",
      "\n",
      "1. A stem (input) layer, consisting of a 7x7 convolutional layer, followed by a 3x3 max-pooling layer.\n",
      "2. Multiple Inception modules, which are the core building blocks of the network.\n",
      "3. Auxiliary classifiers, which are added to intermediate layers of the network to mitigate the vanishing gradient problem and provide regularization.\n",
      "4. A global average pooling layer, which reduces the spatial dimensions of the feature maps to 1x1.\n",
      "5. A fully connected layer, which performs the final classification.\n",
      "\n",
      "The network utilizes both dropout and weight decay regularization techniques to prevent overfitting. The model is trained using stochastic gradient descent (SGD) with momentum and learning rate annealing.\n",
      "\n",
      "## Inception Variants\n",
      "\n",
      "Following the success of the original Inception model, several variants have been proposed to further improve its performance and efficiency. Some of these variants include:\n",
      "\n",
      "1. Inception V2 and V3: Introduced by Szegedy et al. in their paper \"Rethinking the Inception Architecture for Computer Vision,\" these models incorporate batch normalization and factorized convolutions to improve training speed and accuracy.\n",
      "2. Inception V4 and Inception-ResNet: Proposed by Szegedy et al. in their paper \"Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning,\" these models combine the Inception architecture with residual connections, leading to improved performance and faster convergence.\n",
      "3. Xception: Introduced by Fran√ßois Chollet in his paper \"Xception: Deep Learning with Depthwise Separable Convolutions,\" this model replaces the Inception modules with depthwise separable convolutions, resulting in a more efficient network with fewer parameters.\n",
      "\n",
      "Other popular CNN architectures, such as ResNet, DenseNet, EfficientNet, and NASNet, have also built upon the ideas introduced by the Inception architecture.\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "The Inception architecture has made significant contributions to the field of deep learning and computer vision, achieving state-of-the-art performance in image classification tasks, while also inspiring other successful architectures. By using the Inception module, the network can efficiently learn different feature representations at various scales, allowing it to capture both local and global information in the input image. This design has proven to be effective and has been further improved upon in several subsequent variants.\n",
      "DONE GENERATING: inception\n",
      "NOW GENERATING: efficientnet\n",
      "FINISH_REASON: stop\n",
      "```\n",
      "{\n",
      "    \"efficientnet\": {\n",
      "        \"title\": \"EfficientNet\",\n",
      "        \"prerequisites\": [\"convolutional_neural_networks\", \"deep_learning_frameworks\", \"optimization_algorithms\", \"transfer_learning\", \"model_architecture\", \"deep_learning\"],\n",
      "        \"further_readings\": [\"efficientnet_b0_to_b7\", \"mixnet\", \"mobilenetv3\", \"mnasnet\", \"neural_architecture_search\", \"compound_scaling\", \"model_scaling\", \"automl\"]\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "# EfficientNet\n",
      "\n",
      "EfficientNet is a family of convolutional neural network (CNN) models that achieve state-of-the-art performance on various computer vision tasks while maintaining lower computational complexity and memory requirements compared to other popular CNN architectures. It was introduced by Mingxing Tan and Quoc V. Le in their 2019 paper, \"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks\".\n",
      "\n",
      "The key innovation behind EfficientNet is the idea of compound scaling, which is a method for uniformly scaling all three dimensions of the network's width, depth, and resolution. This approach allows the model to achieve a better trade-off between accuracy and resource consumption compared to traditional scaling methods, which typically focus on only one or two dimensions.\n",
      "\n",
      "## Compound Scaling\n",
      "\n",
      "Compound scaling is a technique for uniformly scaling the width, depth, and resolution of a convolutional neural network. The idea behind compound scaling is to find optimal values for the scaling factors of each dimension, such that the model's performance improves while maintaining a balance between computational complexity and memory usage.\n",
      "\n",
      "The authors of EfficientNet propose the following formula for compound scaling:\n",
      "\n",
      "- Width scaling: Increase the number of channels in each layer by a factor of $\\alpha$\n",
      "- Depth scaling: Increase the number of layers in the network by a factor of $\\beta$\n",
      "- Resolution scaling: Increase the input image resolution by a factor of $\\gamma$\n",
      "\n",
      "where $\\alpha$, $\\beta$, and $\\gamma$ are positive constants.\n",
      "\n",
      "The authors show that the optimal values for the scaling factors can be determined by solving a constrained optimization problem, where the goal is to maximize the model's accuracy under a given resource constraint. The solution to this problem yields the following relationship between the scaling factors:\n",
      "\n",
      "$$\n",
      "\\alpha \\cdot \\beta^2 \\cdot \\gamma^2 \\approx k\n",
      "$$\n",
      "\n",
      "where $k$ is a constant that depends on the resource constraint.\n",
      "\n",
      "## EfficientNet Models\n",
      "\n",
      "EfficientNet is a family of models, denoted as EfficientNet-B0 to EfficientNet-B7, that are obtained by applying compound scaling to a base model, called EfficientNet-B0. The base model is a relatively small CNN with a simple architecture, consisting of a series of inverted residual blocks with squeeze-and-excitation optimization.\n",
      "\n",
      "The higher-capacity models in the EfficientNet family (B1 to B7) are obtained by scaling up the base model using the compound scaling method. Each model in the family has a different combination of width, depth, and resolution scaling factors, depending on the target performance and resource constraints.\n",
      "\n",
      "EfficientNet models have achieved state-of-the-art performance on various computer vision benchmarks, such as ImageNet, CIFAR-100, and COCO, while maintaining lower computational complexity and memory requirements compared to other popular CNN architectures, such as ResNet and Inception.\n",
      "\n",
      "## Transfer Learning with EfficientNet\n",
      "\n",
      "EfficientNet models have demonstrated excellent transfer learning capabilities, which means that they can be pre-trained on a large dataset (such as ImageNet) and then fine-tuned on a smaller, task-specific dataset. This approach can lead to significant improvements in accuracy and convergence speed compared to training the model from scratch.\n",
      "\n",
      "Several pre-trained EfficientNet models are available in popular deep learning frameworks, such as TensorFlow and PyTorch, making it easy for practitioners to use these models as feature extractors or as the backbone for more complex architectures in various computer vision tasks.\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "EfficientNet is a family of convolutional neural network models that achieve state-of-the-art performance on various computer vision tasks while maintaining lower computational complexity and memory requirements compared to other popular CNN architectures. The key innovation behind EfficientNet is the idea of compound scaling, which allows the model to achieve a better trade-off between accuracy and resource consumption. EfficientNet models have demonstrated excellent transfer learning capabilities, making them a popular choice for various computer vision tasks.\n",
      "DONE GENERATING: efficientnet\n",
      "NOW GENERATING: yolo\n",
      "FINISH_REASON: stop\n",
      "{\n",
      "    \"yolo\": {\n",
      "        \"title\": \"YOLO\",\n",
      "        \"prerequisites\": [\"object_detection\", \"convolutional_neural_networks\", \"bounding_boxes\", \"backpropagation\", \"loss_functions\", \"optimization_algorithms\", \"deep_learning_frameworks\"],\n",
      "        \"further_readings\": [\"yolo_v2\", \"yolo_v3\", \"yolo_v4\", \"yolo_v5\", \"tiny_yolo\", \"real_time_object_detection\", \"single_shot_multibox_detector\", \"region_proposal_networks\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "# YOLO\n",
      "\n",
      "YOLO, short for \"You Only Look Once,\" is a real-time object detection algorithm that combines the tasks of object localization and classification into a single, end-to-end deep learning model. YOLO was first introduced by Joseph Redmon et al. in their 2016 paper, \"You Only Look Once: Unified, Real-Time Object Detection.\" The algorithm has since undergone several updates, with YOLOv2, YOLOv3, YOLOv4, and YOLOv5 being its latest versions.\n",
      "\n",
      "## Overview\n",
      "\n",
      "Traditional object detection approaches, such as R-CNN and its variants, typically involve multiple stages of processing, such as region proposal generation, feature extraction, and classification. While these methods achieve high accuracy, they are computationally expensive and slow, making them unsuitable for real-time applications.\n",
      "\n",
      "YOLO, on the other hand, is designed as a single, unified neural network that processes the entire image in a single pass, significantly reducing the computational complexity and improving the speed of detection. The network divides the input image into a fixed grid (e.g., 13x13 for YOLOv3) and predicts bounding boxes and class probabilities for each grid cell. The network is trained end-to-end using a multi-task loss function that combines localization, classification, and confidence scores.\n",
      "\n",
      "## Network Architecture\n",
      "\n",
      "The YOLO architecture is based on a convolutional neural network (CNN) that consists of several convolutional, pooling, and fully connected layers. The network is designed to predict a fixed number of bounding boxes per grid cell, along with the corresponding class probabilities and confidence scores.\n",
      "\n",
      "The base network used in the original YOLO paper was a modified version of the GoogLeNet architecture, while later versions (e.g., YOLOv2 and YOLOv3) used a custom architecture called Darknet. YOLOv4 introduced an updated version of the Darknet architecture, called CSPDarknet53, which utilized Cross Stage Hierarchical (CSP) connections to improve the gradient flow and increase the network's learning capacity.\n",
      "\n",
      "The final layer of the network is responsible for predicting the bounding box parameters (x, y, width, height), class probabilities, and confidence scores. The output is a tensor of shape (S x S x (B * 5 + C)), where S x S is the grid size, B is the number of bounding boxes per grid cell, and C is the number of classes.\n",
      "\n",
      "## Loss Function\n",
      "\n",
      "The YOLO loss function is a multi-task loss that combines localization, classification, and confidence scores into a single objective. The loss function consists of three main components:\n",
      "\n",
      "1. Localization loss: This term measures the error between the predicted and ground truth bounding box coordinates. The localization loss is computed as the sum of squared errors for the x, y, width, and height parameters of the bounding boxes.\n",
      "\n",
      "2. Classification loss: This term measures the error in the predicted class probabilities for each grid cell. The classification loss is computed as the sum of squared errors between the predicted and ground truth class probabilities.\n",
      "\n",
      "3. Confidence loss: This term measures the error in the predicted confidence scores, which represent the network's confidence in the presence of an object within a bounding box. The confidence loss is computed as the sum of squared errors between the predicted and ground truth confidence scores, where the ground truth confidence scores are given by the intersection over union (IoU) between the predicted and ground truth bounding boxes.\n",
      "\n",
      "The localization and confidence losses are weighted by user-defined parameters to balance the contributions of each term in the overall loss function.\n",
      "\n",
      "## Training and Inference\n",
      "\n",
      "YOLO is trained end-to-end using stochastic gradient descent (SGD) or other optimization algorithms. During training, the network learns to predict bounding boxes and class probabilities directly from the input images. The network is trained on a dataset of annotated images, where each image is labeled with the ground truth bounding boxes and class labels for the objects present in the image.\n",
      "\n",
      "During inference, the network processes the entire image in a single pass and generates a fixed number of bounding box predictions per grid cell. These predictions are then post-processed to remove duplicate detections and threshold the confidence scores. The remaining bounding boxes are combined into a final set of object detections.\n",
      "\n",
      "## Advantages and Limitations\n",
      "\n",
      "YOLO has several advantages over traditional object detection methods:\n",
      "\n",
      "- Speed: YOLO is significantly faster than most other object detection algorithms, making it suitable for real-time applications.\n",
      "- End-to-end learning: YOLO learns to predict bounding boxes and class probabilities directly from the input images, without the need for complex post-processing or additional components.\n",
      "- Unified framework: YOLO combines the tasks of object localization and classification into a single, unified model, simplifying the object detection pipeline.\n",
      "\n",
      "However, YOLO also has some limitations:\n",
      "\n",
      "- Fixed grid size: YOLO divides the input image into a fixed grid, which may not accurately capture objects of different sizes and aspect ratios.\n",
      "- Limited localization accuracy: YOLO's localization accuracy is lower than that of some other object detection methods, such as R-CNN and its variants.\n",
      "- Sensitivity to object size: YOLO tends to perform better on large objects than small objects, as the latter are more challenging to detect and localize accurately.\n",
      "\n",
      "Despite these limitations, YOLO remains a popular choice for real-time object detection applications due to its speed and simplicity.\n",
      "DONE GENERATING: yolo\n",
      "NOW GENERATING: ssd\n",
      "FINISH_REASON: stop\n",
      "{\n",
      "    \"ssd\": {\n",
      "        \"title\": \"SSD\",\n",
      "        \"prerequisites\": [\"convolutional_neural_networks\", \"object_detection\", \"bounding_boxes\", \"anchor_boxes\", \"sliding_window_approach\", \"iou\", \"loss_functions\", \"optimization_algorithms\"],\n",
      "        \"further_readings\": [\"yolo\", \"faster_rcnn\", \"efficientdet\", \"retinanet\", \"mask_rcnn\", \"scale_and_aspect_ratio\", \"hard_negative_mining\", \"multi_box_loss\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "# SSD\n",
      "\n",
      "SSD (Single Shot MultiBox Detector) is an object detection algorithm that aims to detect multiple objects within an image simultaneously. It is a single-shot detector, meaning that it performs object localization and classification in a single forward pass through the network, resulting in faster inference times compared to two-stage detectors like Faster R-CNN. SSD was introduced by Wei Liu et al. in their 2016 paper, \"SSD: Single Shot MultiBox Detector.\"\n",
      "\n",
      "SSD is built upon a base Convolutional Neural Network (CNN), often referred to as the backbone, which is pre-trained on a large-scale image classification dataset, such as ImageNet. The backbone network is used for feature extraction, and it is followed by a series of convolutional layers that predict the class probabilities and bounding box offsets for a fixed set of anchor boxes at multiple scales and aspect ratios.\n",
      "\n",
      "## Anchor Boxes\n",
      "\n",
      "Anchor boxes (also known as default boxes or priors) are pre-defined bounding boxes with different scales and aspect ratios. They serve as reference points for predicting the true bounding boxes of objects in the image. The idea behind using anchor boxes is to handle the variation in object shapes and sizes efficiently. In SSD, the anchor boxes are generated at multiple feature map levels with varying resolutions, which allows the detection of objects at different scales.\n",
      "\n",
      "The anchor boxes are assigned to ground truth objects based on the Intersection over Union (IoU) metric. If the IoU between an anchor box and a ground truth object is above a certain threshold (e.g., 0.5), the anchor box is considered to be a positive match for that object.\n",
      "\n",
      "## Multi-Scale Feature Maps\n",
      "\n",
      "SSD uses multiple feature maps with different resolutions to detect objects of various sizes. These feature maps are generated by applying a series of convolutional layers on top of the backbone network. Each feature map is responsible for detecting objects at a particular scale. The use of multi-scale feature maps helps in improving the detection performance for small objects, which is often a challenge in object detection tasks.\n",
      "\n",
      "## Loss Function\n",
      "\n",
      "The loss function used in SSD is a weighted sum of two components: a localization loss and a confidence loss. The localization loss measures the error in the predicted bounding box offsets (i.e., the difference between the predicted and ground truth bounding box coordinates), and it is usually represented by the Smooth L1 loss. The confidence loss measures the error in the predicted class probabilities and is typically represented by the cross-entropy loss.\n",
      "\n",
      "## Hard Negative Mining\n",
      "\n",
      "SSD employs a technique called hard negative mining to balance the number of positive and negative samples during training. Since the majority of anchor boxes do not contain objects, the model can easily become biased towards the background class. To overcome this issue, SSD uses hard negative mining, which selects a subset of negative samples (i.e., anchor boxes with low IoU to any ground truth object) with the highest classification loss to be included in the training. This process helps in focusing the training on hard examples and improves the overall detection performance.\n",
      "\n",
      "## Training and Inference\n",
      "\n",
      "During training, the model learns to predict the class probabilities and bounding box offsets for each anchor box. The ground truth labels and bounding box coordinates are used to compute the loss, and the model parameters are updated using an optimization algorithm, such as Stochastic Gradient Descent (SGD).\n",
      "\n",
      "During inference, the model takes an input image and generates class probabilities and bounding box predictions for all anchor boxes. A threshold is applied to the class probabilities to filter out low-confidence predictions. Non-Maximum Suppression (NMS) is then applied to remove duplicate detections and retain the most accurate predictions.\n",
      "\n",
      "## Comparison to Other Object Detectors\n",
      "\n",
      "SSD is faster than two-stage detectors like Faster R-CNN because it performs object localization and classification in a single forward pass through the network. However, it may not be as accurate as two-stage detectors, especially for small objects. SSD is also outperformed by more recent single-shot detectors like YOLOv4 and EfficientDet in terms of both speed and accuracy.\n",
      "\n",
      "In summary, SSD is a fast and efficient single-shot object detection algorithm that can detect multiple objects in an image simultaneously. It uses anchor boxes and multi-scale feature maps to handle objects with varying shapes and sizes and employs hard negative mining to balance the training process. SSD has been widely used in various computer vision applications, including autonomous vehicles, robotics, and video surveillance.\n",
      "DONE GENERATING: ssd\n",
      "NOW GENERATING: fast_r-cnn\n",
      "FINISH_REASON: stop\n",
      "{\n",
      "    \"fast_r_cnn\": {\n",
      "        \"title\": \"Fast R-CNN\",\n",
      "        \"prerequisites\": [\"object_detection\", \"convolutional_neural_networks\", \"selective_search\", \"region_proposal_networks\", \"r_cnn\", \"loss_functions\", \"optimization_algorithms\"],\n",
      "        \"further_readings\": [\"faster_r_cnn\", \"mask_r_cnn\", \"yolo\", \"single_shot_multibox_detector\", \"region_based_fully_convolutional_networks\", \"cascade_r_cnn\", \"anchor_boxes\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "# Fast R-CNN\n",
      "\n",
      "Fast R-CNN is a deep learning-based object detection algorithm that improves upon the original R-CNN (Region-based Convolutional Neural Networks) model. The primary goal of Fast R-CNN is to address the speed and performance limitations of R-CNN. Developed by Ross Girshick in 2015, Fast R-CNN achieves this by utilizing a more efficient training and detection pipeline compared to its predecessor.\n",
      "\n",
      "## Background\n",
      "\n",
      "Object detection is a computer vision task that aims to detect and classify objects within an image. In the context of deep learning, convolutional neural networks (CNNs) have become the backbone of many object detection models. R-CNN was designed as a region-based approach to object detection, which involved extracting regions of interest (RoIs) from the input image and then classifying them using a CNN. However, R-CNN suffered from slow training and inference times.\n",
      "\n",
      "Fast R-CNN addresses these issues by introducing a more efficient pipeline that shares the convolutional layers among all RoIs, thus reducing computational redundancy. This results in a significant speedup in both training and inference times.\n",
      "\n",
      "## Architecture\n",
      "\n",
      "The Fast R-CNN architecture consists of the following main components:\n",
      "\n",
      "1. **Convolutional Layers**: The convolutional layers are responsible for extracting features from the input image. These layers can be pre-trained on a large-scale dataset such as ImageNet and then fine-tuned for the specific object detection task.\n",
      "\n",
      "2. **Region of Interest (RoI) Pooling**: Instead of extracting RoIs and feeding them independently into the network as in R-CNN, Fast R-CNN uses RoI pooling to share the convolutional layers among all RoIs. RoI pooling is a max-pooling operation that resizes and pools the features corresponding to each RoI into a fixed size feature map. This enables the network to process all RoIs in parallel, thus reducing computational redundancy and improving efficiency.\n",
      "\n",
      "3. **Fully Connected Layers**: After RoI pooling, the pooled feature maps are fed into fully connected layers to produce class scores and bounding box regression outputs. These layers are trained to predict the object class and refine the bounding box coordinates for each RoI.\n",
      "\n",
      "4. **Multi-task Loss**: Fast R-CNN uses a multi-task loss function that combines both classification and bounding box regression losses. The classification loss is a softmax cross-entropy loss, and the bounding box regression loss is a smooth L1 loss. By optimizing this combined loss function, the network learns to simultaneously classify objects and predict their bounding box coordinates.\n",
      "\n",
      "## Training and Inference\n",
      "\n",
      "The training process of Fast R-CNN involves the following steps:\n",
      "\n",
      "1. **Preprocessing**: The input image is resized such that its shorter side is of a fixed length (e.g., 600 pixels). This ensures that the image dimensions are consistent across the dataset.\n",
      "\n",
      "2. **Region Proposals**: A region proposal algorithm, such as Selective Search or Region Proposal Network (RPN), is used to generate candidate RoIs. These RoIs are then used as input for the RoI pooling layer.\n",
      "\n",
      "3. **Forward Pass**: The input image and RoIs are fed into the network, and a forward pass is performed to compute the class scores and bounding box regression outputs.\n",
      "\n",
      "4. **Backward Pass**: The gradients of the multi-task loss with respect to the network parameters are computed and used to update the parameters using an optimization algorithm (e.g., stochastic gradient descent).\n",
      "\n",
      "During inference, the same pipeline is followed, but the network parameters are kept fixed. The predicted class scores and bounding box regression outputs are used to generate the final object detections. Non-maximum suppression is applied to remove duplicate detections and produce the final set of detected objects.\n",
      "\n",
      "## Advantages and Limitations\n",
      "\n",
      "Fast R-CNN offers several advantages over the original R-CNN model:\n",
      "\n",
      "1. **Speed**: By sharing the convolutional layers among all RoIs, Fast R-CNN significantly reduces the computational redundancy, resulting in faster training and inference times compared to R-CNN.\n",
      "\n",
      "2. **Accuracy**: The multi-task loss function enables Fast R-CNN to learn better object representations by jointly optimizing classification and bounding box regression tasks.\n",
      "\n",
      "However, Fast R-CNN still has some limitations:\n",
      "\n",
      "1. **Region Proposal Dependency**: Fast R-CNN relies on an external region proposal algorithm, which can be a bottleneck in terms of both speed and accuracy.\n",
      "\n",
      "2. **Scalability**: While Fast R-CNN is faster than R-CNN, it may still struggle to process large-scale datasets or real-time applications due to its region-based approach.\n",
      "\n",
      "These limitations have been addressed in subsequent models such as Faster R-CNN, which introduces an end-to-end trainable region proposal network and further improves the speed and accuracy of object detection.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'fast_r-cnn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 60\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mwiki-connections.json\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m     59\u001b[0m     wiki_connections \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(file)\n\u001b[0;32m---> 60\u001b[0m     queue \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m wiki_connections[topic][\u001b[39m'\u001b[39m\u001b[39mprerequisites\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     61\u001b[0m     queue \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m wiki_connections[topic][\u001b[39m'\u001b[39m\u001b[39mfurther_readings\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     63\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mDONE GENERATING:\u001b[39m\u001b[39m'\u001b[39m, topic)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'fast_r-cnn'"
     ]
    }
   ],
   "source": [
    "visited_pages = load_visited_pages()\n",
    "queue = []\n",
    "# visited_pages.remove('recall')\n",
    "\n",
    "if not queue:\n",
    "    with open('wiki-connections.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "        # for key in data:\n",
    "        key = 'f1_score'\n",
    "        for new_topic in data[key]['prerequisites']:\n",
    "            if new_topic not in visited_pages and new_topic not in queue and new_topic not in data:\n",
    "                queue.append(new_topic)\n",
    "        for new_topic in data[key]['further_readings']:\n",
    "            if new_topic not in visited_pages and new_topic not in queue and new_topic not in data:\n",
    "                queue.append(new_topic)\n",
    "        # if len(queue) > 0:\n",
    "        #     break\n",
    "\n",
    "\n",
    "while queue:\n",
    "    topic = queue.pop(0)\n",
    "    topic = topic.lower()\n",
    "    topic = topic.replace(\"'\", \"\")\n",
    "\n",
    "    if topic in visited_pages:\n",
    "        continue\n",
    "\n",
    "    with open('wiki-connections.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "        if topic in data:\n",
    "            continue\n",
    "        while len(queue) == 0:\n",
    "            for key in data:\n",
    "                for new_topic in data[key]['prerequisites']:\n",
    "                    if new_topic not in visited_pages and new_topic not in queue and new_topic != topic and new_topic not in data:\n",
    "                        queue.append(new_topic)\n",
    "                for new_topic in data[key]['further_readings']:\n",
    "                    if new_topic not in visited_pages and new_topic not in queue and new_topic != topic and new_topic not in data:\n",
    "                        queue.append(new_topic)\n",
    "\n",
    "    print('NOW GENERATING:', topic)\n",
    "    prompt = generate_prompt(topic)\n",
    "    finish_reason, message, completion = generate_completion(prompt)\n",
    "    print(\"FINISH_REASON:\", finish_reason)\n",
    "    print(message)\n",
    "\n",
    "    if finish_reason != 'stop':\n",
    "        print(\"Error: Did not finish generating.\")\n",
    "        exit(1)\n",
    "    \n",
    "    generate_json(message, topic)\n",
    "    generate_markdown(message, topic)\n",
    "    generate_js(topic)\n",
    "\n",
    "    visited_pages.add(topic)\n",
    "    save_visited_pages(visited_pages)\n",
    "\n",
    "    with open('wiki-connections.json', 'r') as file:\n",
    "        wiki_connections = json.load(file)\n",
    "        queue += wiki_connections[topic]['prerequisites']\n",
    "        queue += wiki_connections[topic]['further_readings']\n",
    "\n",
    "    print('DONE GENERATING:', topic)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
