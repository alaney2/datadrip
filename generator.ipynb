{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import shutil\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('.env.local')\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(topic):\n",
    "    parts = topic.split('_')\n",
    "    parts = [part.capitalize() for part in parts]\n",
    "    topic = ' '.join(parts)\n",
    "    prompt = f'Topic: {topic}\\n' + '''\n",
    "    You are a world-renowned AI and ML expert.\n",
    "    Provide a JSON object containing the topic, a list of 1-12 prerequisite topics, and a list of 1-12 further readings related to AI, ML, and DL. \n",
    "    Ensure that the prerequisites and further readings are specifically relevant to the given, rather than broad topics like calculus or statistics.\n",
    "    Ensure that the title field is properly capitalized and spaced and has the right punctuation (such as Q-Learning).\n",
    "    Also ensure that the topic, prerequisites, and further readings are in snake_case.\n",
    "    Use a similar format to the example provided below.:\n",
    "\n",
    "    Example:\n",
    "    {\n",
    "        \"generative_adversarial_network\": {\n",
    "            \"title\": \"Generative Adversarial Network\",\n",
    "            \"prerequisites\": [\"expectation_maximization_algorithm\", \"probability_distributions\", \"convolutional_neural_networks\", \"backpropagation\", \"stochastic_gradient_descent\", \"loss_functions\", \"optimization_algorithms\", \"deep_learning_frameworks\", \"regularization_techniques\", \"unsupervised_learning\"],\n",
    "            \"further_readings\": [\"conditional_gans\", \"cycle_gans\", \"stylegan_and_stylegan2\", \"wasserstein_gans\", \"domain_adaptation\", \"image_to_image_translation\", \"semi_supervised_learning\", \"adversarial_training\", \"adversarial_attacks_and_defenses\", \"transfer_learning\"]\n",
    "        }\n",
    "    }\n",
    "    Next, write a detailed wiki page about the given topic in Markdown format. Always write from a third-person perspective and remain unopinionated.\n",
    "    Ensure that this wiki page is explicitly in code format. \n",
    "    Do not include a \"Contents\" section. \n",
    "    Do not include a \"Further Readings\" nor a \"Prerequisites\" section if they just include related topics.\n",
    "    Use a neutral, unbiased tone without exclamation marks. \n",
    "    Ensure that the heading is the same as the title in the JSON object.\n",
    "    Follow Markdown syntax for headings and formatting, and use LaTeX for equations, with inline equations in pairs of $ and multiline equations in $$.\n",
    "    Ensure the entire output is less than 3600 tokens long and does not include an extra line at the end of the Markdown.\n",
    "    '''\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def generate_completion(prompt):\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    finish_reason = completion.choices[0]['finish_reason']\n",
    "    message = completion.choices[0].message.content\n",
    "    return finish_reason, message, completion\n",
    "\n",
    "\n",
    "def generate_json(message, topic):\n",
    "    message = message.strip()\n",
    "    json_string = re.search(r'(?s){\\s*\\\"[^\"]+\\\":\\s*{.*?}\\s*}', message, re.DOTALL)\n",
    "    \n",
    "    if json_string:\n",
    "        json_string = json_string.group()\n",
    "        json_string = json_string.lower()\n",
    "        json_object = json.loads(json_string)\n",
    "\n",
    "        if topic not in json_string:\n",
    "            print(\"Error: Could not find topic in JSON.\")\n",
    "            exit(1)\n",
    "\n",
    "        with open('wiki-connections.json', 'r') as file:\n",
    "            existing_data = json.load(file)\n",
    "\n",
    "        existing_data.update(json_object)\n",
    "        \n",
    "        with open('wiki-connections.json', 'w') as file:\n",
    "            json.dump(existing_data, file, indent=4)\n",
    "    else:\n",
    "        print(\"Error: Could not extract JSON from message.\")\n",
    "        exit(1)\n",
    "\n",
    "\n",
    "def generate_markdown(message, topic):\n",
    "    message = message.strip()\n",
    "    markdown_start_pos = message.find('#')\n",
    "    markdown_content = message[markdown_start_pos:].strip() + '\\n'\n",
    "\n",
    "    md_filename = topic + '.md'\n",
    "    with open(md_filename, 'w') as file:\n",
    "        file.write(markdown_content)\n",
    "\n",
    "    destination_folder = 'data'\n",
    "    shutil.move(md_filename, destination_folder)\n",
    "\n",
    "\n",
    "def generate_js(topic):\n",
    "    md_filename = topic + '.md'\n",
    "    js_string = f'''\n",
    "    import React from 'react';\n",
    "    import path from 'path';\n",
    "    import fs from 'fs';\n",
    "    import PageContent from '@/components/PageContent/PageContent';\n",
    "\n",
    "    const filename = '{md_filename}';\n",
    "\n",
    "    export default function MarkdownPage({{ markdownContent }}) {{\n",
    "    return <PageContent content={{markdownContent}} filename={{filename}} />;\n",
    "    }}\n",
    "\n",
    "    export async function getStaticProps() {{\n",
    "    const filePath = path.join(process.cwd(), 'data', filename);\n",
    "    const markdownContent = fs.readFileSync(filePath, 'utf8');\n",
    "    return {{\n",
    "        props: {{\n",
    "        markdownContent,\n",
    "        }},\n",
    "    }};\n",
    "    }}\n",
    "    '''\n",
    "    js_filename = topic + '.js'\n",
    "    with open(js_filename, 'w') as file:\n",
    "        file.write(js_string)\n",
    "\n",
    "    destination_folder = 'pages'\n",
    "    shutil.move(js_filename, destination_folder)\n",
    "\n",
    "\n",
    "def extract_markdown(message):\n",
    "    markdown_start = message.find('```')\n",
    "    markdown_end = message.rfind('```')\n",
    "    markdown_string = message[markdown_start:markdown_end+3]\n",
    "    return markdown_string\n",
    "\n",
    "\n",
    "def save_visited_pages(visited_pages, file_name='visited_pages.pickle'):\n",
    "    with open(file_name, 'wb') as handle:\n",
    "        pickle.dump(visited_pages, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def load_visited_pages(file_name='visited_pages.pickle'):\n",
    "    try:\n",
    "        with open(file_name, 'rb') as handle:\n",
    "            visited_pages = pickle.load(handle)\n",
    "        return visited_pages\n",
    "    except FileNotFoundError:\n",
    "        return set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW GENERATING: policy_gradient_methods\n",
      "stop\n",
      "{\n",
      "    \"policy_gradient_methods\": {\n",
      "        \"title\": \"Policy Gradient Methods\",\n",
      "        \"prerequisites\": [\"reinforcement_learning\", \"markov_decision_process\", \"value_iteration\", \"q_learning\", \"monte_carlo_tree_search\", \"neural_networks\", \"backpropagation\", \"stochastic_gradient_descent\", \"optimization_algorithms\", \"gradient_descent\", \"convolutional_neural_networks\"],\n",
      "        \"further_readings\": [\"actor_critic_methods\", \"proximal_policy_optimization\", \"trust_region_policy_optimization\", \"asynchronous_advantage_actor_critic\", \"deep_deterministic_policy_gradient\", \"dual_learning\", \"imitation_learning\", \"reinforcement_learning_with_unsupervised_auxiliary_tasks\", \"hierarchical_reinforcement_learning\", \"multi_agent_reinforcement_learning\"]\n",
      "    }\n",
      "}\n",
      "\n",
      "# Policy Gradient Methods\n",
      "\n",
      "Policy Gradient Methods are a class of reinforcement learning techniques used to train an agent to learn an optimal policy for a given task. In contrast to value-based methods, policy gradient methods directly optimize the policy of the agent without estimating a value function. This makes them particularly useful in situations where the environment is partially observable or the action space is continuous.\n",
      "\n",
      "## Background\n",
      "\n",
      "Reinforcement Learning (RL) is a branch of machine learning concerned with training an agent to learn an optimal policy for a given task by interacting with its environment. RL is based on the Markov Decision Process (MDP), which is a mathematical framework that models the interaction between an agent and its environment as a sequence of states, actions, and rewards.\n",
      "\n",
      "There are two main classes of RL algorithms: value-based and policy-based. In value-based methods, the agent learns an estimate of the value function, which is the expected reward that the agent will receive by following a given policy. In policy-based methods, the agent directly learns the policy that maps states to actions.\n",
      "\n",
      "Policy Gradient Methods fall under the category of policy-based methods. They optimize the policy by taking the gradient of the expected reward with respect to the policy parameters and updating them using an optimization algorithm such as stochastic gradient descent.\n",
      "\n",
      "## Algorithm\n",
      "\n",
      "The basic algorithm for Policy Gradient Methods is as follows:\n",
      "\n",
      "1. Initialize the policy parameters\n",
      "2. For each episode:\n",
      "    - Generate a trajectory by following the current policy\n",
      "    - Compute the total reward for the trajectory\n",
      "    - Compute the gradient of the expected reward with respect to the policy parameters\n",
      "    - Update the policy parameters using the gradient and an optimization algorithm\n",
      "\n",
      "The gradient of the expected reward with respect to the policy parameters is given by:\n",
      "\n",
      "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim p_\\theta(\\tau)}[\\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) Q^{\\pi_\\theta}(s_t, a_t)]$$\n",
      "\n",
      "where $\\theta$ are the policy parameters, $\\tau$ is a trajectory, $p_\\theta(\\tau)$ is the probability of generating the trajectory under the policy $\\pi_\\theta$, $\\pi_\\theta(a_t|s_t)$ is the probability of taking action $a_t$ in state $s_t$ under policy $\\pi_\\theta$, and $Q^{\\pi_\\theta}(s_t, a_t)$ is the expected return starting from state $s_t$ and taking action $a_t$ under policy $\\pi_\\theta$.\n",
      "\n",
      "## Variants\n",
      "\n",
      "There are several variants of Policy Gradient Methods, including:\n",
      "\n",
      "- Actor-Critic Methods\n",
      "- Proximal Policy Optimization (PPO)\n",
      "- Trust Region Policy Optimization (TRPO)\n",
      "- Asynchronous Advantage Actor-Critic (A3C)\n",
      "- Deep Deterministic Policy Gradient (DDPG)\n",
      "- Dual Learning\n",
      "- Imitation Learning\n",
      "- Reinforcement Learning with Unsupervised Auxiliary Tasks\n",
      "- Hierarchical Reinforcement Learning\n",
      "- Multi-Agent Reinforcement Learning\n",
      "\n",
      "These variants differ in their approach to estimating the policy gradient and updating the policy parameters.\n",
      "\n",
      "## Applications\n",
      "\n",
      "Policy Gradient Methods have been successfully applied to a wide range of tasks, including:\n",
      "\n",
      "- Playing Atari games using Deep Reinforcement Learning\n",
      "- Learning to play board games such as Go and Chess\n",
      "- Robotics control\n",
      "- Natural language processing\n",
      "- Recommendation systems\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "Policy Gradient Methods are a powerful class of reinforcement learning techniques that allow an agent to learn an optimal policy for a given task by directly optimizing the policy parameters. They have been successfully applied to a wide range of applications and are particularly useful in situations where the environment is partially observable or the action space is continuous.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'title'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mif\u001b[39;00m finish_reason \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mstop\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     17\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m generate_json(message, topic)\n\u001b[1;32m     20\u001b[0m generate_markdown(message, topic)\n\u001b[1;32m     21\u001b[0m generate_js(topic)\n",
      "Cell \u001b[0;32mIn[2], line 55\u001b[0m, in \u001b[0;36mgenerate_json\u001b[0;34m(message, topic)\u001b[0m\n\u001b[1;32m     52\u001b[0m json_string \u001b[39m=\u001b[39m json_string\u001b[39m.\u001b[39mlower()\n\u001b[1;32m     53\u001b[0m json_object \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mloads(json_string)\n\u001b[0;32m---> 55\u001b[0m \u001b[39mif\u001b[39;00m json_object[\u001b[39m'\u001b[39;49m\u001b[39mtitle\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m!=\u001b[39m topic:\n\u001b[1;32m     56\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mError: Title field does not match topic.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     57\u001b[0m     exit(\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'title'"
     ]
    }
   ],
   "source": [
    "visited_pages = load_visited_pages()\n",
    "queue = ['imitation_learning']\n",
    "\n",
    "while queue:\n",
    "    topic = queue.pop(0)\n",
    "    topic = topic.lower()\n",
    "    if topic in visited_pages:\n",
    "        continue\n",
    "\n",
    "    print('NOW GENERATING:', topic)\n",
    "    prompt = generate_prompt(topic)\n",
    "    finish_reason, message, completion = generate_completion(prompt)\n",
    "    print(finish_reason)\n",
    "    print(message)\n",
    "\n",
    "    if finish_reason != 'stop':\n",
    "        continue\n",
    "    \n",
    "    generate_json(message, topic)\n",
    "    generate_markdown(message, topic)\n",
    "    generate_js(topic)\n",
    "\n",
    "    visited_pages.add(topic)\n",
    "    save_visited_pages(visited_pages)\n",
    "\n",
    "    with open('wiki-connections.json', 'r') as file:\n",
    "        wiki_connections = json.load(file)\n",
    "        queue += wiki_connections[topic]['prerequisites']\n",
    "        queue += wiki_connections[topic]['further_readings']\n",
    "\n",
    "    print('DONE GENERATING:', topic)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
